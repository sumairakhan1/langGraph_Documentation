# 4.  How to Add Semantic Search to Your LangGraph Deployment 🌐🔍

In this guide, we’ll explain how to add **semantic search** to your LangGraph deployment, enabling your agent to search for memories and documents based on **semantic similarity** rather than just exact matches. This approach makes searching more intelligent, as it considers the meaning of the words rather than just their presence.

## Prerequisites 🛠️
Before we get started, make sure you have:

- A LangGraph deployment set up (check out how to deploy).
- API keys for your embedding provider (in this case, we’ll use **OpenAI**).
- **langchain** version `>= 0.3.8`.

### What is Semantic Search? 🤖
**Semantic search** allows a system to understand the meaning behind a search query, rather than just matching words exactly. For example, if you search for "best way to cook chicken," a traditional keyword search might only match documents containing the exact phrase "best way to cook chicken." However, semantic search understands that the phrase could also match documents about "chicken cooking tips" or "how to prepare chicken."

### Setting Up the Store in `langgraph.json` 📁

The first step is to configure the **store** in your **`langgraph.json`** file, which will allow the LangGraph deployment to use semantic search. Here’s an example of the configuration:

```json
{
    "store": {
        "index": {
            "embed": "openai:text-embeddings-3-small",
            "dims": 1536,
            "fields": ["$"]
        }
    }
}
```

### Explanation of the Configuration 🔧

1. **"embed"**: This specifies which **embedding model** to use for generating the embeddings. In this case, we’re using **OpenAI's** `"text-embeddings-3-small"` model.
2. **"dims"**: This is the **dimension** of the embeddings. For the `"text-embeddings-3-small"` model, the output has a dimension of **1536**.
3. **"fields"**: This specifies which **fields** of your data should be indexed for search. The value `["$"]` means index everything in your data, but you can also specify certain fields like `["text", "metadata.title"]`.

### Install Dependencies 📦

Ensure you have the correct version of `langchain` installed by adding it to your project dependencies.

If you’re using **`pyproject.toml`**:

```toml
[project]
dependencies = [
    "langchain>=0.3.8"
]
```

Or if using **`requirements.txt`**:

```txt
langchain>=0.3.8
```

### Using Semantic Search in LangGraph Nodes 🔍

Once you have everything configured, you can use semantic search inside your LangGraph nodes to find similar documents or memories. Here’s an example of how you might implement this in code:

```javascript
function search_memory(state, store) {
    // Perform a semantic search on the store using a search query
    // The namespace helps organize your memories (e.g., "user_facts" or "conversation_summaries")
    const results = store.search({
        namespace: ["memory", "facts"],  // Organize memories by type
        query: "your search query",      // The query you want to search for
        limit: 3                         // Limit the number of results returned
    });
    return results;
}
```

### Explanation of the Code 👨‍💻

1. **`state`**: Represents the current state of the LangGraph.
2. **`store`**: Represents the store where your indexed data is stored.
3. **`namespace`**: A tuple used to organize memories. In this example, we are organizing them under the categories `memory` and `facts`.
4. **`query`**: The search query you are using to look for similar documents or memories.
5. **`limit`**: The maximum number of search results to return (in this case, 3).

### Custom Embeddings 🔧

If you want to use **custom embeddings** rather than OpenAI’s default embeddings, you can specify a custom function that generates the embeddings for your data. Here’s how to do it:

#### Configuration in `langgraph.json`:

```json
{
    "store": {
        "index": {
            "embed": "path/to/embedding_function.py:embed",  // Path to custom embedding function
            "dims": 1536,
            "fields": ["$"]
        }
    }
}
```

#### Example Custom Embedding Function:

```python
from openai import AsyncOpenAI

client = AsyncOpenAI()

async def aembed_texts(texts: list[str]) -> list[list[float]]:
    """Custom embedding function that must:
    1. Be async
    2. Accept a list of strings
    3. Return a list of float arrays (embeddings)
    """
    response = await client.embeddings.create(
        model="text-embedding-3-small",  # The OpenAI model to use for embeddings
        input=texts  # Pass the input texts to the model
    )
    return [e.embedding for e in response.data]
```

### Explanation of the Custom Embedding Function 🧑‍💻

1. **`AsyncOpenAI()`**: Initializes an asynchronous OpenAI client.
2. **`aembed_texts()`**: This function will generate embeddings for a list of input texts.
3. **`await client.embeddings.create()`**: Asynchronously calls the OpenAI API to generate embeddings for the provided texts.
4. **`e.embedding`**: Extracts the actual embedding vector (list of floats) from the OpenAI API response.

### Querying via the API 🔌

If you want to query the store via the LangGraph SDK, you can do so asynchronously. Here’s an example of how you would do this:

```javascript
import { get_client } from 'langgraph_sdk';

async function search_store() {
    const client = get_client();
    const results = await client.store.search_items(
        ["memory", "facts"],  // Namespace to organize memories
        "your search query",   // Search query
        3                       // Number of results to return
    );
    return results;
}

// Use in an async context
const results = await search_store();
```

### Explanation of the API Query 👩‍💻

1. **`get_client()`**: Initializes the LangGraph client.
2. **`client.store.search_items()`**: This function is used to search the store. It takes the namespace, query, and a limit on the number of results.
3. **`await search_store()`**: Since this is an async function, we use `await` to ensure the search is completed before we proceed.

### Real-World Example 🌍

Let’s imagine you’re building a **personalized assistant** that helps users with health advice. This assistant stores users' previous conversations, preferences, and health facts (e.g., allergies, food preferences). When a user asks, “What should I eat for dinner?”, instead of searching for exact matches, your assistant uses semantic search to find relevant advice, even if the exact words aren’t the same.

For instance:
- **Query**: “What should I eat for dinner?”
- **Result**: The system might find memories such as “User prefers vegetarian meals” or “User is allergic to nuts” and provide personalized suggestions based on this information.

### Conclusion 🎯

By integrating **semantic search** into your LangGraph deployment, you create a system that goes beyond traditional keyword matching. This results in more relevant and intelligent search results, improving user interactions and the overall quality of your app. Whether you’re building a chatbot, recommendation engine, or personalized assistant, semantic search adds a powerful layer of context to the search process.