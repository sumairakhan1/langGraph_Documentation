# Rebuild Graph at Runtime in LangGraph

Rebuilding a graph at runtime means changing how your graph behaves during execution based on certain configurations. This is useful when you need to modify the structure or logic of your graph dynamically, depending on conditions such as user input, environment variables, or other runtime factors.

## ðŸŽ¯ **Purpose and Use Cases**

In many real-world applications, graphs can represent workflows or logic that need to be adapted based on certain factors. For example:
- **Personalized User Experience**: You might want to modify a chatbot's behavior based on whether the user is a new user or an existing one. For new users, the bot might offer an onboarding experience, while for existing users, it might focus on more specific tasks.
- **Custom Configuration**: Imagine a recommendation system that changes its recommendations based on the user's profile. The system would dynamically adjust the logic of its graph based on user preferences or behavior.

Letâ€™s dive into the details of how to rebuild a graph at runtime using LangGraph.

## ðŸ”‘ **Prerequisites**

Before we proceed, ensure that you have:
- LangGraph app set up correctly.
- Familiarity with the concept of a **graph** and **nodes** in LangGraph (if not, review related guides on LangGraph).
- Familiarity with **Python** and the **LangChain** library, as they are fundamental for interacting with LangGraph.

## ðŸ›  **How It Works**

Youâ€™ll learn how to define a simple graph and then modify it dynamically based on a configuration.

### Example of a Simple Graph

In a simple graph, we define a sequence of steps (or **nodes**) that represent tasks. Letâ€™s say weâ€™re building an agent that fetches responses from an LLM (Large Language Model) and returns them to the user.

Here's the setup for a basic agent:

```python
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, MessageGraph

# Define the LLM model
model = ChatOpenAI(temperature=0)

# Initialize the graph workflow
graph_workflow = MessageGraph()

# Add an 'agent' node that calls the model
graph_workflow.add_node("agent", model)

# Add edges that define the flow of the graph
graph_workflow.add_edge("agent", END)
graph_workflow.add_edge(START, "agent")

# Compile the graph
agent = graph_workflow.compile()
```

### Explanation:
- **model**: This is an instance of `ChatOpenAI` which is the language model we are interacting with.
- **graph_workflow**: This is the object where we define the nodes and edges of our graph. It represents the flow of tasks (here, calling the model).
- **add_node**: Adds a node called "agent" to the graph, representing the LLM agent.
- **add_edge**: Defines the connections between nodes (i.e., the sequence of operations). We start at the `START` node and move to the `agent` node, which then moves to the `END` node.
- **compile**: This step compiles the graph into a form that can be executed by the LangGraph API.

### LangGraph Configuration File

Now, the server needs to know where your graph is defined. This is done via a configuration file called `langgraph.json`.

```json
{
    "dependencies": ["."],
    "graphs": {
        "openai_agent": "./openai_agent.py:agent"
    },
    "env": "./.env"
}
```

### What Does This Mean?
- **dependencies**: Lists any directories or files that are required for your graph to run.
- **graphs**: Specifies the location of the graph variable (`openai_agent`).
- **env**: Specifies the location of your environment variables (e.g., API keys).

## ðŸ”„ **Rebuilding the Graph at Runtime**

Now letâ€™s get into the dynamic part! Instead of using a static graph, we want to rebuild the graph based on runtime configurations.

Hereâ€™s an updated version of `openai_agent.py` where the graph is rebuilt based on the configuration.

### Example Code:

```python
from typing import Annotated
from typing_extensions import TypedDict
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, MessageGraph
from langgraph.graph.state import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableConfig

# Define the state structure for messages
class State(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]

# Define the model with a temperature setting
model = ChatOpenAI(temperature=0)

def make_default_graph():
    """Make a simple LLM agent"""
    graph_workflow = StateGraph(State)

    def call_model(state):
        return {"messages": [model.invoke(state["messages"])]}

    graph_workflow.add_node("agent", call_model)
    graph_workflow.add_edge("agent", END)
    graph_workflow.add_edge(START, "agent")

    agent = graph_workflow.compile()
    return agent

def make_alternative_graph():
    """Make a tool-calling agent"""
    
    @tool
    def add(a: float, b: float):
        """Adds two numbers."""
        return a + b

    tool_node = ToolNode([add])
    model_with_tools = model.bind_tools([add])

    def call_model(state):
        return {"messages": [model_with_tools.invoke(state["messages"])]}

    def should_continue(state: State):
        if state["messages"][-1].tool_calls:
            return "tools"
        else:
            return END

    graph_workflow = StateGraph(State)
    graph_workflow.add_node("agent", call_model)
    graph_workflow.add_node("tools", tool_node)
    graph_workflow.add_edge("tools", "agent")
    graph_workflow.add_edge(START, "agent")
    graph_workflow.add_conditional_edges("agent", should_continue)

    agent = graph_workflow.compile()
    return agent

def make_graph(config: RunnableConfig):
    """Decide which graph to build based on the config"""
    user_id = config.get("configurable", {}).get("user_id")
    if user_id == "1":
        return make_default_graph()
    else:
        return make_alternative_graph()
```

### Explanation of the Code:

1. **StateGraph**: This class is used to define a state machine in the graph. We pass `State` as the type for the state, which contains a list of messages.
2. **make_default_graph**: This function creates a basic graph where the agent simply calls the LLM model and returns its response.
3. **make_alternative_graph**: This function creates a more advanced graph, which includes a tool-calling agent (e.g., a calculator that adds two numbers). 
4. **make_graph**: This function decides which graph to build based on the provided `config`. If the `user_id` is "1", it creates the default graph; otherwise, it builds the alternative graph.

## ðŸ“œ **Final Configuration**

Now, the LangGraph API configuration (`langgraph.json`) should point to the function `make_graph`:

```json
{
    "dependencies": ["."],
    "graphs": {
        "openai_agent": "./openai_agent.py:make_graph"
    },
    "env": "./.env"
}
```

## âœ… **Testing and Deployment**

- **For Development**: You can test this setup by running the API server in memory mode with the command:

```bash
langgraph dev
```

- **For Production**: Use a persistent storage backend by running:

```bash
langgraph up
```

Make sure Docker is installed for this option.

---

## ðŸŽ¯ **Real World Use Cases for Dynamic Graph Rebuilding**

1. **Customer Support**: A customer service chatbot might have different workflows for different types of issues. For example, if a customer reports an issue with an order, the graph might be rebuilt to handle that specific problem differently than a general inquiry.
   
2. **Personalized Content Recommendation**: In a recommendation system, the logic behind the recommendations may vary based on user behavior or preferences. For new users, the system might use a generic recommendation engine, while for returning users, it might adapt based on their previous interactions.

3. **Dynamic Workflow Adjustments**: A workflow automation tool may need to adjust its execution paths depending on the status of certain tasks or environmental conditions. For instance, a user in a different region might have a slightly different set of tools available in the workflow.

---

### In Summary:
Rebuilding graphs at runtime allows you to adapt workflows dynamically based on configurations. By creating different graphs and selecting them based on user input or other conditions, you can create flexible, scalable, and personalized applications.

