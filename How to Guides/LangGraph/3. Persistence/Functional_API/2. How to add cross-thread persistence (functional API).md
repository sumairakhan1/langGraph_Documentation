# How to add cross-thread persistence (functional API)

# 🧠 How to Add Cross-Thread Persistence in LangGraph (Functional API)  

Cross-thread persistence is an essential feature for AI applications that require **long-term memory** across different sessions, conversations, or interactions. It enables AI models to remember user preferences, past interactions, and other important details, even when a new thread (conversation) starts.

## 📌 **Why is Cross-Thread Persistence Important?**  
Many AI-powered applications, such as chatbots, virtual assistants, and customer support systems, need to remember user details (like name, preferences, or past interactions) **across multiple conversations**. Without cross-thread persistence, every conversation would start from scratch.

### 📍 **Real-World Example**  
Imagine you are using an AI-powered **shopping assistant**. If the assistant remembers your past preferences (e.g., favorite clothing brands, size, budget) **across multiple sessions**, it can provide more **personalized recommendations** even if you start a new conversation later.

---

# 🚀 **Setting Up Cross-Thread Persistence in LangGraph**
To implement cross-thread memory, LangGraph provides a **Store Interface**, which allows you to save and retrieve data across different conversation threads.

---

## 🔧 **Step 1: Install Required Packages**  
Before we start coding, ensure that you have installed the required dependencies:

```python
!pip install -U langchain_anthropic langchain_openai langgraph
```

Next, we need to set API keys for OpenAI and Anthropic:

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
_set_env("OPENAI_API_KEY")
```
**🔍 Explanation:**  
- We check if environment variables for the API keys are set.  
- If not, we prompt the user to enter the keys.  

---

## 📦 **Step 2: Define the In-Memory Store**  
We will use `InMemoryStore` to store user preferences across different conversations.

```python
from langgraph.store.memory import InMemoryStore
from langchain_openai import OpenAIEmbeddings

# Initialize an in-memory store with an embedding model
in_memory_store = InMemoryStore(
    index={
        "embed": OpenAIEmbeddings(model="text-embedding-3-small"),
        "dims": 1536,
    }
)
```
**🔍 Explanation:**  
- `InMemoryStore` is used to store and retrieve user information.  
- We use `OpenAIEmbeddings` to **index stored data** for efficient searching.  

---

## 💡 **Step 3: Define the Chat Model**  
Now, we define the AI model we will be using:

```python
from langchain_anthropic import ChatAnthropic

# Define the chat model
model = ChatAnthropic(model="claude-3-5-sonnet-latest")
```
**🔍 Explanation:**  
- We are using **Claude 3.5 Sonnet**, an AI chatbot from **Anthropic**, to respond to users.

---

## 🔄 **Step 4: Implement the Memory-Enabled Chat Function**  
Now, we define a function that retrieves and stores user memories.

```python
import uuid
from langchain_core.messages import BaseMessage
from langgraph.func import task

@task
def call_model(messages: list[BaseMessage], memory_store: InMemoryStore, user_id: str):
    namespace = ("memories", user_id)  # Define namespace for storing user data
    last_message = messages[-1]  # Get the latest user message

    # Retrieve memories relevant to the latest message
    memories = memory_store.search(namespace, query=str(last_message.content))
    info = "\n".join([d.value["data"] for d in memories])

    system_msg = f"You are a helpful assistant talking to the user. User info: {info}"

    # Store new memories if the user requests it
    if "remember" in last_message.content.lower():
        memory = "User name is Bob"
        memory_store.put(namespace, str(uuid.uuid4()), {"data": memory})

    # Generate response using the AI model
    response = model.invoke([{"role": "system", "content": system_msg}] + messages)
    return response
```
**🔍 Explanation:**  
- **Retrieves past memories** based on the latest user message.  
- **Stores new memories** if the user explicitly asks to "remember" something.  
- **Generates a response** considering both new input and past stored memories.  

---

## 🔗 **Step 5: Create the Workflow with Memory Persistence**
Now, let's define our **entrypoint** function and connect everything:

```python
from langgraph.func import entrypoint
from langgraph.graph import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.base import BaseStore
from langchain_core.runnables import RunnableConfig

# Define the workflow with cross-thread persistence
@entrypoint(checkpointer=MemorySaver(), store=in_memory_store)
def workflow(
    inputs: list[BaseMessage],
    *,
    previous: list[BaseMessage],
    config: RunnableConfig,
    store: BaseStore,
):
    user_id = config["configurable"]["user_id"]  # Retrieve user ID
    previous = previous or []
    inputs = add_messages(previous, inputs)  # Combine past and current messages

    # Call the AI model with memory
    response = call_model(inputs, store, user_id).result()
    
    # Return the AI response and save memory
    return entrypoint.final(value=response, save=add_messages(inputs, response))
```
**🔍 Explanation:**  
- `checkpointer=MemorySaver()` enables **short-term memory** within a session.  
- `store=in_memory_store` enables **long-term memory** across sessions.  
- `config["configurable"]["user_id"]` ensures **memories are tied to specific users**.  
- `add_messages(previous, inputs)` combines past and current user messages.  

---

## 🎯 **Step 6: Running the Workflow**
Now, let's test our chatbot's memory capabilities.

### **📝 Case 1: Remembering User Name**
```python
config = {"configurable": {"thread_id": "1", "user_id": "1"}}
input_message = {"role": "user", "content": "Hi! Remember: my name is Bob"}

for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()
```
**Expected Output:**  
```
Hello Bob! Nice to meet you. I'll remember that your name is Bob. How can I help you today?
```
📌 **Now the AI has remembered the user’s name!**  

---

### **🔍 Case 2: Asking AI for Stored Information**
```python
config = {"configurable": {"thread_id": "2", "user_id": "1"}}
input_message = {"role": "user", "content": "What is my name?"}

for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()
```
**Expected Output:**  
```
Your name is Bob.
```
🎉 **The AI successfully recalls the stored memory, even in a new conversation!**  

---

### **🚫 Case 3: Checking If Memory is User-Specific**
```python
config = {"configurable": {"thread_id": "3", "user_id": "2"}}
input_message = {"role": "user", "content": "What is my name?"}

for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()
```
**Expected Output:**  
```
I don't have any information about your name. Each conversation starts fresh.
```
✅ **The AI does NOT remember data from a different user!**  

---

## 🏆 **Key Takeaways**
✅ **Cross-thread persistence** allows AI to remember data across multiple conversations.  
✅ **InMemoryStore** is used to save and retrieve memories.  
✅ **User-specific storage** ensures data is not shared across different users.  
✅ **Memory retrieval and updating** help personalize AI interactions.  

---

## 📌 **Final Thoughts**  
By implementing **cross-thread persistence**, you can build **smarter, memory-aware chatbots and AI applications** that **remember user preferences** even across different conversations.  

Would you like to extend this example further by **storing memories in a database** for long-term storage? 🚀 Let me know! 😊