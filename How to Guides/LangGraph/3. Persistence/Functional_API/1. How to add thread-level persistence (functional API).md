# ğŸ§  **Adding Thread-Level Persistence in LangGraph (Functional API)**

Many AI applications, such as chatbots, need to remember previous messages within the same conversation. This memory is known as **thread-level persistence** and ensures that context is retained for multiple turns within a single interaction.

LangGraph provides a simple way to enable this feature using **checkpointers**. In this guide, weâ€™ll walk through how to implement thread-level persistence step by step, with **real-world use cases, code examples, and explanations**.

---

## ğŸ”¹ **Why Do We Need Thread-Level Persistence?**

Imagine a chatbot in a customer support system. If a user asks:  
1ï¸âƒ£ "Hi, my name is Bob."  
2ï¸âƒ£ "Whatâ€™s my name?"  

The chatbot should remember "Bob" and respond correctly. However, if a new user starts a fresh conversation, the chatbot should not retain Bobâ€™s information.

This is where **thread-level persistence** comes in:  
âœ… **Keeps memory within a single conversation thread**  
âœ… **Forgets data when a new conversation starts**  
âœ… **Helps chatbots, virtual assistants, and AI applications maintain context**

---

## ğŸ›  **Step 1: Install Required Packages**
Before starting, install **LangGraph** and **LangChain**:

```python
!pip install --quiet -U langgraph langchain_anthropic
```

This installs:
- `langgraph`: A framework for building AI workflows
- `langchain_anthropic`: Integration for the **Claude** model by Anthropic

---

## ğŸ”‘ **Step 2: Set Up API Key**
Since weâ€™re using **Anthropicâ€™s Claude model**, we need to set up an API key:

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
```
ğŸ” **Explanation:**  
- We use `getpass.getpass()` to securely enter the API key.  
- The key is stored in `os.environ` to be used throughout the session.

---

## ğŸ’¬ **Step 3: Define the AI Model**
We will use **Claude (Claude-3-5-sonnet-latest)** from Anthropic:

```python
from langchain_anthropic import ChatAnthropic

# Initialize the Claude model
model = ChatAnthropic(model="claude-3-5-sonnet-latest")
```

ğŸ” **Explanation:**  
- `ChatAnthropic` is a wrapper to interact with Claude.  
- `model="claude-3-5-sonnet-latest"` specifies the latest version of the model.  
- This model will be used to generate responses in our chatbot.

---

## ğŸ— **Step 4: Add Thread-Level Persistence**
To **retain memory within the same thread**, we need to use a **Checkpointer**.

### âœ… **Create a Memory Checkpointer**
```python
from langgraph.checkpoint.memory import MemorySaver

# Create an instance of MemorySaver to store conversation history
checkpointer = MemorySaver()
```

ğŸ” **Explanation:**  
- `MemorySaver` stores and retrieves messages **within the same thread**.  
- If a new thread starts, it **forgets** previous interactions.

---

## ğŸ” **Step 5: Define the AI Task (Chatbot Function)**
We define a **task** to process the messages using the AI model.

```python
from langchain_core.messages import BaseMessage
from langgraph.func import task

@task
def call_model(messages: list[BaseMessage]):
    # Invoke the model with the user's messages
    response = model.invoke(messages)
    return response
```

ğŸ” **Explanation:**  
- `@task` defines a function that **processes a step** in the AI workflow.  
- `messages: list[BaseMessage]` is a **list of chat messages** exchanged so far.  
- `model.invoke(messages)` calls the Claude model and returns a response.

---

## ğŸ”„ **Step 6: Create the Workflow with Persistence**
Now, we use the `entrypoint()` decorator to ensure **memory is retained**.

```python
from langgraph.graph import add_messages
from langgraph.func import entrypoint

@entrypoint(checkpointer=checkpointer)
def workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage] = None):
    # Combine new inputs with previous conversation history
    if previous:
        inputs = add_messages(previous, inputs)

    # Get the response from the AI model
    response = call_model(inputs).result()

    # Save both inputs and AI responses for future messages
    return entrypoint.final(value=response, save=add_messages(inputs, response))
```

ğŸ” **Explanation:**  
- `@entrypoint(checkpointer=checkpointer)`:  
  âœ… Ensures the chatbot **remembers previous messages** in the same thread.  
  âœ… **Forgets history** when a new conversation starts.  
- `previous: list[BaseMessage] = None`:  
  âœ… Holds **past conversation history**.  
  âœ… If no history exists, it starts fresh.  
- `add_messages(previous, inputs)`:  
  âœ… Merges old and new messages.  
- `call_model(inputs).result()`:  
  âœ… Calls the AI model to generate a response.  
- `entrypoint.final(value=response, save=add_messages(inputs, response))`:  
  âœ… **Saves** the chat for future messages.  

---

## ğŸ¯ **Step 7: Test the Chatbot with Memory**
Letâ€™s test if our chatbot remembers the conversation.

### âœ… **User Starts a Conversation**
```python
config = {"configurable": {"thread_id": "1"}}
input_message = {"role": "user", "content": "hi! I'm Bob"}

for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()
```

ğŸ’¬ **Output:**
```
Hi Bob! I'm Claude. Nice to meet you! How are you today?
```

ğŸ” **Whatâ€™s Happening?**  
- We **set a thread ID** (`"1"`) to track this conversation.  
- The chatbot **remembers Bobâ€™s name** for future messages.

---

### âœ… **User Asks the Chatbot to Recall Information**
```python
input_message = {"role": "user", "content": "what's my name?"}

for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()
```

ğŸ’¬ **Output:**
```
Your name is Bob.
```

ğŸ” **Whatâ€™s Happening?**  
- Since the same **thread ID ("1")** is used, the chatbot **remembers Bobâ€™s name**.  
- The AI retrieves stored messages using **MemorySaver**.

---

### âœ… **Starting a New Conversation (Forgetting Old Data)**
If a new user starts a fresh conversation with a **different thread ID**, memory is reset.

```python
input_message = {"role": "user", "content": "what's my name?"}

for chunk in workflow.stream(
    [input_message],
    {"configurable": {"thread_id": "2"}},  # New thread ID
    stream_mode="values",
):
    chunk.pretty_print()
```

ğŸ’¬ **Output:**
```
I don't know your name unless you tell me. Each conversation I have starts fresh.
```

ğŸ” **Whatâ€™s Happening?**  
- Since we changed **`thread_id` to `"2"`**, the chatbot **forgets Bobâ€™s name**.  
- Each **thread has its own memory**, allowing independent conversations.

---

## ğŸŒ **Real-World Use Cases**
Thread-level persistence is useful in many applications, such as:

ğŸ’¬ **Chatbots & Virtual Assistants**: Remembering user preferences in a session.  
ğŸ’³ **Customer Support Systems**: Tracking customer queries until resolved.  
ğŸ“š **Education & Tutoring Apps**: Keeping lesson context within a session.  
ğŸ“ **Call Centers**: Retaining conversation history for an agent session.  

---

## ğŸ¯ **Conclusion**
In this guide, we learned:
âœ… **Why thread-level persistence is important**  
âœ… **How to use LangGraphâ€™s MemorySaver to store conversations**  
âœ… **How to implement a chatbot that remembers user messages**  
âœ… **How to start a fresh conversation by changing the thread ID**  

By using LangGraphâ€™s **functional API** and `MemorySaver`, you can **add memory to AI applications effortlessly** while maintaining control over when to reset it. ğŸš€
