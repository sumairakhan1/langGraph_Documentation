# 🧠 **Adding Thread-Level Persistence in LangGraph (Functional API)**

Many AI applications, such as chatbots, need to remember previous messages within the same conversation. This memory is known as **thread-level persistence** and ensures that context is retained for multiple turns within a single interaction.

LangGraph provides a simple way to enable this feature using **checkpointers**. In this guide, we’ll walk through how to implement thread-level persistence step by step, with **real-world use cases, code examples, and explanations**.

---

## 🔹 **Why Do We Need Thread-Level Persistence?**

Imagine a chatbot in a customer support system. If a user asks:  
1️⃣ "Hi, my name is Bob."  
2️⃣ "What’s my name?"  

The chatbot should remember "Bob" and respond correctly. However, if a new user starts a fresh conversation, the chatbot should not retain Bob’s information.

This is where **thread-level persistence** comes in:  
✅ **Keeps memory within a single conversation thread**  
✅ **Forgets data when a new conversation starts**  
✅ **Helps chatbots, virtual assistants, and AI applications maintain context**

---

## 🛠 **Step 1: Install Required Packages**
Before starting, install **LangGraph** and **LangChain**:

```python
!pip install --quiet -U langgraph langchain_anthropic
```

This installs:
- `langgraph`: A framework for building AI workflows
- `langchain_anthropic`: Integration for the **Claude** model by Anthropic

---

## 🔑 **Step 2: Set Up API Key**
Since we’re using **Anthropic’s Claude model**, we need to set up an API key:

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
```
🔍 **Explanation:**  
- We use `getpass.getpass()` to securely enter the API key.  
- The key is stored in `os.environ` to be used throughout the session.

---

## 💬 **Step 3: Define the AI Model**
We will use **Claude (Claude-3-5-sonnet-latest)** from Anthropic:

```python
from langchain_anthropic import ChatAnthropic

# Initialize the Claude model
model = ChatAnthropic(model="claude-3-5-sonnet-latest")
```

🔍 **Explanation:**  
- `ChatAnthropic` is a wrapper to interact with Claude.  
- `model="claude-3-5-sonnet-latest"` specifies the latest version of the model.  
- This model will be used to generate responses in our chatbot.

---

## 🏗 **Step 4: Add Thread-Level Persistence**
To **retain memory within the same thread**, we need to use a **Checkpointer**.

### ✅ **Create a Memory Checkpointer**
```python
from langgraph.checkpoint.memory import MemorySaver

# Create an instance of MemorySaver to store conversation history
checkpointer = MemorySaver()
```

🔍 **Explanation:**  
- `MemorySaver` stores and retrieves messages **within the same thread**.  
- If a new thread starts, it **forgets** previous interactions.

---

## 🔁 **Step 5: Define the AI Task (Chatbot Function)**
We define a **task** to process the messages using the AI model.

```python
from langchain_core.messages import BaseMessage
from langgraph.func import task

@task
def call_model(messages: list[BaseMessage]):
    # Invoke the model with the user's messages
    response = model.invoke(messages)
    return response
```

🔍 **Explanation:**  
- `@task` defines a function that **processes a step** in the AI workflow.  
- `messages: list[BaseMessage]` is a **list of chat messages** exchanged so far.  
- `model.invoke(messages)` calls the Claude model and returns a response.

---

## 🔄 **Step 6: Create the Workflow with Persistence**
Now, we use the `entrypoint()` decorator to ensure **memory is retained**.

```python
from langgraph.graph import add_messages
from langgraph.func import entrypoint

@entrypoint(checkpointer=checkpointer)
def workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage] = None):
    # Combine new inputs with previous conversation history
    if previous:
        inputs = add_messages(previous, inputs)

    # Get the response from the AI model
    response = call_model(inputs).result()

    # Save both inputs and AI responses for future messages
    return entrypoint.final(value=response, save=add_messages(inputs, response))
```

🔍 **Explanation:**  
- `@entrypoint(checkpointer=checkpointer)`:  
  ✅ Ensures the chatbot **remembers previous messages** in the same thread.  
  ✅ **Forgets history** when a new conversation starts.  
- `previous: list[BaseMessage] = None`:  
  ✅ Holds **past conversation history**.  
  ✅ If no history exists, it starts fresh.  
- `add_messages(previous, inputs)`:  
  ✅ Merges old and new messages.  
- `call_model(inputs).result()`:  
  ✅ Calls the AI model to generate a response.  
- `entrypoint.final(value=response, save=add_messages(inputs, response))`:  
  ✅ **Saves** the chat for future messages.  

---

## 🎯 **Step 7: Test the Chatbot with Memory**
Let’s test if our chatbot remembers the conversation.

### ✅ **User Starts a Conversation**
```python
config = {"configurable": {"thread_id": "1"}}
input_message = {"role": "user", "content": "hi! I'm Bob"}

for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()
```

💬 **Output:**
```
Hi Bob! I'm Claude. Nice to meet you! How are you today?
```

🔍 **What’s Happening?**  
- We **set a thread ID** (`"1"`) to track this conversation.  
- The chatbot **remembers Bob’s name** for future messages.

---

### ✅ **User Asks the Chatbot to Recall Information**
```python
input_message = {"role": "user", "content": "what's my name?"}

for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()
```

💬 **Output:**
```
Your name is Bob.
```

🔍 **What’s Happening?**  
- Since the same **thread ID ("1")** is used, the chatbot **remembers Bob’s name**.  
- The AI retrieves stored messages using **MemorySaver**.

---

### ✅ **Starting a New Conversation (Forgetting Old Data)**
If a new user starts a fresh conversation with a **different thread ID**, memory is reset.

```python
input_message = {"role": "user", "content": "what's my name?"}

for chunk in workflow.stream(
    [input_message],
    {"configurable": {"thread_id": "2"}},  # New thread ID
    stream_mode="values",
):
    chunk.pretty_print()
```

💬 **Output:**
```
I don't know your name unless you tell me. Each conversation I have starts fresh.
```

🔍 **What’s Happening?**  
- Since we changed **`thread_id` to `"2"`**, the chatbot **forgets Bob’s name**.  
- Each **thread has its own memory**, allowing independent conversations.

---

## 🌍 **Real-World Use Cases**
Thread-level persistence is useful in many applications, such as:

💬 **Chatbots & Virtual Assistants**: Remembering user preferences in a session.  
💳 **Customer Support Systems**: Tracking customer queries until resolved.  
📚 **Education & Tutoring Apps**: Keeping lesson context within a session.  
📞 **Call Centers**: Retaining conversation history for an agent session.  

---

## 🎯 **Conclusion**
In this guide, we learned:
✅ **Why thread-level persistence is important**  
✅ **How to use LangGraph’s MemorySaver to store conversations**  
✅ **How to implement a chatbot that remembers user messages**  
✅ **How to start a fresh conversation by changing the thread ID**  

By using LangGraph’s **functional API** and `MemorySaver`, you can **add memory to AI applications effortlessly** while maintaining control over when to reset it. 🚀
