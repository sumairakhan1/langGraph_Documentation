# 1. How to add thread-level persistence to your graph

Here's a detailed, beginner-friendly guide explaining how to add **thread-level persistence** to your LangGraph-based application. ğŸš€  

---

# ğŸ§  How to Add Thread-Level Persistence to Your Graph  

Many AI applications need memory to **retain context across multiple interactions**. Without persistence, every interaction is **stateless**, meaning the AI forgets previous conversations.  

LangGraph allows us to **store and retrieve conversation history** using **thread-level persistence**. This guide will cover:  
âœ… Why persistence is important  
âœ… How to implement it in **LangGraph**  
âœ… Real-world examples  
âœ… A complete **code breakdown**  

---

## ğŸ›ï¸ **1. Why Do We Need Persistence?**  

Imagine you're building a **chatbot** that assists users with customer support. The chatbot should remember:  
âœ… The user's name  
âœ… Their past queries  
âœ… Any details shared in previous messages  

Without persistence, the chatbot will **forget everything** after each message. ğŸ˜  

### **ğŸ“Œ Real-World Use Case**:  
Think of a **customer support chatbot** for an e-commerce website. If a customer asks:  
- "Where is my order?"  
- Later asks: "What is my order number?"  

The chatbot should **remember** the previous conversation to provide a relevant answer.  

---

## ğŸ”§ **2. Setting Up LangGraph**  

### ğŸ› ï¸ **Install Required Packages**  

Before we start coding, let's install **LangGraph** and the required dependencies:  

```python
!pip install --quiet -U langgraph langchain_anthropic
```

This installs:  
- **LangGraph** â†’ Used to create AI-based workflows  
- **LangChain-Anthropic** â†’ Allows us to use Claude models  

---

## ğŸ”‘ **3. Setting Up API Keys**  

Since we're using **Anthropic's Claude model**, we need to set up an API key for authentication.  

```python
import getpass
import os

def _set_env(var: str):
    """Set environment variable if it's not already set."""
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
```

### **ğŸ” Code Explanation:**  
- We use `getpass.getpass()` to **securely** input the API key.  
- The `_set_env` function ensures the key is set **only if itâ€™s missing**.  
- This avoids **hardcoding** API keys into our script. ğŸ”’  

---

## ğŸ”„ **4. Defining the Chat Model**  

Now, let's define the AI model weâ€™ll use:  

```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-5-sonnet-20240620")
```

### **ğŸ” Code Explanation:**  
- We import `ChatAnthropic` to interact with Claude models.  
- We initialize the model **"claude-3-5-sonnet-20240620"** for conversation processing.  

---

## ğŸ—ï¸ **5. Creating the StateGraph**  

A **StateGraph** is a flowchart-like structure that processes messages.  

### **ğŸ“Œ Steps:**  
1ï¸âƒ£ Define a function to call the AI model.  
2ï¸âƒ£ Create a **graph** and add a node.  
3ï¸âƒ£ Compile and execute the graph.  

#### **ğŸ“ Define the Model Node:**  

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, MessagesState, START

def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    return {"messages": response}
```

### **ğŸ” Code Explanation:**  
- `MessagesState` represents the **conversation history**.  
- `call_model(state)` takes user messages and gets a **response from the AI**.  

#### **ğŸ“ Create the Graph:**  

```python
builder = StateGraph(MessagesState)
builder.add_node("call_model", call_model)
builder.add_edge(START, "call_model")
graph = builder.compile()
```

### **ğŸ” Code Explanation:**  
- `StateGraph(MessagesState)` initializes a graph that **stores messages**.  
- `add_node("call_model", call_model)` adds a **node** to process AI responses.  
- `add_edge(START, "call_model")` ensures execution starts from `call_model`.  
- `compile()` finalizes the graph.  

---

## âŒ **6. Running Without Persistence (Forgetting Messages)**  

Let's test what happens **before adding persistence**:  

```python
input_message = {"role": "user", "content": "hi! I'm Bob"}
for chunk in graph.stream({"messages": [input_message]}, stream_mode="values"):
    chunk["messages"][-1].pretty_print()

input_message = {"role": "user", "content": "what's my name?"}
for chunk in graph.stream({"messages": [input_message]}, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

### **ğŸ’¡ Output (Before Persistence)**
```
Human: hi! I'm Bob
AI: Hello Bob! How can I assist you?

Human: what's my name?
AI: Sorry, I don't remember previous conversations.
```
ğŸ˜² **Oops! The AI forgot Bob's name!**  

---

## ğŸ§  **7. Adding Thread-Level Persistence**  

We need to **save and recall messages** using a `Checkpointer`.  

#### **ğŸ“ Enable Memory:**  
```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
graph = builder.compile(checkpointer=memory)
```

### **ğŸ” Code Explanation:**  
- `MemorySaver()` creates a **memory checkpoint**.  
- `compile(checkpointer=memory)` enables **persistence**.  

---

## âœ… **8. Running With Persistence (Remembering Messages)**  

Now let's try the conversation **again**:  

```python
config = {"configurable": {"thread_id": "1"}}

input_message = {"role": "user", "content": "hi! I'm Bob"}
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()

input_message = {"role": "user", "content": "what's my name?"}
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

### **ğŸ’¡ Output (With Persistence)**
```
Human: hi! I'm Bob
AI: Hello Bob! How can I assist you?

Human: what's my name?
AI: Your name is Bob.
```
ğŸ‰ **Success! The AI remembers the userâ€™s name.**  

---

## ğŸ”„ **9. Starting a New Conversation (Forgetting Old Data)**  

If we want to start a **fresh conversation**, we can pass a **new thread ID**:  

```python
input_message = {"role": "user", "content": "what's my name?"}
for chunk in graph.stream(
    {"messages": [input_message]},
    {"configurable": {"thread_id": "2"}},
    stream_mode="values",
):
    chunk["messages"][-1].pretty_print()
```

### **ğŸ’¡ Output (New Thread)**
```
Human: what's my name?
AI: Sorry, I don't remember previous conversations.
```
**Poof!** ğŸª„ **All memories are gone for this new session.**  

---

# ğŸ¯ **Conclusion**  

âœ… **Without persistence**, the AI **forgets everything** between interactions.  
âœ… **With `MemorySaver()`**, we store conversation history **thread-by-thread**.  
âœ… **Changing `thread_id`** allows us to create separate conversations.  

---

# ğŸ“Œ **Where Can You Use This?**  
âœ”ï¸ AI **chatbots** (customer support, personal assistants)  
âœ”ï¸ AI-powered **education tutors**  
âœ”ï¸ AI **game characters** that remember player interactions  

---

# ğŸ› ï¸ **Next Steps**  
ğŸš€ Try different **memory-saving techniques** like databases.  
ğŸ“Œ Experiment with **LangGraph Cloud** for built-in persistence.  
ğŸ” Integrate **vector databases** for long-term AI memory.  

Want more deep-dive tutorials? Let me know! ğŸš€âœ¨

---

# O3 mini Response:

# ğŸš€ Thread-Level Persistence in LangGraph

Thread-level persistence lets your AI application remember past interactions within a single conversation thread. This means that when a user interacts with your chatbot multiple times, the system can recall previous messages and context. Letâ€™s break down the concept, see some code examples, and understand how you might use this in the real world.

---

## ğŸ’¡ What Is Thread-Level Persistence?

When you chat with a virtual assistant or customer support bot, you expect it to remember details from earlier in the conversation. With **thread-level persistence**, the system stores the conversation state (like the messages exchanged) so that every new interaction in the same thread is aware of the previous context. 

- **Without Persistence:** Each message is treated as an isolated request.
- **With Persistence:** The chatbot â€œremembersâ€ earlier messages, enabling more natural and coherent conversations.

---

## ğŸ”§ Real-World Example

Imagine a customer service chatbot on an e-commerce website:
- **Without persistence:** The bot might ask for your name every time you ask about your order status.
- **With persistence:** Once you say, â€œHi, Iâ€™m Alex,â€ the bot remembers your name. Later, when you ask, â€œWhatâ€™s my name?â€ it confidently replies, â€œYour name is Alex!â€

This makes interactions smoother and more human-like.

---

## ğŸ› ï¸ Setting Up Your Environment

Before diving into the code, you need to install the required packages and set up your API keys. Hereâ€™s how you can do it:

```python
# Install required packages quietly without showing stderr output
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_anthropic

# Import modules to securely set API keys
import getpass
import os

# Define a helper function to set environment variables if not already set
def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

# Set your Anthropic API key for the LLM
_set_env("ANTHROPIC_API_KEY")
```

**Explanation:**
- **Package Installation:** The `%pip install` command installs `langgraph` and `langchain_anthropic` for interacting with AI models.
- **API Key Setup:** The `_set_env` function ensures your API key is securely set in your environment variables.

---

## ğŸ—ï¸ Defining the Graph

Now, letâ€™s define a simple graph that calls a chat model.

### 1. **Import and Initialize the Model**

```python
from langchain_anthropic import ChatAnthropic

# Initialize the chat model with a specific configuration
model = ChatAnthropic(model="claude-3-5-sonnet-20240620")
```

**Explanation:**
- **Importing:** We import `ChatAnthropic`, a client for interacting with Anthropicâ€™s chat models.
- **Initializing:** We create an instance of the model. The parameter `"claude-3-5-sonnet-20240620"` selects a specific model version.

### 2. **Building the Graph**

```python
from langgraph.graph import StateGraph, MessagesState, START

# Define a function that the graph will use to call the model
def call_model(state: MessagesState):
    # Invoke the model using the current conversation messages
    response = model.invoke(state["messages"])
    # Return the response wrapped in a dictionary under "messages"
    return {"messages": response}

# Create a StateGraph with the initial state type (MessagesState)
builder = StateGraph(MessagesState)

# Add a node named "call_model" that uses our function
builder.add_node("call_model", call_model)

# Define an edge from the starting point to our node
builder.add_edge(START, "call_model")

# Compile the graph to prepare it for execution
graph = builder.compile()
```

**Explanation:**
- **StateGraph:** Represents the flow of our conversation. It holds the state (messages) and defines how to process them.
- **call_model Function:** 
  - **Input:** Receives the current conversation state (`state["messages"]`).
  - **Process:** Calls the chat model with the current messages.
  - **Output:** Returns a new state containing the model's response.
- **Graph Building:** 
  - **add_node:** Adds our processing function to the graph.
  - **add_edge:** Connects the start of the conversation to our node.
  - **compile:** Prepares the graph for use. At this stage, without persistence, each call resets the conversation state.

---

## ğŸ”„ Adding Thread-Level Persistence

To remember past messages within the same conversation, we add persistence using a **checkpointer**.

### 1. **Integrate MemorySaver**

```python
from langgraph.checkpoint.memory import MemorySaver

# Create a MemorySaver instance that will handle state persistence
memory = MemorySaver()

# Re-compile the graph, now passing in our checkpointer
graph = builder.compile(checkpointer=memory)
```

**Explanation:**
- **MemorySaver:** This is a built-in checkpointer that saves and restores the conversation state in memory.
- **Passing the Checkpointer:** When compiling the graph with `checkpointer=memory`, the graph is now aware of past interactions. It stores the state per conversation thread.

### 2. **Using the Persistent Graph**

To interact with the graph and see persistence in action, use a configuration dictionary that includes a `thread_id`.

#### Starting a Conversation (Thread ID "1")

```python
# Configuration for the thread; thread_id "1" represents a specific conversation
config = {"configurable": {"thread_id": "1"}}

# User starts the conversation by introducing themselves
input_message = {"role": "user", "content": "hi! I'm bob"}

# Stream the response from the graph and print the latest message
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

**Explanation:**
- **Config Dictionary:** Includes a `thread_id` which uniquely identifies the conversation. Here, `"1"` is used for the first conversation.
- **Input Message:** The user introduces themselves.
- **Graph Stream:** The graph processes the input, and the response is printed.

#### Continuing the Conversation

```python
# User asks a follow-up question in the same thread
input_message = {"role": "user", "content": "what's my name?"}

# Stream the response from the graph in thread "1"
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

**Explanation:**
- The same `thread_id` is used, so the graph recalls that Bob was introduced earlier.
- The chatbot can now answer, â€œYour name is Bob,â€ because of the persisted state.

#### Starting a New Conversation (Thread ID "2")

```python
# For a new conversation, use a different thread_id
input_message = {"role": "user", "content": "what's my name?"}
new_config = {"configurable": {"thread_id": "2"}}

# Since this thread has no history, the bot won't remember any name
for chunk in graph.stream({"messages": [input_message]}, new_config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

**Explanation:**
- **New Thread:** By setting a different `thread_id` (here, `"2"`), we start a fresh conversation with no prior state.
- The bot will treat the input as if itâ€™s a new interaction, so it wonâ€™t know the userâ€™s name.

---

## ğŸ“ Recap

- **Thread-Level Persistence:** Allows your graph to store conversation history per thread.
- **MemorySaver:** A checkpointer that keeps state in memory, ensuring context is maintained between interactions.
- **Real-World Benefit:** Makes conversational AI applications more intuitive by remembering details (like user names or previous queries).

By incorporating persistence, your AI chat models can simulate more natural and engaging conversationsâ€”vital for customer support, personal assistants, and any application where continuity is key.

Feel free to experiment with these concepts in your own projects, and remember: every conversation is a chance to make your AI smarter and more human-like!