# 1. How to add thread-level persistence to your graph

Here's a detailed, beginner-friendly guide explaining how to add **thread-level persistence** to your LangGraph-based application. 🚀  

---

# 🧠 How to Add Thread-Level Persistence to Your Graph  

Many AI applications need memory to **retain context across multiple interactions**. Without persistence, every interaction is **stateless**, meaning the AI forgets previous conversations.  

LangGraph allows us to **store and retrieve conversation history** using **thread-level persistence**. This guide will cover:  
✅ Why persistence is important  
✅ How to implement it in **LangGraph**  
✅ Real-world examples  
✅ A complete **code breakdown**  

---

## 🏛️ **1. Why Do We Need Persistence?**  

Imagine you're building a **chatbot** that assists users with customer support. The chatbot should remember:  
✅ The user's name  
✅ Their past queries  
✅ Any details shared in previous messages  

Without persistence, the chatbot will **forget everything** after each message. 😞  

### **📌 Real-World Use Case**:  
Think of a **customer support chatbot** for an e-commerce website. If a customer asks:  
- "Where is my order?"  
- Later asks: "What is my order number?"  

The chatbot should **remember** the previous conversation to provide a relevant answer.  

---

## 🔧 **2. Setting Up LangGraph**  

### 🛠️ **Install Required Packages**  

Before we start coding, let's install **LangGraph** and the required dependencies:  

```python
!pip install --quiet -U langgraph langchain_anthropic
```

This installs:  
- **LangGraph** → Used to create AI-based workflows  
- **LangChain-Anthropic** → Allows us to use Claude models  

---

## 🔑 **3. Setting Up API Keys**  

Since we're using **Anthropic's Claude model**, we need to set up an API key for authentication.  

```python
import getpass
import os

def _set_env(var: str):
    """Set environment variable if it's not already set."""
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
```

### **🔍 Code Explanation:**  
- We use `getpass.getpass()` to **securely** input the API key.  
- The `_set_env` function ensures the key is set **only if it’s missing**.  
- This avoids **hardcoding** API keys into our script. 🔒  

---

## 🔄 **4. Defining the Chat Model**  

Now, let's define the AI model we’ll use:  

```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-5-sonnet-20240620")
```

### **🔍 Code Explanation:**  
- We import `ChatAnthropic` to interact with Claude models.  
- We initialize the model **"claude-3-5-sonnet-20240620"** for conversation processing.  

---

## 🏗️ **5. Creating the StateGraph**  

A **StateGraph** is a flowchart-like structure that processes messages.  

### **📌 Steps:**  
1️⃣ Define a function to call the AI model.  
2️⃣ Create a **graph** and add a node.  
3️⃣ Compile and execute the graph.  

#### **📝 Define the Model Node:**  

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, MessagesState, START

def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    return {"messages": response}
```

### **🔍 Code Explanation:**  
- `MessagesState` represents the **conversation history**.  
- `call_model(state)` takes user messages and gets a **response from the AI**.  

#### **📝 Create the Graph:**  

```python
builder = StateGraph(MessagesState)
builder.add_node("call_model", call_model)
builder.add_edge(START, "call_model")
graph = builder.compile()
```

### **🔍 Code Explanation:**  
- `StateGraph(MessagesState)` initializes a graph that **stores messages**.  
- `add_node("call_model", call_model)` adds a **node** to process AI responses.  
- `add_edge(START, "call_model")` ensures execution starts from `call_model`.  
- `compile()` finalizes the graph.  

---

## ❌ **6. Running Without Persistence (Forgetting Messages)**  

Let's test what happens **before adding persistence**:  

```python
input_message = {"role": "user", "content": "hi! I'm Bob"}
for chunk in graph.stream({"messages": [input_message]}, stream_mode="values"):
    chunk["messages"][-1].pretty_print()

input_message = {"role": "user", "content": "what's my name?"}
for chunk in graph.stream({"messages": [input_message]}, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

### **💡 Output (Before Persistence)**
```
Human: hi! I'm Bob
AI: Hello Bob! How can I assist you?

Human: what's my name?
AI: Sorry, I don't remember previous conversations.
```
😲 **Oops! The AI forgot Bob's name!**  

---

## 🧠 **7. Adding Thread-Level Persistence**  

We need to **save and recall messages** using a `Checkpointer`.  

#### **📝 Enable Memory:**  
```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
graph = builder.compile(checkpointer=memory)
```

### **🔍 Code Explanation:**  
- `MemorySaver()` creates a **memory checkpoint**.  
- `compile(checkpointer=memory)` enables **persistence**.  

---

## ✅ **8. Running With Persistence (Remembering Messages)**  

Now let's try the conversation **again**:  

```python
config = {"configurable": {"thread_id": "1"}}

input_message = {"role": "user", "content": "hi! I'm Bob"}
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()

input_message = {"role": "user", "content": "what's my name?"}
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

### **💡 Output (With Persistence)**
```
Human: hi! I'm Bob
AI: Hello Bob! How can I assist you?

Human: what's my name?
AI: Your name is Bob.
```
🎉 **Success! The AI remembers the user’s name.**  

---

## 🔄 **9. Starting a New Conversation (Forgetting Old Data)**  

If we want to start a **fresh conversation**, we can pass a **new thread ID**:  

```python
input_message = {"role": "user", "content": "what's my name?"}
for chunk in graph.stream(
    {"messages": [input_message]},
    {"configurable": {"thread_id": "2"}},
    stream_mode="values",
):
    chunk["messages"][-1].pretty_print()
```

### **💡 Output (New Thread)**
```
Human: what's my name?
AI: Sorry, I don't remember previous conversations.
```
**Poof!** 🪄 **All memories are gone for this new session.**  

---

# 🎯 **Conclusion**  

✅ **Without persistence**, the AI **forgets everything** between interactions.  
✅ **With `MemorySaver()`**, we store conversation history **thread-by-thread**.  
✅ **Changing `thread_id`** allows us to create separate conversations.  

---

# 📌 **Where Can You Use This?**  
✔️ AI **chatbots** (customer support, personal assistants)  
✔️ AI-powered **education tutors**  
✔️ AI **game characters** that remember player interactions  

---

# 🛠️ **Next Steps**  
🚀 Try different **memory-saving techniques** like databases.  
📌 Experiment with **LangGraph Cloud** for built-in persistence.  
🔍 Integrate **vector databases** for long-term AI memory.  

Want more deep-dive tutorials? Let me know! 🚀✨

---

# O3 mini Response:

# 🚀 Thread-Level Persistence in LangGraph

Thread-level persistence lets your AI application remember past interactions within a single conversation thread. This means that when a user interacts with your chatbot multiple times, the system can recall previous messages and context. Let’s break down the concept, see some code examples, and understand how you might use this in the real world.

---

## 💡 What Is Thread-Level Persistence?

When you chat with a virtual assistant or customer support bot, you expect it to remember details from earlier in the conversation. With **thread-level persistence**, the system stores the conversation state (like the messages exchanged) so that every new interaction in the same thread is aware of the previous context. 

- **Without Persistence:** Each message is treated as an isolated request.
- **With Persistence:** The chatbot “remembers” earlier messages, enabling more natural and coherent conversations.

---

## 🔧 Real-World Example

Imagine a customer service chatbot on an e-commerce website:
- **Without persistence:** The bot might ask for your name every time you ask about your order status.
- **With persistence:** Once you say, “Hi, I’m Alex,” the bot remembers your name. Later, when you ask, “What’s my name?” it confidently replies, “Your name is Alex!”

This makes interactions smoother and more human-like.

---

## 🛠️ Setting Up Your Environment

Before diving into the code, you need to install the required packages and set up your API keys. Here’s how you can do it:

```python
# Install required packages quietly without showing stderr output
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_anthropic

# Import modules to securely set API keys
import getpass
import os

# Define a helper function to set environment variables if not already set
def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

# Set your Anthropic API key for the LLM
_set_env("ANTHROPIC_API_KEY")
```

**Explanation:**
- **Package Installation:** The `%pip install` command installs `langgraph` and `langchain_anthropic` for interacting with AI models.
- **API Key Setup:** The `_set_env` function ensures your API key is securely set in your environment variables.

---

## 🏗️ Defining the Graph

Now, let’s define a simple graph that calls a chat model.

### 1. **Import and Initialize the Model**

```python
from langchain_anthropic import ChatAnthropic

# Initialize the chat model with a specific configuration
model = ChatAnthropic(model="claude-3-5-sonnet-20240620")
```

**Explanation:**
- **Importing:** We import `ChatAnthropic`, a client for interacting with Anthropic’s chat models.
- **Initializing:** We create an instance of the model. The parameter `"claude-3-5-sonnet-20240620"` selects a specific model version.

### 2. **Building the Graph**

```python
from langgraph.graph import StateGraph, MessagesState, START

# Define a function that the graph will use to call the model
def call_model(state: MessagesState):
    # Invoke the model using the current conversation messages
    response = model.invoke(state["messages"])
    # Return the response wrapped in a dictionary under "messages"
    return {"messages": response}

# Create a StateGraph with the initial state type (MessagesState)
builder = StateGraph(MessagesState)

# Add a node named "call_model" that uses our function
builder.add_node("call_model", call_model)

# Define an edge from the starting point to our node
builder.add_edge(START, "call_model")

# Compile the graph to prepare it for execution
graph = builder.compile()
```

**Explanation:**
- **StateGraph:** Represents the flow of our conversation. It holds the state (messages) and defines how to process them.
- **call_model Function:** 
  - **Input:** Receives the current conversation state (`state["messages"]`).
  - **Process:** Calls the chat model with the current messages.
  - **Output:** Returns a new state containing the model's response.
- **Graph Building:** 
  - **add_node:** Adds our processing function to the graph.
  - **add_edge:** Connects the start of the conversation to our node.
  - **compile:** Prepares the graph for use. At this stage, without persistence, each call resets the conversation state.

---

## 🔄 Adding Thread-Level Persistence

To remember past messages within the same conversation, we add persistence using a **checkpointer**.

### 1. **Integrate MemorySaver**

```python
from langgraph.checkpoint.memory import MemorySaver

# Create a MemorySaver instance that will handle state persistence
memory = MemorySaver()

# Re-compile the graph, now passing in our checkpointer
graph = builder.compile(checkpointer=memory)
```

**Explanation:**
- **MemorySaver:** This is a built-in checkpointer that saves and restores the conversation state in memory.
- **Passing the Checkpointer:** When compiling the graph with `checkpointer=memory`, the graph is now aware of past interactions. It stores the state per conversation thread.

### 2. **Using the Persistent Graph**

To interact with the graph and see persistence in action, use a configuration dictionary that includes a `thread_id`.

#### Starting a Conversation (Thread ID "1")

```python
# Configuration for the thread; thread_id "1" represents a specific conversation
config = {"configurable": {"thread_id": "1"}}

# User starts the conversation by introducing themselves
input_message = {"role": "user", "content": "hi! I'm bob"}

# Stream the response from the graph and print the latest message
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

**Explanation:**
- **Config Dictionary:** Includes a `thread_id` which uniquely identifies the conversation. Here, `"1"` is used for the first conversation.
- **Input Message:** The user introduces themselves.
- **Graph Stream:** The graph processes the input, and the response is printed.

#### Continuing the Conversation

```python
# User asks a follow-up question in the same thread
input_message = {"role": "user", "content": "what's my name?"}

# Stream the response from the graph in thread "1"
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

**Explanation:**
- The same `thread_id` is used, so the graph recalls that Bob was introduced earlier.
- The chatbot can now answer, “Your name is Bob,” because of the persisted state.

#### Starting a New Conversation (Thread ID "2")

```python
# For a new conversation, use a different thread_id
input_message = {"role": "user", "content": "what's my name?"}
new_config = {"configurable": {"thread_id": "2"}}

# Since this thread has no history, the bot won't remember any name
for chunk in graph.stream({"messages": [input_message]}, new_config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

**Explanation:**
- **New Thread:** By setting a different `thread_id` (here, `"2"`), we start a fresh conversation with no prior state.
- The bot will treat the input as if it’s a new interaction, so it won’t know the user’s name.

---

## 🎓 Recap

- **Thread-Level Persistence:** Allows your graph to store conversation history per thread.
- **MemorySaver:** A checkpointer that keeps state in memory, ensuring context is maintained between interactions.
- **Real-World Benefit:** Makes conversational AI applications more intuitive by remembering details (like user names or previous queries).

By incorporating persistence, your AI chat models can simulate more natural and engaging conversations—vital for customer support, personal assistants, and any application where continuity is key.

Feel free to experiment with these concepts in your own projects, and remember: every conversation is a chance to make your AI smarter and more human-like!