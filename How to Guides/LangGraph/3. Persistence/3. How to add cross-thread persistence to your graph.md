# 3. How to add cross-thread persistence to your graph

Here's a well-structured and visually appealing explanation of **"How to Add Cross-Thread Persistence to Your Graph"** with clear explanations, real-world applications, code examples, and detailed explanations of each line of code.  

---

# 🧠 How to Add Cross-Thread Persistence to Your Graph  

## 🚀 Introduction  
When building AI-powered chat applications, it's important to **remember user data across different conversation threads**. For example, a chatbot should remember a user's **name, preferences, or previous conversations**, even if they start a new chat session.  

💡 **LangGraph** provides a way to persist information across multiple interactions using the **Store API**, allowing a chatbot to store and retrieve user-specific data.  

### 🏗️ **Real-World Use Case**  
Imagine a **customer support chatbot**:  
- A user named **Bob** chats with the bot and says, *"Remember: my name is Bob."*  
- Later, Bob starts a **new conversation** and asks, *"What is my name?"*  
- Instead of forgetting the user's name, the bot retrieves it from memory and responds, *"Your name is Bob."*  

Now, let's implement this using **LangGraph**.  

---

# 🛠️ **Setup: Installing Required Packages**  
First, install the necessary dependencies.  

```python
%%capture --no-stderr
%pip install -U langchain_openai langgraph
```

### 🔑 **Setting Up API Keys**  
We need API keys for **Anthropic AI** and **OpenAI** for language model processing.  

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
_set_env("OPENAI_API_KEY")
```
🔹 **What this does?**  
- The `_set_env` function ensures the API keys are set as environment variables before running our code.  

---

# 🏗️ **Defining an In-Memory Store**  
We'll use **InMemoryStore** to temporarily store user memories.

```python
from langgraph.store.memory import InMemoryStore
from langchain_openai import OpenAIEmbeddings

in_memory_store = InMemoryStore(
    index={
        "embed": OpenAIEmbeddings(model="text-embedding-3-small"),
        "dims": 1536,
    }
)
```

🔹 **Explanation:**  
- `InMemoryStore` creates a **temporary storage** where user data can be stored and retrieved.  
- `OpenAIEmbeddings` allows us to **embed user messages**, making them searchable later.  

---

# 🔄 **Creating the Graph**  
Now, we define how the chatbot will interact with users while remembering previous interactions.

### 🛠️ **Import Required Libraries**  
```python
import uuid
from typing import Annotated
from typing_extensions import TypedDict

from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.base import BaseStore
```

🔹 **Why do we need these imports?**  
- `ChatAnthropic`: The AI model that generates responses.  
- `StateGraph`: Manages chatbot states.  
- `MemorySaver`: Saves chatbot history.  
- `BaseStore`: Base class for storing and retrieving user information.  

---

## 🤖 **Defining the Chatbot Logic**  
We'll define a function that processes user messages and stores important information.

```python
model = ChatAnthropic(model="claude-3-5-sonnet-20240620")

def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):
    user_id = config["configurable"]["user_id"]
    namespace = ("memories", user_id)
    
    # Retrieve past user messages from the store
    memories = store.search(namespace, query=str(state["messages"][-1].content))
    info = "\n".join([d.value["data"] for d in memories])
    
    system_msg = f"You are a helpful assistant talking to the user. User info: {info}"

    # Store new memories if the user asks to remember something
    last_message = state["messages"][-1]
    if "remember" in last_message.content.lower():
        memory = "User name is Bob"
        store.put(namespace, str(uuid.uuid4()), {"data": memory})

    # Generate AI response
    response = model.invoke(
        [{"role": "system", "content": system_msg}] + state["messages"]
    )
    return {"messages": response}
```

🔹 **Breaking it down:**  
1. **Retrieving stored data:**  
   - `store.search(namespace, query=str(state["messages"][-1].content))`  
   - Retrieves previous interactions for the **specific user**.  
2. **Processing user input:**  
   - If the message contains `"remember"`, we store `"User name is Bob"`.  
3. **Generating a response:**  
   - AI model generates a reply using **ChatAnthropic**.  

---

## 🏗️ **Building the Conversation Graph**  
Now, let's define how the chatbot will process messages.

```python
builder = StateGraph(MessagesState)
builder.add_node("call_model", call_model)
builder.add_edge(START, "call_model")

graph = builder.compile(checkpointer=MemorySaver(), store=in_memory_store)
```

🔹 **Explanation:**  
- `StateGraph(MessagesState)`: Defines the structure of the chatbot.  
- `add_node("call_model", call_model)`: Connects the chatbot logic.  
- `compile(checkpointer=MemorySaver(), store=in_memory_store)`: Saves chat history.  

---

# 🚀 **Running the Graph**  
Let's test the chatbot!

### 🧑 **User Interaction 1: Remembering Name**
```python
config = {"configurable": {"thread_id": "1", "user_id": "1"}}
input_message = {"role": "user", "content": "Hi! Remember: my name is Bob"}
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

📝 **Expected Output:**  
```
================================ Human Message ================================
Hi! Remember: my name is Bob
================================== Ai Message ================================
Hello Bob! It's nice to meet you. I'll remember that your name is Bob. How can I assist you today?
```

### 🧑 **User Interaction 2: Asking for Name**
```python
config = {"configurable": {"thread_id": "2", "user_id": "1"}}
input_message = {"role": "user", "content": "What is my name?"}
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

📝 **Expected Output:**  
```
================================ Human Message ================================
What is my name?
================================== Ai Message ================================
Your name is Bob.
```

### 🧑 **New User: No Stored Data**
```python
config = {"configurable": {"thread_id": "3", "user_id": "2"}}
input_message = {"role": "user", "content": "What is my name?"}
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

📝 **Expected Output:**  
```
================================ Human Message ================================
What is my name?
================================== Ai Message ================================
I apologize, but I don't have any information about your name.
```

---

# 🎯 **Key Takeaways**
✅ **Cross-thread persistence** allows AI models to remember user data.  
✅ **InMemoryStore** helps in storing and retrieving user-specific information.  
✅ **LangGraph** makes it easy to build structured conversation graphs.  
✅ **Real-world applications** include customer support chatbots, virtual assistants, and AI-powered recommendation systems.  

---

# 🏆 **Final Thoughts**
With **LangGraph's Store API**, we can now build **smart AI applications** that remember users across multiple interactions. This enables **personalized conversations** and **better user experiences**.  

🚀 **Next Steps:**  
- Try **storing more user preferences** (e.g., favorite products, locations).  
- Use **a persistent database** instead of `InMemoryStore`.  
- Integrate with **LangGraph Cloud** for production-ready deployments.  

💡 **Would you like to learn more about storing chatbot memory using a database like Redis or MongoDB? Let me know!** 🚀


---

# O3 mini Response:

# 🚀 Cross-Thread Persistence in LangGraph

Cross-thread persistence allows your graph to share and reuse data (like user information or preferences) **across multiple conversation threads**. This is especially useful when you want your AI assistant to remember details about a user, even if the conversation starts a new thread. In this guide, we'll see how to build a graph that leverages shared memory using the **Store interface**.

---

## 🌟 What Is Cross-Thread Persistence?

When working with AI applications, sometimes you need to remember information about a user across different interactions—even if those interactions happen in separate threads. With **cross-thread persistence**, you can store user data (like names, preferences, etc.) in a shared memory space, so that every thread can access and update this information. 

### Real-World Example

Imagine a customer support chatbot that keeps track of a user's previous orders or preferences. If a customer starts a new conversation (a new thread), the chatbot can still recall their order history or preferences from shared memory, offering a personalized experience.

---

## 🛠️ Setup: Installing Required Packages and Setting API Keys

First, install the required packages and set up your API keys. This step ensures that you have everything needed to run your LangGraph application.

```python
%%capture --no-stderr
%pip install -U langchain_openai langgraph

import getpass
import os

# Helper function to securely set environment variables if not already set
def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

# Set your API keys for Anthropic and OpenAI
_set_env("ANTHROPIC_API_KEY")
_set_env("OPENAI_API_KEY")
```

**Explanation:**

- **Package Installation:**  
  - The `%%capture --no-stderr` magic ensures installation messages don't clutter the output.  
  - `%pip install -U langchain_openai langgraph` installs the necessary libraries.
  
- **API Keys Setup:**  
  - The helper function `_set_env` prompts for the API keys if they are not already set in the environment.  
  - We set keys for both Anthropic and OpenAI as they might be used in your graph.

---

## 🗄️ Define the In-Memory Store

We create an **InMemoryStore** that will hold our shared memories. The store uses an embedding model to index the data, making it searchable.

```python
from langgraph.store.memory import InMemoryStore
from langchain_openai import OpenAIEmbeddings

# Create an in-memory store with an embedding index
in_memory_store = InMemoryStore(
    index={
        "embed": OpenAIEmbeddings(model="text-embedding-3-small"),  # Embedding model for text
        "dims": 1536,  # Dimensionality of the embedding space
    }
)
```

**Explanation:**

- **Importing:**  
  - We import `InMemoryStore` from `langgraph.store.memory` to create a memory-based store.
  - `OpenAIEmbeddings` is imported to provide text embeddings.

- **Creating the Store:**  
  - `in_memory_store` is initialized with an index configuration:
    - `"embed": OpenAIEmbeddings(...)` defines the embedding model.
    - `"dims": 1536` specifies the dimensions for the embedding vectors.

---

## 🔨 Create the Graph with Cross-Thread Persistence

Now, let's build a graph that leverages this shared store. In our graph, the node function will:
- Retrieve stored memories for a user.
- Update the memory if the user asks to remember something.
- Construct a system message that includes the retrieved user information.

```python
import uuid
from typing import Annotated
from typing_extensions import TypedDict

from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.base import BaseStore

# Initialize the chat model (Anthropic)
model = ChatAnthropic(model="claude-3-5-sonnet-20240620")

# Define the node function that calls the chat model and uses the shared store
def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):
    # Extract the user_id from the config to identify the user
    user_id = config["configurable"]["user_id"]
    
    # Define the namespace for this user's memories
    namespace = ("memories", user_id)
    
    # Search for memories in the store based on the content of the last message
    memories = store.search(namespace, query=str(state["messages"][-1].content))
    
    # Combine found memories into a single string (each memory has a 'data' field)
    info = "\n".join([d.value["data"] for d in memories])
    
    # Construct a system message that includes the user info
    system_msg = f"You are a helpful assistant talking to the user. User info: {info}"
    
    # Check if the last message instructs to "remember" something
    last_message = state["messages"][-1]
    if "remember" in last_message.content.lower():
        # Define new memory data; here we hard-code the memory for simplicity
        memory = "User name is Bob"
        # Store the new memory in the user's namespace with a random key (UUID)
        store.put(namespace, str(uuid.uuid4()), {"data": memory})
    
    # Invoke the chat model, prepending the system message to the conversation
    response = model.invoke(
        [{"role": "system", "content": system_msg}] + state["messages"]
    )
    # Return the response wrapped in the 'messages' key
    return {"messages": response}

# Build the graph with a single node calling our function
builder = StateGraph(MessagesState)
builder.add_node("call_model", call_model)
builder.add_edge(START, "call_model")

# Compile the graph with a memory checkpointer and the shared store
graph = builder.compile(checkpointer=MemorySaver(), store=in_memory_store)
```

**Explanation:**

- **Imports and Model Initialization:**
  - `uuid` is used to generate unique keys when storing new memories.
  - `ChatAnthropic` initializes the chat model.
  - `StateGraph`, `MessagesState`, and `START` are used to build the graph.
  - `MemorySaver` provides thread-level persistence for the conversation.
  - `BaseStore` is the type for our shared store.
  
- **`call_model` Function:**
  - **Extracting User ID:**  
    ```python
    user_id = config["configurable"]["user_id"]
    ```
    Retrieves the user identifier from the config. This helps in segregating memories by user.

  - **Setting the Namespace:**  
    ```python
    namespace = ("memories", user_id)
    ```
    Uses a tuple to create a unique namespace for each user.

  - **Searching for Memories:**  
    ```python
    memories = store.search(namespace, query=str(state["messages"][-1].content))
    ```
    Searches the store for memories related to the content of the last message.

  - **Formatting Retrieved Info:**  
    ```python
    info = "\n".join([d.value["data"] for d in memories])
    ```
    Aggregates the data from all matching memories into a single string.

  - **Creating the System Message:**  
    ```python
    system_msg = f"You are a helpful assistant talking to the user. User info: {info}"
    ```
    Constructs a message that provides context to the assistant based on stored memories.

  - **Storing New Memory:**  
    ```python
    if "remember" in last_message.content.lower():
        memory = "User name is Bob"
        store.put(namespace, str(uuid.uuid4()), {"data": memory})
    ```
    Checks if the last user message includes the word "remember." If so, a new memory is stored under the user's namespace.

  - **Calling the Chat Model:**  
    ```python
    response = model.invoke(
        [{"role": "system", "content": system_msg}] + state["messages"]
    )
    ```
    Prepares the conversation by inserting the system message and then invoking the chat model with the updated message history.

- **Graph Construction and Compilation:**
  - We create a `StateGraph` using `MessagesState`.
  - Add the `call_model` node and define the flow from `START` to `call_model`.
  - Compile the graph with a `MemorySaver` (for thread-level persistence) and pass the shared `in_memory_store`.

---

## 🎬 Run the Graph and Test Cross-Thread Persistence

Now, we can run our graph with different configurations to see how shared memory works across threads.

### **First Interaction (User 1, Thread 1): Store Memory**

```python
config = {"configurable": {"thread_id": "1", "user_id": "1"}}
input_message = {"role": "user", "content": "Hi! Remember: my name is Bob"}

# Run the graph and print the latest response message
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

**What Happens Here:**

- The user (with `user_id: "1"`) sends a message asking the assistant to remember their name.
- The node function detects the keyword "remember" and stores `"User name is Bob"` in the shared store under the namespace `("memories", "1")`.
- The assistant responds, acknowledging Bob as the user's name.

### **Second Interaction (User 1, New Thread): Retrieve Memory**

```python
config = {"configurable": {"thread_id": "2", "user_id": "1"}}
input_message = {"role": "user", "content": "what is my name?"}

# Run the graph with a new thread but same user_id; it should recall the memory.
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

**What Happens Here:**

- Although this is a new thread (`thread_id: "2"`), the `user_id` remains `"1"`.
- The graph uses the shared store to retrieve the memory, so the assistant correctly responds: "Your name is Bob."

### **Third Interaction (Different User): No Memory Retrieved**

```python
config = {"configurable": {"thread_id": "3", "user_id": "2"}}
input_message = {"role": "user", "content": "what is my name?"}

# Run the graph for a different user; since no memory is stored for user_id "2", the assistant won't recall any name.
for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

**What Happens Here:**

- For `user_id: "2"`, there is no stored memory.
- The assistant responds that it doesn't have any information about the user's name.

### **Inspecting the In-Memory Store**

You can also inspect the stored memories to verify that data is maintained correctly.

```python
# Print all memories stored under the namespace for user_id "1"
for memory in in_memory_store.search(("memories", "1")):
    print(memory.value)
```

**Expected Output:**

```python
{'data': 'User name is Bob'}
```

---

## 🚀 Final Thoughts

- **Cross-Thread Persistence** enables your LangGraph application to share data across different conversation threads by using a common store.
- **Shared Memory** (via the Store API) ensures that user-specific information is maintained and retrievable, even when a new thread begins.
- **Real-World Impact:** This is especially useful in scenarios like personalized chatbots, where remembering user details across sessions improves the user experience.

By following these steps, you can build a robust AI application that leverages cross-thread persistence to deliver a truly personalized experience. Happy coding!