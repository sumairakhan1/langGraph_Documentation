# 3. How to add runtime configuration to your graph

# 📌 How to Add Runtime Configuration to Your Graph

## 🔧 Setup
Before we start, let's install the necessary packages and set up API keys.

### 📌 Install Required Packages
We need to install `langgraph` and `langchain_anthropic` to build our graph-based LLM application.

```python
%%capture --no-stderr
%pip install -U langgraph langchain_anthropic
```

### 🔑 Set Up API Keys
We must set up the API key for Anthropic to use its LLM services.

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
```

## 🚀 Defining the Graph
Now, let's define a simple LangGraph model using Anthropic's `claude-2.1`.

### 📌 Import Required Modules
```python
import operator
from typing import Annotated, Sequence
from typing_extensions import TypedDict

from langchain_anthropic import ChatAnthropic
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph import END, StateGraph, START
```

### 🏗️ Create the Model and State
```python
model = ChatAnthropic(model_name="claude-2.1")

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
```

### 🏗️ Define the Function to Call the Model
```python
def _call_model(state):
    response = model.invoke(state["messages"])
    return {"messages": [response]}
```

### 🌐 Build the Graph
```python
builder = StateGraph(AgentState)
builder.add_node("model", _call_model)
builder.add_edge(START, "model")
builder.add_edge("model", END)

graph = builder.compile()
```

## 🔧 Configuring the Graph at Runtime
We can extend our graph to allow runtime selection of different LLMs by passing a configuration.

### 📌 Define Multiple LLMs
```python
from langchain_openai import ChatOpenAI
from typing import Optional
from langchain_core.runnables.config import RunnableConfig

openai_model = ChatOpenAI()

models = {
    "anthropic": model,
    "openai": openai_model,
}
```

### 📌 Modify the Model Function to Accept Configuration
```python
def _call_model(state: AgentState, config: RunnableConfig):
    model_name = config["configurable"].get("model", "anthropic")
    model = models[model_name]
    response = model.invoke(state["messages"])
    return {"messages": [response]}
```

### 🌐 Rebuild the Graph with Config Support
```python
builder = StateGraph(AgentState)
builder.add_node("model", _call_model)
builder.add_edge(START, "model")
builder.add_edge("model", END)

graph = builder.compile()
```

### 🔄 Using Runtime Configuration
#### ✅ Without Configuration (Defaults to Anthropic)
```python
graph.invoke({"messages": [HumanMessage(content="hi")]})
```

#### ✅ Using OpenAI Instead of Anthropic
```python
config = {"configurable": {"model": "openai"}}
graph.invoke({"messages": [HumanMessage(content="hi")]}, config=config)
```

## 🛠️ Extending Configuration for System Messages
We can further modify our graph to allow system messages for more control over responses.

### 📌 Define a Configuration Schema
```python
from langchain_core.messages import SystemMessage

class ConfigSchema(TypedDict):
    model: Optional[str]
    system_message: Optional[str]
```

### 📌 Modify the Model Function to Accept System Messages
```python
def _call_model(state: AgentState, config: RunnableConfig):
    model_name = config["configurable"].get("model", "anthropic")
    model = models[model_name]
    messages = state["messages"]
    
    if "system_message" in config["configurable"]:
        messages = [SystemMessage(content=config["configurable"]["system_message"])] + messages
    
    response = model.invoke(messages)
    return {"messages": [response]}
```

### 🌐 Rebuild the Graph with System Message Support
```python
workflow = StateGraph(AgentState, ConfigSchema)
workflow.add_node("model", _call_model)
workflow.add_edge(START, "model")
workflow.add_edge("model", END)

graph = workflow.compile()
```

### 🔄 Using System Message Configuration
#### ✅ Without Configuration
```python
graph.invoke({"messages": [HumanMessage(content="hi")]})
```

#### ✅ With System Message (Respond in Italian)
```python
config = {"configurable": {"system_message": "respond in Italian"}}
graph.invoke({"messages": [HumanMessage(content="hi")]}, config=config)
```

## 🏗️ Real-World Applications
1. **Chatbots**: Implementing dynamic configurations to switch between different LLMs for enhanced user experience.
2. **AI Assistants**: Adapting responses based on runtime settings like language preferences or response styles.
3. **Enterprise Solutions**: Providing multiple model support to meet compliance and cost-effectiveness.

## 🎯 Summary
- We built a **graph-based LLM application** using LangGraph.
- We configured runtime **LLM selection** (Anthropic & OpenAI).
- We extended the graph to support **system messages**.
- We demonstrated **real-world applications** for this approach.

This setup allows **dynamic control** over an AI agent’s behavior, making it highly adaptable for various AI-driven applications.

---

# O3 mini Response

---

# 🚀 Adding Runtime Configuration to Your Graph

In this guide, we'll explore how to add runtime configuration to your LangGraph workflow. Runtime configuration allows you to dynamically change certain parameters of your graph (such as selecting different language models or injecting system messages) without altering the core state. This is especially useful when you need to adjust behavior based on external settings or user input.

Below, you'll find a detailed explanation along with code examples and real-world applications.

---

## 🛠️ Setup: Installing Packages & Setting API Keys

Before building our graph, we need to install the required packages and configure our environment. This ensures that our code can connect to the necessary language models.

```python
%%capture --no-stderr
%pip install -U langgraph langchain_anthropic
```

**Explanation:**

- **`%%capture --no-stderr`**  
  This Jupyter magic command hides the installation output, making your notebook output cleaner.
  
- **`%pip install -U langgraph langchain_anthropic`**  
  This command installs (or updates) the `langgraph` and `langchain_anthropic` packages which we will use to build our graph and interact with the Anthropic model.

Next, we set up our API keys:

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
```

**Explanation:**

- **`import getpass, os`**  
  These libraries help us securely get input from the user and access environment variables.
  
- **`def _set_env(var: str):`**  
  This function checks if an environment variable (like `ANTHROPIC_API_KEY`) exists.
  
- **`if not os.environ.get(var):`**  
  If the variable is not set, it prompts the user to enter the API key securely.
  
- **`_set_env("ANTHROPIC_API_KEY")`**  
  We call the function to ensure our Anthropic API key is available.

---

## 📚 Define a Simple Graph

Let's create a very basic graph that will use a language model to process messages. In this graph, the state contains a list of messages, and our node will call the model to generate a response.

```python
import operator
from typing import Annotated, Sequence
from typing_extensions import TypedDict

from langchain_anthropic import ChatAnthropic
from langchain_core.messages import BaseMessage, HumanMessage

from langgraph.graph import END, StateGraph, START

# Initialize the Anthropic model with a specific model name.
model = ChatAnthropic(model_name="claude-2.1")

# Define our graph's state using a TypedDict.
class AgentState(TypedDict):
    # We use operator.add to combine messages from different nodes.
    messages: Annotated[Sequence[BaseMessage], operator.add]

# Define a function that calls the language model.
def _call_model(state: AgentState):
    # Extract the messages from the state.
    state["messages"]
    # Call the model and get a response based on the current messages.
    response = model.invoke(state["messages"])
    # Return the new state with the response appended.
    return {"messages": [response]}

# Build the graph by adding nodes and edges.
builder = StateGraph(AgentState)
builder.add_node("model", _call_model)
builder.add_edge(START, "model")
builder.add_edge("model", END)

# Compile the graph to create an executable application.
graph = builder.compile()
```

**Explanation (Line-by-Line):**

- **Model Initialization:**  
  ```python
  model = ChatAnthropic(model_name="claude-2.1")
  ```  
  We instantiate a language model (Anthropic's Claude) by providing its model name.

- **State Definition:**  
  ```python
  class AgentState(TypedDict):
      messages: Annotated[Sequence[BaseMessage], operator.add]
  ```  
  This defines the graph state to include a sequence of messages. The `operator.add` reducer ensures that when messages from multiple nodes are combined, they concatenate as expected.

- **Model Call Node:**  
  ```python
  def _call_model(state: AgentState):
      state["messages"]
      response = model.invoke(state["messages"])
      return {"messages": [response]}
  ```  
  - **`state["messages"]`** retrieves the current messages.
  - **`model.invoke(state["messages"])`** calls the language model with the messages.
  - **Return Value:** The function returns a new state containing the model's response.

- **Graph Building:**  
  ```python
  builder = StateGraph(AgentState)
  builder.add_node("model", _call_model)
  builder.add_edge(START, "model")
  builder.add_edge("model", END)
  ```  
  We create a graph, add our node, and specify the flow from start to our node and then to the end.

- **Compilation:**  
  ```python
  graph = builder.compile()
  ```  
  The graph is compiled into an executable application.

---

## ⚙️ Configure the Graph for Runtime Options

Now, let's extend our graph so that the user can choose from multiple language models at runtime. We can achieve this by passing a configuration that isn’t part of the core state.

### Step 1: Define Multiple Models

```python
from langchain_openai import ChatOpenAI
from typing import Optional
from langchain_core.runnables.config import RunnableConfig

# Initialize an OpenAI model.
openai_model = ChatOpenAI()

# A dictionary to store our available models.
models = {
    "anthropic": model,   # Default Anthropic model from above.
    "openai": openai_model,
}
```

**Explanation:**

- **`ChatOpenAI`** is imported and initialized to represent an alternative model.
- **`models` dictionary** maps model names to their respective model objects.

### Step 2: Modify the Node to Accept Configuration

```python
def _call_model(state: AgentState, config: RunnableConfig):
    # Access the configuration to select the model. Defaults to "anthropic" if not provided.
    model_name = config["configurable"].get("model", "anthropic")
    model = models[model_name]
    response = model.invoke(state["messages"])
    return {"messages": [response]}
```

**Explanation:**

- **`config: RunnableConfig`** is a new parameter to our function.
- **`config["configurable"].get("model", "anthropic")`**  
  Retrieves the model name from the configuration. If not provided, it defaults to `"anthropic"`.
- **Model Selection:**  
  The chosen model is used to invoke the language model.
- **Return:**  
  The function returns the updated state with the response.

### Step 3: Rebuild the Graph with Configurable Options

```python
# Define a new graph using the updated node.
builder = StateGraph(AgentState)
builder.add_node("model", _call_model)
builder.add_edge(START, "model")
builder.add_edge("model", END)

# Compile the updated graph.
graph = builder.compile()
```

**Explanation:**

- We rebuild the graph similarly as before but now our node `_call_model` accepts configuration options.

### Step 4: Invoking the Graph with and without Configuration

**Without configuration (default model is used):**

```python
from langchain_core.messages import HumanMessage

result_default = graph.invoke({"messages": [HumanMessage(content="hi")]})
print(result_default)
```

**With configuration (using the OpenAI model):**

```python
config = {"configurable": {"model": "openai"}}
result_openai = graph.invoke({"messages": [HumanMessage(content="hi")]}, config=config)
print(result_openai)
```

**Explanation:**

- **Without Config:**  
  The graph uses the default `"anthropic"` model.
  
- **With Config:**  
  We pass a configuration dictionary that tells the graph to use the `"openai"` model instead.

---

## 🌟 Extending the Configuration: Adding a System Message

We can further extend our runtime configuration to include other options, such as injecting a system message. This is useful when you want to set a specific context or behavior for the language model.

### Code Example: Adding a System Message

```python
from langchain_core.messages import SystemMessage

# Define a configuration schema to indicate available configuration options.
class ConfigSchema(TypedDict):
    model: Optional[str]
    system_message: Optional[str]

def _call_model(state: AgentState, config: RunnableConfig):
    # Determine which model to use, defaulting to "anthropic".
    model_name = config["configurable"].get("model", "anthropic")
    model = models[model_name]
    
    messages = state["messages"]
    # If a system message is provided, prepend it to the list of messages.
    if "system_message" in config["configurable"]:
        messages = [SystemMessage(content=config["configurable"]["system_message"])] + messages
        
    response = model.invoke(messages)
    return {"messages": [response]}

# Define a new graph that supports additional configuration.
workflow = StateGraph(AgentState, ConfigSchema)
workflow.add_node("model", _call_model)
workflow.add_edge(START, "model")
workflow.add_edge("model", END)

# Compile the graph.
graph = workflow.compile()
```

**Explanation:**

- **`ConfigSchema`**:  
  A TypedDict that outlines our optional configuration fields (`model` and `system_message`).

- **System Message Injection:**  
  Inside `_call_model`, if the configuration contains a `"system_message"`, it is added as the first message. This sets a context for the language model.

- **Graph Building:**  
  We pass the `ConfigSchema` when creating the `StateGraph`, making our configuration explicit and type-checked.

### Invoking the Graph with a System Message

```python
# Invocation without a system message.
result_default = graph.invoke({"messages": [HumanMessage(content="hi")]})
print(result_default)

# Invocation with a system message (e.g., instructing the model to respond in Italian).
config = {"configurable": {"system_message": "respond in italian"}}
result_system = graph.invoke({"messages": [HumanMessage(content="hi")]}, config=config)
print(result_system)
```

**Explanation:**

- **Default Invocation:**  
  Runs the graph without additional configuration, using default behaviors.
  
- **Configured Invocation:**  
  Passes a system message in the configuration. The model receives this system prompt first, guiding its response accordingly.

---

## 🌍 Real-World Example: Dynamic Chatbot Behavior

Imagine you’re building a customer support chatbot that must adapt its behavior based on the time of day or current promotions:

- **Default Behavior:**  
  The chatbot uses a standard model to answer FAQs.
  
- **Special Behavior:**  
  During a promotional event, you might want the chatbot to use a specific system message like "Offer details: Get 20% off on all items!" This can be injected via the runtime configuration without changing the chatbot’s underlying code.

By dynamically configuring the chatbot, you can easily adjust its responses and behavior according to context and business needs.

---

## 💡 Key Takeaways

- **Runtime Configuration:**  
  Allows you to adjust your graph's behavior on-the-fly without modifying the core state.
  
- **Separation of Concerns:**  
  The configuration holds data that influences the processing (like choosing a model or setting a system message) but isn’t part of the main input state.
  
- **Flexibility:**  
  This pattern is ideal for scenarios where behavior needs to change dynamically, such as in multi-model applications or context-aware chatbots.

By understanding and applying runtime configuration in your graphs, you can create highly flexible and adaptive workflows that cater to diverse requirements and real-world use cases.

Happy coding!