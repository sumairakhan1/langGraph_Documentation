# 🛠️ **Fine-Grained Control: Creating Map-Reduce Branches for Parallel Execution**  

Map-reduce is a powerful technique that allows efficient parallel processing by dividing a large task into smaller sub-tasks, executing them concurrently, and combining the results. This is particularly useful in workflows that involve generating multiple outputs based on an input and then consolidating them to extract meaningful insights.

---

## 🔎 **Real-World Application of Map-Reduce**  
A **real-world example** of this technique is sentiment analysis in customer feedback. Suppose a business receives thousands of customer reviews daily. Instead of processing them sequentially, we can:
1. **Divide** the reviews into smaller batches.
2. **Process each batch in parallel** to extract sentiment (positive, negative, neutral).
3. **Aggregate** the results to compute overall sentiment trends.  

This reduces processing time and enhances efficiency.

---

# 🏗️ **Understanding Parallel Execution in LangGraph**
### 📌 **Key Challenges in Parallel Processing**
When implementing map-reduce using **LangGraph**, we face two challenges:
1. **Unknown Number of Sub-Tasks:** The number of parallel executions is not always fixed in advance.
2. **Managing State for Each Task:** Each task requires its own specific input while maintaining a shared state.

LangGraph solves these using the **Send API**, which dynamically creates branches and distributes different states to multiple nodes.

---

## 🔧 **Step 1: Setting Up the Environment**  
First, install the required libraries and set up the API key.

```python
%%capture --no-stderr
%pip install -U langchain-anthropic langgraph

import os
import getpass

def _set_env(name: str):
    if not os.getenv(name):
        os.environ[name] = getpass.getpass(f"{name}: ")

_set_env("ANTHROPIC_API_KEY")
```
### 🔍 **Explanation**
- We install the required packages (`langchain-anthropic`, `langgraph`).
- `_set_env` function ensures the **API key** is securely stored before making API calls.

---

## 🌐 **Step 2: Defining the Graph and Models**
We use **Pydantic v2** to structure data and define how our graph nodes interact.

### **Define Prompts**
```python
subjects_prompt = """Generate a comma-separated list of between 2 and 5 examples related to: {topic}."""
joke_prompt = """Generate a joke about {subject}"""
best_joke_prompt = """Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one.

{jokes}"""
```
### 🔍 **Explanation**
- **`subjects_prompt`**: Generates a list of related topics.
- **`joke_prompt`**: Creates a joke for each topic.
- **`best_joke_prompt`**: Selects the best joke.

---

### **Create Data Models**
```python
from pydantic import BaseModel, Field

class Subjects(BaseModel):
    subjects: list[str]

class Joke(BaseModel):
    joke: str

class BestJoke(BaseModel):
    id: int = Field(description="Index of the best joke, starting with 0", ge=0)
```
### 🔍 **Explanation**
- **`Subjects`**: Stores a list of generated topics.
- **`Joke`**: Represents a joke for each subject.
- **`BestJoke`**: Holds the ID of the best joke.

---

## 🔁 **Step 3: Defining the State**
We define different states for our graph, ensuring parallel execution.

### **Define Overall Graph State**
```python
import operator
from typing import Annotated
from typing_extensions import TypedDict

class OverallState(TypedDict):
    topic: str
    subjects: list
    jokes: Annotated[list, operator.add]  # Reduce step
    best_selected_joke: str
```
### 🔍 **Explanation**
- **`topic`**: The main topic provided by the user.
- **`subjects`**: List of related subjects (e.g., for jokes).
- **`jokes`**: List of jokes, aggregated using `operator.add` (reduce step).
- **`best_selected_joke`**: Stores the best joke.

---

### **Define Sub-Task State**
```python
class JokeState(TypedDict):
    subject: str
```
### 🔍 **Explanation**
- **`JokeState`**: Represents the state for each parallel joke generation task.

---

## 🎭 **Step 4: Creating Functions for Each Node**
Each function represents a node in the graph.

### **Generate Related Topics**
```python
def generate_topics(state: OverallState):
    prompt = subjects_prompt.format(topic=state["topic"])
    response = model.with_structured_output(Subjects).invoke(prompt)
    return {"subjects": response.subjects}
```
### 🔍 **Explanation**
- Formats the prompt with the user's topic.
- Calls the language model to generate related subjects.
- Returns the subjects as a list.

---

### **Generate a Joke for Each Subject**
```python
def generate_joke(state: JokeState):
    prompt = joke_prompt.format(subject=state["subject"])
    response = model.with_structured_output(Joke).invoke(prompt)
    return {"jokes": [response.joke]}
```
### 🔍 **Explanation**
- Formats the joke prompt with the subject.
- Calls the language model to generate a joke.
- Returns a list containing the joke.

---

### **Map Function to Distribute Tasks**
```python
from langgraph.types import Send

def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"subject": s}) for s in state["subjects"]]
```
### 🔍 **Explanation**
- Distributes each subject to **separate parallel executions** of the `generate_joke` function.

---

### **Select the Best Joke**
```python
def best_joke(state: OverallState):
    jokes = "\n\n".join(state["jokes"])
    prompt = best_joke_prompt.format(topic=state["topic"], jokes=jokes)
    response = model.with_structured_output(BestJoke).invoke(prompt)
    return {"best_selected_joke": state["jokes"][response.id]}
```
### 🔍 **Explanation**
- Combines all jokes.
- Calls the model to select the best joke.
- Returns the best joke.

---

## 🛠️ **Step 5: Building the Graph**
Now, we construct the graph using **LangGraph**.

```python
from langgraph.graph import StateGraph, START, END

graph = StateGraph(OverallState)

graph.add_node("generate_topics", generate_topics)
graph.add_node("generate_joke", generate_joke)
graph.add_node("best_joke", best_joke)

graph.add_edge(START, "generate_topics")
graph.add_conditional_edges("generate_topics", continue_to_jokes, ["generate_joke"])
graph.add_edge("generate_joke", "best_joke")
graph.add_edge("best_joke", END)

app = graph.compile()
```
### 🔍 **Explanation**
- **`StateGraph(OverallState)`**: Creates a graph with `OverallState` as the main state.
- **`add_node`**: Defines nodes (`generate_topics`, `generate_joke`, `best_joke`).
- **`add_edge`**: Connects nodes to define execution flow.
- **`add_conditional_edges`**: Maps multiple subjects to parallel joke generation.

---

## 🚀 **Step 6: Running the Graph**
Now, we execute the graph with a topic.

```python
for s in app.stream({"topic": "animals"}):
    print(s)
```
### 🔍 **Expected Output**
```plaintext
{'generate_topics': {'subjects': ['Lions', 'Elephants', 'Penguins', 'Dolphins']}}
{'generate_joke': {'jokes': ["Why don't elephants use computers? They're afraid of the mouse!"]}}
{'generate_joke': {'jokes': ["Why don't dolphins use smartphones? Because they're afraid of phishing!"]}}
{'generate_joke': {'jokes': ["Why don't you see penguins in Britain? Because they're afraid of Wales!"]}}
{'generate_joke': {'jokes': ["Why don't lions like fast food? Because they can't catch it!"]}}
{'best_joke': {'best_selected_joke': "Why don't dolphins use smartphones? Because they're afraid of phishing!"}}
```

---

## 🎯 **Conclusion**
Using **LangGraph**, we created a **map-reduce pipeline** that:
✅ Dynamically generates related topics.  
✅ Processes each topic **in parallel** to generate jokes.  
✅ Selects the **best joke** using an AI model.  

This **parallel execution** ensures efficiency and scalability, making it ideal for real-world applications like **content generation, sentiment analysis, and AI-driven decision-making**. 🚀


---

# O3 mini Response:

---

Below is a detailed explanation of how to create map‐reduce branches for parallel execution using LangGraph. We’ll break down the concepts step by step, explain the code line by line, and illustrate with real-world examples. This guide is designed for beginners, so even if you’re new to these ideas, you should be able to follow along.

---

# 🚀 Overview: What Is Map-Reduce in LangGraph?

Map-reduce is a design pattern that splits a large task into smaller sub-tasks (the **map** phase), processes them in parallel, and then aggregates (or reduces) the results back into one final result (the **reduce** phase). In our example, we:

1. **Map**: Generate a list of subjects based on a user-supplied topic.
2. **Parallel Execution**: For each subject, generate a joke simultaneously.
3. **Reduce**: Collect all the jokes and decide which one is the best.

**Real-World Example:**  
Imagine you run a news website. You have a list of articles (subjects) and want to generate summaries (jokes) for each article concurrently. Once all summaries are generated, you then choose the best summary to feature as the "Editor's Pick" (best joke).

---

# 🛠️ Setup & Prerequisites

Before diving into the graph logic, we need to install packages and set up environment variables for the API keys. The installation commands and environment setup ensure that our code can communicate with the underlying language models.

```python
%%capture --no-stderr
%pip install -U langchain-anthropic langgraph

import os
import getpass

def _set_env(name: str):
    if not os.getenv(name):
        os.environ[name] = getpass.getpass(f"{name}: ")

_set_env("ANTHROPIC_API_KEY")
```

**Explanation:**

- **Installation Command:**  
  ```python
  %pip install -U langchain-anthropic langgraph
  ```  
  This installs the required libraries.

- **Environment Setup:**  
  The `_set_env` function checks if an environment variable (e.g., `ANTHROPIC_API_KEY`) is already set; if not, it prompts the user to enter it using `getpass` for secure input.

---

# 📚 Defining the Graph Components

The graph is built from multiple components that work together:

1. **State Models:** Define the structure of data passed between nodes.
2. **Prompt Templates:** Pre-defined text instructions used to interact with the language model.
3. **Nodes:** Functions that process data.  
4. **Edges:** Define the flow between nodes, including conditional branches for parallel processing.

---

## 📝 Model and Prompt Definitions

We define prompts for generating subjects, jokes, and selecting the best joke. We also define Pydantic models to enforce the data structure.

```python
# Prompt templates for our tasks
subjects_prompt = """Generate a comma separated list of between 2 and 5 examples related to: {topic}."""
joke_prompt = """Generate a joke about {subject}"""
best_joke_prompt = """Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one.

{jokes}"""

# Pydantic models for structured output
from pydantic import BaseModel, Field

class Subjects(BaseModel):
    subjects: list[str]

class Joke(BaseModel):
    joke: str

class BestJoke(BaseModel):
    id: int = Field(description="Index of the best joke, starting with 0", ge=0)
```

**Explanation:**

- **Subjects Prompt:**  
  Instructs the model to list examples related to a given topic.
  
- **Joke Prompt:**  
  Instructs the model to create a joke for a given subject.

- **Best Joke Prompt:**  
  Provides a list of jokes and asks for the best one by returning its index.

- **Pydantic Models:**  
  These models (`Subjects`, `Joke`, and `BestJoke`) ensure that the responses from the language model follow a predictable structure.

---

## 🔗 Defining the Overall Graph State

The **OverallState** defines the complete state passed through the graph, and **JokeState** defines the state for each individual joke generation node.

```python
from typing import Annotated
from typing_extensions import TypedDict
import operator

# Overall state that will flow through our graph
class OverallState(TypedDict):
    topic: str
    subjects: list
    # Using operator.add tells LangGraph how to combine lists of jokes from different nodes
    jokes: Annotated[list, operator.add]
    best_selected_joke: str

# State for each individual joke generation node (map phase)
class JokeState(TypedDict):
    subject: str
```

**Explanation:**

- **OverallState:**  
  Contains the main `topic`, a list of `subjects`, an aggregated list of `jokes` (using `operator.add` to combine results), and the `best_selected_joke`.

- **JokeState:**  
  Contains the `subject` that is passed to each joke generation node.

---

## 🔄 Creating the Nodes (Map and Reduce Functions)

### 1. Generating Subjects (Initial Map)

```python
def generate_topics(state: OverallState):
    prompt = subjects_prompt.format(topic=state["topic"])
    response = model.with_structured_output(Subjects).invoke(prompt)
    return {"subjects": response.subjects}
```

**Explanation:**

- **Function Purpose:**  
  Generates a list of subjects based on the provided topic.
  
- **Line-by-Line:**  
  - **Prompt Creation:**  
    Uses the `subjects_prompt` and replaces `{topic}` with the actual topic from `state`.
  - **Model Invocation:**  
    Calls the language model to get a structured output conforming to the `Subjects` model.
  - **Return Value:**  
    Returns a dictionary with the list of subjects.

### 2. Generating a Joke for Each Subject (Parallel Map)

```python
def generate_joke(state: JokeState):
    prompt = joke_prompt.format(subject=state["subject"])
    response = model.with_structured_output(Joke).invoke(prompt)
    return {"jokes": [response.joke]}
```

**Explanation:**

- **Function Purpose:**  
  Generates a joke for a given subject.
  
- **Line-by-Line:**  
  - **Prompt Creation:**  
    Fills in the `joke_prompt` with the subject.
  - **Model Invocation:**  
    Retrieves a joke using the language model with the expected `Joke` structure.
  - **Return Value:**  
    Returns the joke inside a list. (This list format allows easy aggregation later.)

### 3. Dispatching Subjects to Parallel Joke Generation Nodes

```python
from langgraph.types import Send

def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"subject": s}) for s in state["subjects"]]
```

**Explanation:**

- **Function Purpose:**  
  Takes the list of subjects and creates a separate branch (node) for each subject to generate a joke.
  
- **Line-by-Line:**  
  - **List Comprehension:**  
    Iterates over each subject and creates a `Send` object that tells LangGraph to execute the `generate_joke` node with the given subject.
  - **Return Value:**  
    A list of `Send` objects for parallel processing.

### 4. Selecting the Best Joke (Reduce)

```python
def best_joke(state: OverallState):
    jokes = "\n\n".join(state["jokes"])
    prompt = best_joke_prompt.format(topic=state["topic"], jokes=jokes)
    response = model.with_structured_output(BestJoke).invoke(prompt)
    return {"best_selected_joke": state["jokes"][response.id]}
```

**Explanation:**

- **Function Purpose:**  
  Aggregates all the generated jokes and selects the best one.
  
- **Line-by-Line:**  
  - **Joke Aggregation:**  
    Joins all jokes into one string separated by new lines.
  - **Prompt Creation:**  
    Fills the `best_joke_prompt` with the topic and aggregated jokes.
  - **Model Invocation:**  
    Asks the language model to pick the best joke based on the provided input, returning the index of the best joke.
  - **Return Value:**  
    Uses the index to retrieve and return the best joke from the list.

---

# 🌐 Constructing the Graph

Now we assemble our nodes and define how they connect to form the complete graph.

```python
from langgraph.graph import END, StateGraph, START

# Instantiate the graph with the overall state
graph = StateGraph(OverallState)

# Add our defined nodes to the graph
graph.add_node("generate_topics", generate_topics)
graph.add_node("generate_joke", generate_joke)
graph.add_node("best_joke", best_joke)

# Define the flow of the graph
graph.add_edge(START, "generate_topics")
graph.add_conditional_edges("generate_topics", continue_to_jokes, ["generate_joke"])
graph.add_edge("generate_joke", "best_joke")
graph.add_edge("best_joke", END)

# Compile the graph into an executable app
app = graph.compile()
```

**Explanation:**

- **StateGraph Initialization:**  
  Creates a graph that will carry the `OverallState`.

- **Adding Nodes:**  
  Each `add_node` call registers a function (node) with a unique name.
  
- **Edges and Conditional Edges:**  
  - `add_edge(START, "generate_topics")` sets the starting point.
  - `add_conditional_edges("generate_topics", continue_to_jokes, ["generate_joke"])` creates multiple edges based on the subjects generated, thereby launching parallel `generate_joke` nodes.
  - Subsequent edges define the flow until the final `best_joke` node and then to the end.

- **Compiling the Graph:**  
  The `compile()` method turns the defined graph into an executable application.

---

# 🖼️ Visualizing the Graph

The graph can be visualized to see how nodes connect. This is useful for debugging and understanding the flow.

```python
from IPython.display import Image
Image(app.get_graph().draw_mermaid_png())
```

**Explanation:**

- **Visualization:**  
  Uses the `draw_mermaid_png()` method to render the graph as an image. This helps to see the map (parallel joke generation) and reduce (selecting best joke) branches.

---

# ⚙️ Running the Graph: End-to-End Example

Finally, we run the graph by providing a topic. The output will show how each node’s result is passed along the chain.

```python
# Run the graph with the topic "animals"
for s in app.stream({"topic": "animals"}):
    print(s)
```

**Explanation:**

- **Execution:**  
  The `app.stream()` method runs the graph with the input state `{"topic": "animals"}`.
  
- **Output Flow:**  
  You will see:
  - The generated subjects.
  - Individual jokes produced in parallel for each subject.
  - The final best joke selected.

**Example Output:**

```plaintext
{'generate_topics': {'subjects': ['Lions', 'Elephants', 'Penguins', 'Dolphins']}}
{'generate_joke': {'jokes': ["Why don't elephants use computers? They're afraid of the mouse!"]}}
{'generate_joke': {'jokes': ["Why don't dolphins use smartphones? Because they're afraid of phishing!"]}}
{'generate_joke': {'jokes': ["Why don't you see penguins in Britain? Because they're afraid of Wales!"]}}
{'generate_joke': {'jokes': ["Why don't lions like fast food? Because they can't catch it!"]}}
{'best_joke': {'best_selected_joke': "Why don't dolphins use smartphones? Because they're afraid of phishing!"}}
```

---

# 💡 Key Takeaways

- **Map-Reduce Paradigm:**  
  Splits a complex task (generate subjects → generate jokes) into smaller tasks that can be executed in parallel and then combines the results.
  
- **Parallel Execution:**  
  The use of conditional edges with the `Send` API allows us to dynamically spawn nodes based on the data generated (subjects).
  
- **Graph-based Workflow:**  
  LangGraph helps you visually and programmatically design workflows, making it easier to manage state and flow between tasks.

- **Real-World Application:**  
  Beyond generating jokes, this approach can be applied in scenarios such as processing large datasets, parallel image processing, or any task where operations can be executed independently and then aggregated.

By understanding and applying these principles, you can build efficient, parallelized applications that take full advantage of modern API capabilities and distributed processing.

Happy coding!