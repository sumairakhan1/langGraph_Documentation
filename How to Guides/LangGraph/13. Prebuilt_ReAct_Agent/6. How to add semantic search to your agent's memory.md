# 6. How to add semantic search to your agent's memory


## üß† **How to Add Semantic Search to Your Agent's Memory**

Adding semantic search to your agent‚Äôs memory is a powerful way to enhance its ability to recall and utilize past information. By integrating semantic search, your agent can better understand the meaning behind user queries and provide more relevant responses based on past interactions, even if those interactions were not explicitly matched by keywords.

In this section, we will walk through the process of setting up semantic search, storing data, and making the agent recall memories. 

### üîë **Prerequisites**

Before we dive into the code, make sure you've installed the necessary libraries:

```bash
%pip install -U langgraph langchain-openai langchain
```

This will install LangChain, LangGraph, and OpenAI's tools, which are all essential for enabling semantic search in your agent.

### üí° **What is Semantic Search?**

Semantic search involves using machine learning to interpret the meaning of a search query and match it with the most relevant results based on concepts, not just keywords. This is more sophisticated than traditional keyword-based search because it takes into account the context and intent behind the user's query.

### üåê **Setting Up the Semantic Search**

1. **Initialize the Embedding Model**  
   First, we need to set up a text embedding model. The model will transform text into numerical vectors that represent the meanings of the text. These vectors are used for searching based on semantic meaning.

   ```python
   from langchain.embeddings import init_embeddings

   # Initialize embeddings with a pre-trained model
   embeddings = init_embeddings("openai:text-embedding-3-small")
   ```

   **Explanation:**
   - `init_embeddings` loads the embedding model "openai:text-embedding-3-small" from OpenAI. 
   - This model turns text into vectors, which are then used for semantic search.

2. **Create the Store with Semantic Search**  
   Now, let‚Äôs set up an in-memory store with semantic search enabled. This store will allow us to store memories and later search through them using semantic search.

   ```python
   from langgraph.store.memory import InMemoryStore

   store = InMemoryStore(
       index={
           "embed": embeddings,  # Embedding model for vector search
           "dims": 1536,  # The dimension size of the embeddings
       }
   )
   ```

   **Explanation:**
   - `InMemoryStore` is where all memories are stored temporarily.
   - We specify the embeddings model to be used for indexing the memories.
   - `"dims": 1536` tells the system that the vector embeddings have 1536 dimensions, which is typical for the OpenAI text embedding model.

3. **Storing Memories**  
   Once we have the store set up, we can store memories with a user ID and key-value pairs. Each memory will be indexed using its semantic meaning, so that later searches will understand the context.

   ```python
   # Storing user memories in the store
   store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
   store.put(("user_123", "memories"), "2", {"text": "I prefer Italian food"})
   store.put(("user_123", "memories"), "3", {"text": "I don't like spicy food"})
   store.put(("user_123", "memories"), "4", {"text": "I am studying econometrics"})
   store.put(("user_123", "memories"), "5", {"text": "I am a plumber"})
   ```

   **Explanation:**
   - `store.put()` is used to add memories to the store.
   - Each memory is identified by a unique key (e.g., `"1"`, `"2"`, etc.).
   - The memory content is a dictionary with a `"text"` field containing the memory text (e.g., "I love pizza").

### üîç **Searching Memories Using Semantic Search**

Now that we have stored memories, we can search for relevant memories based on natural language queries. This will enable the agent to find memories that are contextually relevant, even if the exact words don‚Äôt match.

```python
# Searching for memories based on a natural language query
memories = store.search(("user_123", "memories"), query="I like food?", limit=5)

for memory in memories:
    print(f'Memory: {memory.value["text"]} (similarity: {memory.score})')
```

**Explanation:**
- `store.search()` is used to perform a semantic search.
- We provide a query (e.g., "I like food?") and specify a limit on the number of results (e.g., `limit=5`).
- The search results will be based on semantic similarity, meaning the agent will find memories that are contextually similar to the query.
- The `memory.score` represents the similarity score between the query and the retrieved memory.

### ü§ñ **Using Semantic Search in Your Agent**

You can inject the semantic search functionality directly into your agent‚Äôs workflow. In the following example, we‚Äôll create a simple chat system where the agent recalls relevant memories based on the user‚Äôs last message.

```python
from langchain.chat_models import init_chat_model
from langgraph.graph import START, MessagesState, StateGraph

# Initialize a chat model
llm = init_chat_model("openai:gpt-4o-mini")

def chat(state, *, store: BaseStore):
    # Search based on user's last message
    items = store.search(
        ("user_123", "memories"), query=state["messages"][-1].content, limit=2
    )
    memories = "\n".join(item.value["text"] for item in items)
    memories = f"## Memories of user\n{memories}" if memories else ""
    response = llm.invoke(
        [
            {"role": "system", "content": f"You are a helpful assistant.\n{memories}"},
            *state["messages"],
        ]
    )
    return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(chat)
builder.add_edge(START, "chat")
graph = builder.compile(store=store)

# Stream the conversation and get responses
for message, metadata in graph.stream(
    input={"messages": [{"role": "user", "content": "I'm hungry"}]},
    stream_mode="messages",
):
    print(message.content, end="")
```

**Explanation:**
- `chat()` is the function that handles the interaction. It searches for relevant memories based on the user's message.
- `store.search()` is used to find the most relevant memories, which are then passed to the LLM (Language Model).
- The model responds with a message that includes relevant memories. For example, if the user says "I'm hungry", the agent may recall memories like "I love pizza" or "I prefer Italian food".

### üåç **Real-World Use Case:**

Imagine you're building a virtual assistant for a customer service bot. The assistant should remember customer preferences, complaints, and past orders. For example:
- **Customer:** "I love pizza"
- **Bot (in the future):** "What type of pizza would you like to order today? I remember you love pizza!"

By storing and searching through past conversations using semantic search, your assistant can provide more personalized, context-aware responses.

---

### üßë‚Äçüíª **Summary:**

1. **Set up Semantic Search**  
   Use embeddings to convert text into vectors and store them in a memory store. This allows the agent to perform context-aware searches.

2. **Store Memories**  
   Add memories using `store.put()` and associate them with user IDs and keys.

3. **Search for Relevant Memories**  
   Use `store.search()` to find memories that are contextually relevant to the user‚Äôs current query.

4. **Integrate with Your Agent**  
   Inject the memory store into your agent's workflow to make it aware of past conversations, enhancing its ability to provide personalized, context-aware responses.

By incorporating semantic search, you create an agent that remembers user preferences, responds with relevant information, and provides a more human-like experience.


# Using Semantic Search in a Create React Agent for Memory Management üîç

In this section, we are going to discuss how to integrate **semantic search** into your agent's memory using **React agents**. We'll break down the logic, explain the code, and showcase how these concepts can be applied in the real world.

## What is Semantic Search? ü§ñ

**Semantic search** is a technique where you don‚Äôt just search for exact keyword matches but also for meaning or context. It enables your system to understand the intent behind the query and retrieve more relevant results, even if they don‚Äôt contain the exact words.

For example:
- If a user says ‚ÄúI‚Äôm hungry and want to eat something spicy,‚Äù the semantic search should return memories related to food preferences or spicy food.

## Using Semantic Search in `create_react_agent` üß†

In this case, we‚Äôll build an agent that can search and store memories using **semantic search**. This agent will use the store to query past conversations, retrieve relevant memories, and store new memories for future interactions.

### Step-by-Step Code Breakdown üßë‚Äçüíª

First, let‚Äôs look at the required code and explain each part:

```python
import uuid
from typing import Optional
from langchain.chat_models import init_chat_model
from langgraph.prebuilt import InjectedStore
from langgraph.store.base import BaseStore
from typing_extensions import Annotated
from langgraph.prebuilt import create_react_agent
```

- **Imports**:
  - `uuid`: Used to generate unique IDs for memories.
  - `Optional`: A type hint indicating that a variable can either be of a specific type or `None`.
  - `langchain.chat_models.init_chat_model`: This is used to initialize a pre-trained language model, which powers your agent.
  - `langgraph.prebuilt.InjectedStore`: This is used to inject the store into your agent to allow memory search and storage.
  - `langgraph.store.base.BaseStore`: Base class for the memory store that holds your agent‚Äôs data.
  - `langgraph.prebuilt.create_react_agent`: A function to create a React agent that reacts to user input.

### Preparing Messages for the Agent üìù

Now let‚Äôs create a function that prepares messages before sending them to the language model (LLM). This function will use **semantic search** to retrieve relevant memories based on the last user message.

```python
def prepare_messages(state, *, store: BaseStore):
    # Search based on user's last message
    items = store.search(
        ("user_123", "memories"), query=state["messages"][-1].content, limit=2
    )
    # Extract the memory text from the search results
    memories = "\n".join(item.value["text"] for item in items)
    memories = f"## Memories of user\n{memories}" if memories else ""
    return [
        {"role": "system", "content": f"You are a helpful assistant.\n{memories}"}
    ] + state["messages"]
```

- **`prepare_messages`**:
  - This function receives the current **state** (which contains the conversation history) and the **store** (which holds the memories).
  - It performs a **semantic search** using the **last user message** to find related memories. The query (`query=state["messages"][-1].content`) searches the store for memories related to the user‚Äôs last input.
  - The results are formatted and prepended to the **system message** to provide context for the agent. If there are no memories, it simply adds an empty string.

### Storing Memories with Upsert Memory üß†

Next, let‚Äôs define a tool that allows the agent to **store memories**. This tool is used whenever the agent wants to save a new memory to the store.

```python
def upsert_memory(
    content: str,
    *,
    memory_id: Optional[uuid.UUID] = None,
    store: Annotated[BaseStore, InjectedStore],
):
    """Upsert a memory in the database."""
    # The LLM can use this tool to store a new memory
    mem_id = memory_id or uuid.uuid4()
    store.put(
        ("user_123", "memories"),
        key=str(mem_id),
        value={"text": content},
    )
    return f"Stored memory {mem_id}"
```

- **`upsert_memory`**:
  - This function is responsible for adding or updating memories in the store.
  - It accepts the **content** (the new memory), an optional **memory ID**, and the **store** where memories are stored.
  - If no **memory ID** is provided, it generates a new one using **UUID**.
  - The memory is stored using the `put` method, associating it with the user ID (`user_123`) and the "memories" category.
  - After storing, it returns a message indicating the memory ID.

### Creating the React Agent üí¨

Finally, we use the `create_react_agent` function to create an agent that can handle user queries and memories. This function takes the following:

```python
agent = create_react_agent(
    init_chat_model("openai:gpt-4o-mini"),
    tools=[upsert_memory],
    # The 'prompt' function is run to prepare the messages for the LLM. It is called
    # right before each LLM call
    prompt=prepare_messages,
    store=store,
)
```

- **`create_react_agent`**:
  - Initializes the **chat model** (in this case, a mini version of GPT-4).
  - Uses **tools** (such as `upsert_memory`) that the agent can call to store or retrieve information.
  - The **`prompt` function** (here, `prepare_messages`) prepares the context for the agent before sending the message to the LLM. It ensures the agent has access to relevant memories.
  - **`store`**: The memory store where all the user data is kept. 

### Running the Agent üèÉ‚Äç‚ôÇÔ∏è

Now, let‚Äôs simulate a conversation and see how the agent reacts based on stored memories:

```python
for message, metadata in agent.stream(
    input={"messages": [{"role": "user", "content": "I'm hungry"}]},
    stream_mode="messages",
):
    print(message.content, end="")
```

- **`stream`**:
  - This allows the agent to generate a continuous stream of responses based on the user‚Äôs input.
  - The input message is `"I'm hungry"`. The agent will search for related memories (e.g., preferences for food) and respond accordingly.
  - The agent‚Äôs response could be based on prior memories of the user‚Äôs food preferences, making the conversation more personalized and engaging.

### Example Output üó£Ô∏è

```
What are you in the mood for? Since you love Italian food and pizza, maybe something in that realm would be great! Would you like suggestions for a specific dish or restaurant?
```

The agent provides a response that is based on prior memories about the user‚Äôs food preferences, showcasing how **semantic search** makes the agent smarter and more contextually aware.

---

## Real-World Use Case: Personal Assistant üì±

### Scenario: Virtual Assistant

Imagine a **virtual assistant** helping a user with daily tasks. The assistant would store memories about preferences, past activities, and favorite things. When a user asks, "What did I do last weekend?", the assistant can retrieve memories of past activities, such as "You went hiking" or "You watched a movie."

This makes the assistant more intelligent and efficient because it doesn‚Äôt just respond based on keywords but can understand context, intent, and the user‚Äôs personal history.

## Conclusion ‚ú®

In this guide, we explored how to add **semantic search** to your React agent for smarter memory management. By enabling the agent to recall past memories, you create a more engaging and personalized experience for users. You also learned how to store new memories and retrieve relevant information based on user input, making the interaction much more meaningful.

This concept can be applied to real-world applications like virtual assistants, customer support bots, or even recommendation systems that suggest content based on user preferences and history.

---

# Advanced Usage: Multi-Vector Indexing in Memory Management üìö

In this section, we will dive into the concept of **multi-vector indexing** in memory management. This technique is used to store and search different aspects of memories separately. By doing so, it improves recall, enables more focused searches, and can also allow for omitting certain fields from being indexed, providing more flexibility in how we retrieve data.

## What is Multi-Vector Indexing? ü§î

Multi-vector indexing allows you to index multiple **vectors** (pieces of data) in a store, each representing a different aspect of a memory or piece of information. This way, you can have more control over which aspects of your data should be searchable, while keeping other parts less indexed or excluded altogether.

### Real-World Example üßë‚Äçüç≥

Imagine a food app that tracks your eating experiences. You might want to store **memories of what you've eaten**, but you'd also like to track how **you felt** while eating (happy, sad, excited, etc.). These two pieces of information (the memory itself and the emotional context) could be stored separately for better search and analysis.

### Example Scenario: Food Experience Tracker üçï

- **Memory**: "Had pizza with friends at Mario's"
- **Emotional Context**: "Felt happy and connected"
- **Non-indexed Info**: "I prefer ravioli though"

By storing these in separate vectors (memory and emotion), we can search based on either the content or the emotional context without being restricted to only one type of data.

## Code Explanation üë©‚Äçüíª

Let's break down the code provided, which implements multi-vector indexing for better recall and search functionality.

### 1. **Storing Memories with Multiple Aspects**

```python
store = InMemoryStore(
    index={"embed": embeddings, "dims": 1536, "fields": ["memory", "emotional_context"]}
)
```

**Explanation**:  
- `InMemoryStore`: This is where we're storing our data in memory (RAM), so it‚Äôs fast and easy to access.
- `index`: We're specifying how we want to index the data. We use **embeddings** (numerical representations of the data) for storing memories, and we're indexing two fields:
  1. **Memory**: The text of the memory (e.g., "Had pizza with friends").
  2. **Emotional Context**: The emotion felt during that memory (e.g., "Felt happy and connected").
  
- **dims**: The number `1536` refers to the number of dimensions for the embeddings (the size of each vector used to represent memories).

### 2. **Storing Memories**

```python
store.put(
    ("user_123", "memories"),
    "mem1",
    {
        "memory": "Had pizza with friends at Mario's",
        "emotional_context": "felt happy and connected",
        "this_isnt_indexed": "I prefer ravioli though",
    },
)
```

**Explanation**:  
- `store.put`: This is how we add data to the store. Here, we are storing two pieces of data under a unique identifier (`mem1`) for a user.
- We store three fields:
  - **memory**: The memory of eating pizza.
  - **emotional_context**: The emotion felt (happy and connected).
  - **this_isnt_indexed**: A piece of information that is not indexed (it won‚Äôt be searchable).

### 3. **Searching Based on Emotional State**

```python
results = store.search(
    ("user_123", "memories"), query="times they felt isolated", limit=1
)
```

**Explanation**:  
- **Searching**: We perform a search based on the **emotional context** of the memories.
- **query**: We query the store for memories related to "times they felt isolated."
- **limit**: We limit the number of search results to **1** for simplicity.

The result of this search will bring back **mem2**, which contains the emotional context of "felt a bit lonely."

### 4. **Searching Based on Memory Content**

```python
results = store.search(("user_123", "memories"), query="fun pizza", limit=1)
```

**Explanation**:  
- Here, we're searching based on the **memory content**. The query "fun pizza" will match **mem1**, since it's about eating pizza with friends.

### 5. **Searching for Non-Indexed Information**

```python
results = store.search(("user_123", "memories"), query="ravioli", limit=1)
```

**Explanation**:  
- This search looks for the word "ravioli," which was **not indexed** in the store. As a result, the search will return **mem1** with a lower score, because "ravioli" isn't part of the indexed fields (memory or emotional context).

## Example Outputs üí¨

### Searching for "times they felt isolated":
```python
Expect mem 2
Item: mem2; Score (0.5895009051396596)
Memory: Ate alone at home
Emotion: felt a bit lonely
```

### Searching for "fun pizza":
```python
Expect mem1
Item: mem1; Score (0.6207546534134083)
Memory: Had pizza with friends at Mario's
Emotion: felt happy and connected
```

### Searching for "ravioli":
```python
Expect random lower score (ravioli not indexed)
Item: mem1; Score (0.2686278787315685)
Memory: Had pizza with friends at Mario's
Emotion: felt happy and connected
```

## Benefits of Multi-Vector Indexing ‚ö°

1. **Flexibility**: You can index and search different aspects of your data separately.
2. **Improved Recall**: Allows for more accurate search results based on specific fields.
3. **Efficiency**: Helps avoid indexing unnecessary information, which can reduce storage costs and improve search performance.
4. **Advanced Use Cases**: Perfect for applications that need to manage and search complex data with multiple dimensions, like personal assistants, recommendation systems, or sentiment analysis.

## Use Cases in the Real World üåç

1. **Personal Assistants (e.g., Siri, Alexa)**: They can remember the user‚Äôs preferences and emotional states. You could store memories of past interactions (e.g., "Asked about pizza") along with the emotional context (e.g., "User was happy").
   
2. **Recommendation Systems**: By using multi-vector indexing, a recommendation engine could combine various factors (e.g., genre preferences and mood) to offer more accurate suggestions.

3. **Mental Health Apps**: Apps could track not only what the user is doing but also how they feel while doing it, providing insights into their mental health over time.

## Conclusion üéâ

Multi-vector indexing is a powerful tool for storing and retrieving data based on multiple aspects of information. By separating different components like memory and emotion, you can have more granular control over search behavior, leading to better recall and more personalized results. This approach is widely applicable in many fields, especially those that deal with complex, multidimensional data!

# Override Fields at Storage Time in Memory Management üß†

In this section, we will explore the concept of **overriding fields at storage time** in memory management systems. This allows us to control exactly which pieces of data (or "fields") are indexed and embedded when storing a memory, regardless of the store's default settings. This capability is helpful when you want more granular control over the data that is indexed for faster and more accurate searches.

## What is Field Override at Storage Time? üîÑ

Field overriding means that when you store a memory, you can specify which parts (or fields) of the memory you want to **embed** (convert into a searchable vector). By default, a store might index all fields, but using the **override** feature, you can choose to only index specific fields when storing data. This provides more flexibility in managing which parts of your data are available for searching.

### Real-World Example üè¢

Imagine you‚Äôre working with an **event management system** that tracks different details of events, such as **event names**, **locations**, **attendees**, and **feedback**. For example:
- **Event Name**: "Annual Charity Gala"
- **Location**: "City Convention Center"
- **Attendees**: "100 guests"
- **Feedback**: "Excellent event!"

Now, you may want to index only the **event name** when searching for events, but **feedback** when analyzing the event experience. By overriding the default indexing behavior, you can achieve this.

### Why Override Fields? ü§î

1. **Efficiency**: Index only the relevant fields to make searches faster.
2. **Control**: Decide exactly which data should be searchable at any given time.
3. **Performance**: Save storage space by not indexing unnecessary data.
4. **Flexibility**: Store memories or data differently based on specific needs or queries.

## Code Breakdown üë©‚Äçüíª

### 1. **Default Configuration: Store and Index "Memory" Field**

```python
store = InMemoryStore(
    index={
        "embed": embeddings,
        "dims": 1536,
        "fields": ["memory"],
    }  # Default to embed memory field
)
```

**Explanation**:  
- `InMemoryStore`: This represents the store where we are saving our data. It's stored temporarily in memory for fast access.
- The **default configuration** specifies that only the **"memory"** field will be indexed and embedded.
  
### 2. **Storing Data Using Default Configuration**

```python
store.put(
    ("user_123", "memories"),
    "mem1",
    {"memory": "I love spicy food", "context": "At a Thai restaurant"},
)
```

**Explanation**:  
- We're storing **mem1** with two fields: **memory** and **context**.
- Since we are using the default configuration, **only** the **memory** field ("I love spicy food") will be indexed, while the **context** ("At a Thai restaurant") will not be searchable.

### 3. **Storing Data with Field Override**

```python
store.put(
    ("user_123", "memories"),
    "mem2",
    {"memory": "The restaurant was too loud", "context": "Dinner at an Italian place"},
    index=["context"],  # Override: only embed the context
)
```

**Explanation**:  
- For **mem2**, we are using an **override** to only index the **"context"** field ("Dinner at an Italian place"). This means the **memory** ("The restaurant was too loud") will not be indexed, and only **context** will be used for searching.

### 4. **Search Based on Memory Content**

```python
results = store.search(
    ("user_123", "memories"), query="what food do they like", limit=1
)
```

**Explanation**:  
- In this search, we query based on the **memory** content ("what food do they like"). Since **mem1** was indexed with the "memory" field, it will match the query and return **mem1** as the result.

### 5. **Search Based on Context (Overridden Field)**

```python
results = store.search(
    ("user_123", "memories"), query="restaurant environment", limit=1
)
```

**Explanation**:  
- In this search, we query based on **context** ("restaurant environment"). Since **mem2** was indexed with the "context" field, it will match the query and return **mem2**.

### 6. **Example Outputs**

#### Search about "Food Preference" (Based on Memory)

```python
Expect mem1
Item: mem1; Score (0.3374968677940555)
Memory: I love spicy food
Context: At a Thai restaurant
```

- Since **mem1** has "memory" indexed, the result will show the memory of loving spicy food.

#### Search about "Restaurant Environment" (Based on Context)

```python
Expect mem2
Item: mem2; Score (0.36784461593247436)
Memory: The restaurant was too loud
Context: Dinner at an Italian place
```

- Since **mem2** has "context" indexed, the search for restaurant environment will return **mem2**.

## Benefits of Overriding Fields üèÜ

1. **Custom Search Capabilities**: Allows you to index only the most relevant fields for each search query, making your system more efficient.
2. **Faster Searches**: By limiting the number of indexed fields, you can speed up the search process.
3. **Reduced Storage**: Indexing only the necessary fields reduces the amount of storage space needed.
4. **Selective Focus**: Tailor the indexing behavior based on the specific needs of each memory or data item.

## Use Cases in the Real World üåé

1. **Customer Feedback Systems**: In a feedback system, you might want to store both **feedback content** and **user satisfaction rating**. For certain queries, you may only want to search the satisfaction ratings, while for others, you may need to look through the full feedback. You can override fields to control what gets indexed.
   
2. **E-commerce Platforms**: For an e-commerce platform that stores **product reviews**, you could store fields like **review content**, **rating**, and **tags**. If you're running a search for "ratings", you may want to index only the **rating** field, and if you are looking for a specific product experience, you can index **review content** instead.
   
3. **Medical Databases**: In medical data storage systems, you might store both **patient symptoms** and **diagnoses**. Depending on the query, you may override and index only the **symptoms** for diagnosis-based searches, or only the **diagnoses** for condition-based searches.

## Conclusion üéØ

Overriding fields at storage time is a powerful feature that gives you full control over what gets indexed and stored in memory. This approach can be used to optimize storage and improve the speed of your search queries. Whether you are building a recommendation system, an event management platform, or a customer feedback system, this concept provides the flexibility you need to store and retrieve data efficiently and accurately!


# Disable Indexing for Specific Memories üîí

In memory management systems, there are scenarios where you may want to store certain pieces of data (or **memories**) but prevent them from being searchable. Disabling indexing allows you to store these memories for internal use without exposing them to search queries. This can be useful for **system-level data** or **sensitive information** that doesn't need to be part of search operations.

## What is Disabling Indexing? üö´

When we store data in a memory system, we typically want that data to be indexed for efficient search. However, there are times when you might not want a specific piece of data to be searchable, but you still want to store it for reference. Disabling indexing allows you to achieve this.

By using the parameter `index=False` when storing data, you tell the system **not to index** that particular memory, even though it will still be stored in the system. This means that the memory will not be found through search queries.

### Real-World Example üè¢

Let‚Äôs imagine you‚Äôre building a **task management system**. You might have:
- **Task data** like descriptions, due dates, and priorities.
- **System data** like timestamps, logs, or completion statuses, which you might not want to appear in search results.

You can disable indexing for the system-level data, so only task descriptions and priorities are searchable while logs and system status data are stored internally.

### Why Disable Indexing? ü§î

1. **Security**: Protect sensitive or private data from being exposed in search results.
2. **Performance**: Reduce the size of the index and make searches faster by excluding non-searchable data.
3. **Clarity**: Store system-related or temporary data without cluttering the search results.

## Code Breakdown üë©‚Äçüíª

### 1. **Default Configuration with Indexing for Memory Fields**

```python
store = InMemoryStore(index={"embed": embeddings, "dims": 1536, "fields": ["memory"]})
```

**Explanation**:
- The **default configuration** sets the store to index the **"memory"** field, so that we can efficiently search based on the memory content.

### 2. **Storing an Indexed Memory (Normal Data)**

```python
store.put(
    ("user_123", "memories"),
    "mem1",
    {"memory": "I love chocolate ice cream", "type": "preference"},
)
```

**Explanation**:
- Here, we are storing **mem1** as a memory that will be indexed. The **"memory"** field will be searchable, and the content "I love chocolate ice cream" will be part of the search index.
- The **type** field here is just additional information, not used for indexing.

### 3. **Storing a Memory Without Indexing (System Data)**

```python
store.put(
    ("user_123", "memories"),
    "mem2",
    {"memory": "User completed onboarding", "type": "system"},
    index=False,  # Disable indexing entirely
)
```

**Explanation**:
- In this case, **mem2** contains system-level information ("User completed onboarding").
- By setting `index=False`, we are telling the system **not to index** this memory. As a result, this memory will not be searchable.

### 4. **Searching for Food Preferences (Indexed Memory)**

```python
results = store.search(
    ("user_123", "memories"), query="what food preferences", limit=1
)
```

**Explanation**:
- When we search for **"food preferences"**, it will match **mem1** since it is indexed. The system will find this memory and return the result.

### 5. **Searching for Onboarding Status (Unindexed Memory)**

```python
results = store.search(
    ("user_123", "memories"), query="onboarding status", limit=1
)
```

**Explanation**:
- Here, we are searching for **"onboarding status"**, but since **mem2** is not indexed, it will not be found in the search. Instead, the system will return the indexed **mem1** with a very low match score.

### 6. **Example Outputs**

#### Searching for Food Preferences

```python
Expect mem1
Item: mem1; Score (0.32269984224327286)
Memory: I love chocolate ice cream
Type: preference
```

- The query for "food preferences" successfully returns **mem1** since it is indexed. The score indicates how well the query matches the memory.

#### Searching for Onboarding Status

```python
Expect low score (mem2 not indexed)
Item: mem1; Score (0.010241633698527089)
Memory: I love chocolate ice cream
Type: preference
```

- The query for "onboarding status" returns **mem1** because **mem2** (which contains the onboarding information) was not indexed. The low score indicates that the result is not a good match for the query.

## Benefits of Disabling Indexing ‚ùå

1. **Security**: Sensitive or internal data (like system logs) can be stored without the risk of it being accidentally exposed through search results.
2. **Improved Search Performance**: Only the relevant data is indexed, reducing the size of the index and making searches faster and more efficient.
3. **Data Organization**: It allows you to store **system-level** data, **metadata**, or **temporary data** without cluttering the search results or index.

## Use Cases in the Real World üåé

1. **Task Management Systems**: In a task manager, you might store **task descriptions** and **user comments** for easy searchability. However, you wouldn't want **system-generated data** like **timestamps** or **status updates** to be indexed.
   
2. **E-commerce Platforms**: When storing **product reviews**, you can index the **review content** and **ratings** but disable indexing for **system-level data** such as **shipping statuses** or **inventory updates**.
   
3. **Health Management Systems**: For **medical records**, you may want to store **patient symptoms** and **diagnoses** in a searchable way. However, **system logs**, **timestamps**, or **internal processing information** should not be indexed or searchable.

## Conclusion üéØ

Disabling indexing for specific memories gives you the flexibility to store and manage data without exposing unnecessary or sensitive information in search results. Whether it‚Äôs for **internal system data**, **logs**, or **temporary information**, this feature allows you to keep your search system clean, secure, and efficient. By selectively indexing only the relevant memories, you can improve both **performance** and **privacy** while still storing all necessary data for your application!