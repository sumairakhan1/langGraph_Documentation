# 8. How to create a ReAct agent from scratch (Functional API)

Here's a detailed beginner-friendly explanation of how to create a **ReAct Agent** from scratch using the **LangGraph Functional API**. I’ll break down the concept step by step, using **headings**, **icons**, and **real-world examples** to make it engaging and easy to understand. 🚀  

---

# 🧠 Understanding ReAct Agent  
A **ReAct Agent** (Reasoning + Acting) is a special type of AI agent that can **call external tools dynamically** to answer questions. Instead of just replying based on its training data, it can:  

1️⃣ **Take a user query** (e.g., "What's the weather in New York?")  
2️⃣ **Decide if it needs extra information** (like calling a weather API)  
3️⃣ **Fetch that information**  
4️⃣ **Return a response** based on both the query and the retrieved data  

🛠 **Example in the Real World**:  
Think of a ReAct Agent like **a virtual assistant (Siri, Google Assistant, or Alexa)** that can look up weather, set reminders, or fetch news when asked.  

---

# 🛠 Setting Up the Project  
Before building the agent, we need to install the necessary tools.  

### 📌 Step 1: Install Dependencies  
```bash
pip install -U langgraph langchain-openai
```
This installs **LangGraph** (to create AI workflows) and **LangChain-OpenAI** (to connect with OpenAI's GPT models).  

### 🔑 Step 2: Set API Key  
Since we are using OpenAI’s API, we need to authenticate it:  

```python
import os
import getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

---

# 🤖 Creating the ReAct Agent  
Now that we’ve installed the tools, let's build our agent.  

## **🔧 Step 1: Define Model and Tools**  
We define:  
✅ A **GPT model** (like GPT-4)  
✅ A **tool** to fetch the weather for a given location  

```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# Use GPT-4 model
model = ChatOpenAI(model="gpt-4o-mini")

# Define a simple weather tool
@tool
def get_weather(location: str):
    """Returns the weather for a given location."""
    if "san francisco" in location.lower():
        return "It's sunny!"
    elif "boston" in location.lower():
        return "It's rainy!"
    else:
        return f"I don't have data for {location}"

tools = [get_weather]
```
🚀 **What’s Happening?**  
- We create a **GPT-4-based chat model**.  
- We define a **tool (get_weather)** that fetches weather based on a location.  

---

## **⚡ Step 2: Define the Agent’s Tasks**  
Our agent has two main jobs:  
1️⃣ **Calling the model** (to understand user queries)  
2️⃣ **Calling tools** (if needed, like fetching weather)  

```python
from langchain_core.messages import ToolMessage
from langgraph.func import entrypoint, task

tools_by_name = {tool.name: tool for tool in tools}

@task
def call_model(messages):
    """Calls the chat model with a sequence of messages."""
    response = model.bind_tools(tools).invoke(messages)
    return response

@task
def call_tool(tool_call):
    """Executes tool calls when needed."""
    tool = tools_by_name[tool_call["name"]]
    observation = tool.invoke(tool_call["args"])
    return ToolMessage(content=observation, tool_call_id=tool_call["id"])
```
🛠 **How it Works?**  
- The model **reads the user’s message** and decides whether to call a tool.  
- If a tool call is needed, it **executes the tool function** (e.g., get_weather).  

---

## **📢 Step 3: Define the Main Agent Logic**  
Now, let’s define the main agent function:  

```python
from langgraph.graph.message import add_messages

@entrypoint()
def agent(messages):
    """Handles conversation flow between model and tools."""
    llm_response = call_model(messages).result()
    
    while True:
        if not llm_response.tool_calls:
            break  # No more tools to call, return response

        # Execute tool calls
        tool_result_futures = [call_tool(tc) for tc in llm_response.tool_calls]
        tool_results = [fut.result() for fut in tool_result_futures]

        # Add tool responses to conversation
        messages = add_messages(messages, [llm_response, *tool_results])

        # Call model again with updated messages
        llm_response = call_model(messages).result()

    return llm_response
```
🔥 **How it Works?**  
1️⃣ The **agent gets a message** from the user.  
2️⃣ It **asks the model** what to do.  
3️⃣ If a **tool is needed**, it **fetches data**.  
4️⃣ It **returns the final answer** to the user.  

---

# 🚀 Running the ReAct Agent  
Now, let’s test our agent!  

```python
user_message = {"role": "user", "content": "What's the weather in San Francisco?"}

for step in agent.stream([user_message]):
    for task_name, message in step.items():
        if task_name == "agent":
            continue  # Skip agent status messages
        print(f"\n{task_name}:")
        message.pretty_print()
```
🎯 **Expected Output:**  
```plaintext
call_model:
Tool Calls:
  get_weather (location: San Francisco)

call_tool:
It's sunny!

call_model:
The weather in San Francisco is sunny!
```
**💡 What Happened?**  
✅ The **agent detected the need for a tool** (get_weather).  
✅ It **called the tool**, got the weather, and **returned the response**.  

---

# 🛠 Adding Memory (Thread-Level Persistence)  
Currently, our agent **forgets past messages**. To make it **remember past conversations**, we use **thread-level persistence**.  

### **🔹 Step 1: Add a Checkpointer**
```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
```
This **stores past messages** so the agent can recall context.  

### **🔹 Step 2: Update the Agent to Use Memory**
```python
@entrypoint(checkpointer=checkpointer)
def agent(messages, previous):
    """Agent with memory, retains previous messages."""
    if previous is not None:
        messages = add_messages(previous, messages)

    llm_response = call_model(messages).result()

    while True:
        if not llm_response.tool_calls:
            break

        tool_result_futures = [call_tool(tc) for tc in llm_response.tool_calls]
        tool_results = [fut.result() for fut in tool_result_futures]

        messages = add_messages(messages, [llm_response, *tool_results])
        llm_response = call_model(messages).result()

    messages = add_messages(messages, llm_response)
    return entrypoint.final(value=llm_response, save=messages)
```
### **🔹 Step 3: Run with Memory**
```python
config = {"configurable": {"thread_id": "1"}}

user_message = {"role": "user", "content": "What's the weather in San Francisco?"}

for step in agent.stream([user_message], config):
    for task_name, message in step.items():
        print(f"\n{task_name}:")
        message.pretty_print()
```
🚀 **Now, the agent remembers past conversations!**  

---

# 🌍 Real-World Use Cases  
✅ **Customer Support Bots** – Answer FAQs, fetch real-time data (e.g., order status).  
✅ **Virtual Assistants (Alexa, Siri, Google Assistant)** – Look up weather, news, or control smart home devices.  
✅ **AI-Powered Chatbots** – Help users with technical queries, software debugging, etc.  

---

# 🎯 Conclusion  
- The **ReAct Agent** intelligently decides **when** to call tools.  
- We **built an AI agent** that fetches **real-time information**.  
- **Memory** enables the agent to **remember past interactions**.  

Want to build **your own AI assistant?** 🎯 Try extending this project with **more tools**, like fetching news or stock prices! 🚀


# 🚀 How to Create a ReAct Agent from Scratch (Functional API)

In this guide, we’ll walk through how to build a **ReAct agent** using the LangGraph **Functional API**. This method gives you complete control over the agent’s behavior by letting you define its tasks and workflow from the ground up.

---

## 📖 Overview

A **ReAct agent** is a tool-calling agent that operates in two main stages:

1. **Query the Chat Model:**  
   The agent sends a list of messages (conversation history) to a chat model (e.g., GPT) to get a response.

2. **Handle Tool Calls:**  
   - **If the model returns a response with no tool calls:** The agent simply returns that response.
   - **If the model returns tool calls:** The agent executes these tool calls, appends the tool responses as new messages, and then repeats the process until no further tool calls are generated.

This setup is versatile and can be extended with additional features such as memory, human-in-the-loop intervention, and more.

---

## 🔧 Setup

### 1. **Install Required Packages**

First, install the necessary packages:

```bash
%%capture --no-stderr
%pip install -U langgraph langchain-openai
```

### 2. **Set Your API Key**

Next, set up your OpenAI API key so that the chat model can be accessed securely:

```python
import os
import getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

**Explanation:**  
- This code checks if the API key is already in the environment; if not, it prompts you to enter it securely.

---

## 🛠 Define Model and Tools

### **Initialize the Chat Model**

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")
```

**Explanation:**  
- We initialize an OpenAI chat model (here, a mini version of GPT-4).  
- This model will be used to process messages and generate responses.

### **Define a Tool: Get Weather**

```python
from langchain_core.tools import tool

@tool
def get_weather(location: str):
    """Call to get the weather from a specific location."""
    # Placeholder implementation:
    if any([city in location.lower() for city in ["sf", "san francisco"]]):
        return "It's sunny!"
    elif "boston" in location.lower():
        return "It's rainy!"
    else:
        return f"I am not sure what the weather is in {location}"
```

**Explanation:**  
- The `@tool` decorator marks `get_weather` as an external tool that the agent can call.
- The function checks the location provided and returns a simple placeholder response.

### **Bind Tools to the Model**

```python
tools = [get_weather]
model = model.bind_tools(tools)
```

**Explanation:**  
- We create a list of tools and bind them to the model so that the model can generate tool calls when needed.

---

## 📋 Define Tasks

Tasks represent the atomic units of work in the agent's workflow.

### **Task 1: Call the Model**

```python
from langchain_core.messages import ToolMessage
from langgraph.func import task

@task
def call_model(messages):
    """Call model with a sequence of messages."""
    response = model.invoke(messages)
    return response
```

**Explanation:**  
- `call_model` sends a list of messages to the model and returns the response.  
- This is where the agent queries the chat model.

### **Task 2: Call a Tool**

```python
from langgraph.func import task

tools_by_name = {tool.name: tool for tool in tools}

@task
def call_tool(tool_call):
    """Execute a tool call and return its response."""
    tool = tools_by_name[tool_call["name"]]
    observation = tool.invoke(tool_call["args"])
    return ToolMessage(content=observation, tool_call_id=tool_call["id"])
```

**Explanation:**  
- `call_tool` receives a tool call (which contains the tool name, arguments, and an identifier).
- It retrieves the corresponding tool from `tools_by_name` and executes it.
- The result is wrapped in a `ToolMessage` and returned.

---

## 🔗 Define the Entrypoint

The entrypoint orchestrates the workflow by chaining the tasks together.

```python
from langgraph.func import entrypoint
from langgraph.graph.message import add_messages

@entrypoint()
def agent(messages):
    # Call the model with the current messages
    llm_response = call_model(messages).result()
    
    # Loop until the model produces no tool calls
    while True:
        # If no tool calls are generated, stop
        if not llm_response.tool_calls:
            break

        # Execute each tool call concurrently (parallel execution)
        tool_result_futures = [call_tool(tool_call) for tool_call in llm_response.tool_calls]
        tool_results = [fut.result() for fut in tool_result_futures]

        # Append the model's response and tool responses to the message list
        messages = add_messages(messages, [llm_response, *tool_results])

        # Query the model again with the updated messages
        llm_response = call_model(messages).result()

    return llm_response
```

**Explanation:**  
- **Entrypoint Function (`agent`)**: This function manages the conversation.
  - It calls the model using `call_model` with the current messages.
  - It checks if the response contains any tool calls.
  - If there are tool calls, it executes them using `call_tool`, gathers the results, appends them to the conversation history using `add_messages`, and then re-calls the model.
  - This loop continues until no tool calls are generated, at which point the final response is returned.

---

## 🔍 Usage Example

To test our agent, we start by passing a user message:

```python
# Define a sample user message
user_message = {"role": "user", "content": "What's the weather in san francisco?"}
print(user_message)

# Stream the conversation using the agent
for step in agent.stream([user_message]):
    for task_name, message in step.items():
        if task_name == "agent":
            continue  # Skip internal updates; print tool/model responses
        print(f"\n{task_name}:")
        message.pretty_print()
```

**Explanation:**  
- We create a user message asking about the weather in San Francisco.
- The `agent.stream` function processes the conversation step-by-step.
- The output shows the tool call and the final model response after processing the tool's observation.

**Expected Output:**

```
{'role': 'user', 'content': "What's the weather in san francisco?"}

call_model:
================================== Ai Message ==================================
Tool Calls:
  get_weather (call_ID)
  Args:
    location: san francisco

call_tool:
================================== Tool Message ==================================
"It's sunny!"

call_model:
================================== Ai Message ==================================
The weather in San Francisco is sunny!
```

---

## 💾 Adding Thread-Level Persistence (Optional Advanced Feature)

For a persistent conversation, you can add thread-level persistence. This allows the agent to retain the full conversation history across multiple interactions.

### **Set Up a Checkpointer**

```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
```

### **Update the Entrypoint for Persistence**

```python
@entrypoint(checkpointer=checkpointer)
def agent(messages, previous):
    if previous is not None:
        messages = add_messages(previous, messages)

    llm_response = call_model(messages).result()
    while True:
        if not llm_response.tool_calls:
            break

        tool_result_futures = [call_tool(tool_call) for tool_call in llm_response.tool_calls]
        tool_results = [fut.result() for fut in tool_result_futures]
        messages = add_messages(messages, [llm_response, *tool_results])
        llm_response = call_model(messages).result()

    messages = add_messages(messages, llm_response)
    return entrypoint.final(value=llm_response, save=messages)
```

**Explanation:**  
- The entrypoint now accepts a `previous` messages state.
- If there is previous conversation history, it is appended to the current messages.
- The final call to `entrypoint.final` returns the result while saving the full conversation for future interactions.

### **Run with Thread-Level Persistence**

```python
config = {"configurable": {"thread_id": "1"}}
user_message = {"role": "user", "content": "What's the weather in san francisco?"}

for step in agent.stream([user_message], config):
    for task_name, message in step.items():
        if task_name == "agent":
            continue
        print(f"\n{task_name}:")
        message.pretty_print()
```

**Explanation:**  
- A configuration with a `thread_id` is provided to link conversation history.
- Subsequent messages will retain context, allowing the agent to use previous conversation history in its responses.

---

## 🌍 Real-World Use Cases

### Virtual Assistants & Chatbots
- **Weather Bots:** A virtual assistant that fetches current weather conditions using external APIs.
- **Customer Support:** A chatbot that calls APIs to retrieve order statuses, troubleshoot issues, or manage appointments.
- **Smart Home Systems:** Integrating external tools (e.g., thermostat controls, lighting systems) to respond dynamically to user commands.

### Personalized Recommendation Systems
- **E-commerce:** An agent that uses past purchase history and real-time queries to recommend products.
- **Travel Assistants:** Combining travel data and user preferences to provide customized itinerary suggestions.

---

## 🎯 Key Takeaways

- **Functional API Approach:**  
  Build your ReAct agent using modular tasks and an orchestrated entrypoint for maximum flexibility.
  
- **Tool-Calling Mechanism:**  
  The agent seamlessly integrates external tool calls (e.g., `get_weather`) into its reasoning loop.

- **Looping & Persistence:**  
  The agent continuously refines its response by iterating between the model and tool calls. With thread-level persistence, it retains full conversational context across interactions.

- **Real-World Applicability:**  
  This design can be applied to a wide range of applications—from weather bots and customer support to complex virtual assistants.

By creating a ReAct agent from scratch using the LangGraph Functional API, you gain full control over the agent’s behavior, making it highly customizable and adaptable to your specific needs. Happy coding!

---

# kimi response:
🚀 Creating a ReAct Agent from Scratch (Functional API)
📚 Introduction to ReAct Agents
A ReAct agent is a type of AI agent designed to interact with users and call external tools when needed. Think of it as a smart assistant that can not only chat with you but also perform specific tasks by leveraging external tools. For example, it can check the weather, book a flight, or even search the web.
🎯 Key Concepts
Queries to Chat Model: The agent sends user queries to a chat model (like OpenAI's GPT) to get responses.
Tool Calls: If the chat model suggests using a tool (like checking the weather), the agent executes that tool and gets the result.
Repeat Process: The agent keeps querying the chat model and executing tools until it has a complete response for the user.
🛠️ Setting Up the Environment
Before we create our ReAct agent, we need to set up the environment by installing necessary packages and configuring API keys.
📦 Installing Packages
bashCopy
%pip install -U langgraph langchain-openai
🔑 Setting API Keys
We need to set environment variables for our API keys. This ensures that our agent can securely access the chat model and other tools.
```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
🧩 Creating the ReAct Agent
Now that our environment is set up, let's create the ReAct agent step-by-step.
🛠️ Defining the Model and Tools
First, we define the tools and the chat model we will use. In this example, we'll create a simple tool to get the weather for a given location.
```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

model = ChatOpenAI(model="gpt-4o-mini")

@tool
def get_weather(location: str):
    """Call to get the weather from a specific location."""
    if any([city in location.lower() for city in ["sf", "san francisco"]]):
        return "It's sunny!"
    elif "boston" in location.lower():
        return "It's rainy!"
    else:
        return f"I am not sure what the weather is in {location}"

tools = [get_weather]
📝 Defining Tasks
Next, we define the tasks our agent will perform. These tasks include querying the chat model and executing tool calls.
```python
from langchain_core.messages import ToolMessage
from langgraph.func import entrypoint, task

tools_by_name = {tool.name: tool for tool in tools}

@task
def call_model(messages):
    """Call model with a sequence of messages."""
    response = model.bind_tools(tools).invoke(messages)
    return response

@task
def call_tool(tool_call):
    tool = tools_by_name[tool_call["name"]]
    observation = tool.invoke(tool_call["args"])
    return ToolMessage(content=observation, tool_call_id=tool_call["id"])
🔄 Defining the Entry Point
The entry point orchestrates the tasks. It queries the chat model, executes tool calls if needed, and repeats the process until it has a complete response.

```python
from langgraph.graph.message import add_messages

@entrypoint()
def agent(messages):
    llm_response = call_model(messages).result()
    while True:
        if not llm_response.tool_calls:
            break

        # Execute tools
        tool_result_futures = [
            call_tool(tool_call) for tool_call in llm_response.tool_calls
        ]
        tool_results = [fut.result() for fut in tool_result_futures]

        # Append to message list
        messages = add_messages(messages, [llm_response, *tool_results])

        # Call model again
        llm_response = call_model(messages).result()

    return llm_response
```
🎯 Using the Agent
To use our agent, we simply invoke it with a list of messages. The agent will query the chat model, execute any tool calls, and return the final response.
```python
user_message = {"role": "user", "content": "What's the weather in san francisco?"}
for step in agent.stream([user_message]):
    for task_name, message in step.items():
        if task_name == "agent":
            continue  # Just print task updates
        print(f"\n{task_name}:")
        message.pretty_print()
📝 Example Output
plaintextCopy
call_model:
==================================[1m Ai Message [0m==================================
Tool Calls:
  get_weather (call_tNnkrjnoz6MNfCHJpwfuEQ0v)
 Call ID: call_tNnkrjnoz6MNfCHJpwfuEQ0v
  Args:
    location: san francisco

call_tool:
=================================[1m Tool Message [0m=================================

It's sunny!

call_model:
==================================[1m Ai Message [0m==================================
```
The weather in San Francisco is sunny!
🔄 Adding Thread-Level Persistence
To support conversational experiences, we can add thread-level persistence. This allows the agent to remember previous messages and maintain context across multiple interactions.
📦 Setting Up Persistence
We use a checkpointer to save the conversation state.
```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()

@entrypoint(checkpointer=checkpointer)
def agent(messages, previous):
    if previous is not None:
        messages = add_messages(previous, messages)

    llm_response = call_model(messages).result()
    while True:
        if not llm_response.tool_calls:
            break

        # Execute tools
        tool_result_futures = [
            call_tool(tool_call) for tool_call in llm_response.tool_calls
        ]
        tool_results = [fut.result() for fut in tool_result_futures]

        # Append to message list
        messages = add_messages(messages, [llm_response, *tool_results])

        # Call model again
        llm_response = call_model(messages).result()

    # Generate final response
    messages = add_messages(messages, llm_response)
    return entrypoint.final(value=llm_response, save=messages)

```
📝 Using Persistence
We pass a configuration object to specify the conversation thread.
```python
config = {"configurable": {"thread_id": "1"}}

user_message = {"role": "user", "content": "What's the weather in san francisco?"}
for step in agent.stream([user_message], config):
    for task_name, message in step.items():
        if task_name == "agent":
            continue  # Just print task updates
        print(f"\n{task_name}:")
        message.pretty_print()

```
📝 Example Follow-Up
```python
user_message = {"role": "user", "content": "How does it compare to Boston, MA?"}
for step in agent.stream([user_message], config):
    for task_name, message in step.items():
        if task_name == "agent":
            continue  # Just print task updates
        print(f"\n{task_name}:")
        message.pretty_print()
📝 Example Output
plaintextCopy
call_model:
==================================[1m Ai Message [0m==================================
Tool Calls:
  get_weather (call_8sTKYAhSIHOdjLD5d6gaswuV)
 Call ID: call_8sTKYAhSIHOdjLD5d6gaswuV
  Args:
    location: Boston, MA

call_tool:
=================================[1m Tool Message [0m=================================

It's rainy!

call_model:
==================================[1m Ai Message [0m==================================

```
Compared to San Francisco, which is sunny, Boston, MA is experiencing rainy weather.
🌐 Real-World Use Case
Imagine you are building a customer service chatbot for a travel company. The chatbot can answer general questions but also call external APIs to check flight availability, hotel prices, or even the weather at the destination. By using a ReAct agent, you can seamlessly integrate these capabilities into a single conversational interface.
📝 Example Scenario
User: "I'm planning a trip to Paris. What's the weather like there?"
Agent: "Let me check the weather for you."
Agent: "The weather in Paris is sunny with a high of 22°C."
User: "Great! Can you also check if there are any flights available next week?"
Agent: "Sure, let me look up the flights for you."
Agent: "There are several flights available next week. Would you like me to book one for you?"
🎯 Summary
Creating a ReAct agent involves setting up the environment, defining the model and tools, orchestrating tasks, and optionally adding thread-level persistence. This setup allows you to build versatile AI agents that can interact with users and perform complex tasks by leveraging external tools.
By following this guide, you can create your own ReAct agent and extend it with additional features like memory, human-in-the-loop capabilities, and more.