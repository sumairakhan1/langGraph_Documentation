# 3. How to pass runtime values to tools

Hereâ€™s a deep and beginner-friendly explanation of how to **pass runtime values to tools** in **LangChain** and **LangGraph**, complete with real-world use cases, a step-by-step breakdown, and annotated code examples.  

---

# ğŸ”¹ Passing Runtime Values to Tools in LangChain and LangGraph  

## ğŸ“Œ **Introduction**  
When working with **LangChain** or **LangGraph**, we often need to pass values dynamically at runtime instead of letting the LLM populate all tool parameters. This is crucial when:  
- A tool requires **sensitive values** that shouldn't be exposed to the LLM.  
- A tool needs **context** from past steps (e.g., message history, shared state).  
- We want to **inject external dependencies** like API keys or database connections.  

### ğŸ›  **Real-World Use Case**  
Imagine you're building an **AI customer support chatbot**. The chatbot must:  
1. **Retrieve past chat history** to provide relevant responses.  
2. **Access an internal knowledge base** that the LLM should not see directly.  
3. **Fetch user account details** securely without exposing them to the LLM.  

To achieve this, we can use **Injected Arguments** to pass runtime values dynamically.  

---

## ğŸ”¹ **Understanding Injected Arguments in LangChain**  
In **LangChain**, we can use `InjectedToolArg` to **hide specific arguments from the LLM** and populate them at runtime.  

### ğŸ”¹ **Key Concepts**  
1. **Injected Arguments (`InjectedToolArg`)**: Used to pass values manually at runtime instead of exposing them to the LLM.  
2. **RunnableConfig**: A configuration object that allows controlling execution.  
3. **Injected State (`InjectedState`)**: Used in **LangGraph** to pass values dynamically from graph state or memory.  

---

# ğŸš€ **Code Example 1: Using Injected Arguments in LangChain**  

```python
from typing import Annotated
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import InjectedToolArg

# Define a simple tool function
def my_tool(
    user_input: str,  # LLM-provided input
    secret_key: Annotated[str, InjectedToolArg],  # Injected argument
    config: RunnableConfig  # LangChain config (not seen by LLM)
):
    """Secure tool that uses an injected API key."""
    print(f"User input: {user_input}")
    print(f"Using API key: {secret_key}")  # This is injected securely
    return "Processed request"

# Example runtime execution
config = RunnableConfig()  # Create an empty config
my_tool("Hello, AI!", secret_key="MY_SECRET_KEY", config=config)
```

### âœ… **Code Explanation**  
1. **`user_input`** â†’ Provided by the LLM.  
2. **`secret_key`** â†’ Marked with `InjectedToolArg`, meaning it's not visible to the LLM but passed at runtime.  
3. **`RunnableConfig`** â†’ Allows configuration of tool execution.  
4. **At runtime**, the secret key is securely injected.  

ğŸ’¡ **Use Case**: Securely pass API keys, authentication tokens, or internal data to tools **without exposing them to the LLM**.  

---

# ğŸš€ **Code Example 2: Passing State to Tools in LangGraph**  

In **LangGraph**, tools may need **past conversation history** or **shared memory**. We use **InjectedState** to pass such data at runtime.  

### ğŸ”¹ **Define a Shared Graph State**  
```python
from typing import List
from langgraph.prebuilt.chat_agent_executor import AgentState

# Define a shared state schema
class State(AgentState):
    messages: List[str]  # Stores past chat messages
```

### ğŸ”¹ **Pass State to a Tool**  
```python
from typing import Annotated
from langgraph.store.base import BaseStore
from langgraph.prebuilt import InjectedState, InjectedStore

# Tool function that uses injected state
async def my_tool(
    user_query: str,  # Provided by LLM
    store: Annotated[BaseStore, InjectedStore],  # Injected store
    state: Annotated[State, InjectedState],  # Injected shared state
    chat_history: Annotated[list, InjectedState("messages")]  # Extract only "messages" from state
):
    """Tool that uses past conversation history."""
    print(f"User asked: {user_query}")
    print(f"Past messages: {chat_history}")
    return "Here's a response based on chat history!"
```

### âœ… **Code Explanation**  
1. **`user_query`** â†’ Comes from the LLM.  
2. **`store`** â†’ Injected store (e.g., database, cache).  
3. **`state`** â†’ Injected shared state (stores conversation memory).  
4. **`chat_history`** â†’ Extracts only the "messages" field from state.  

ğŸ’¡ **Use Case**: AI chatbots that remember previous conversations **without needing the LLM to track state explicitly**.  

---

# ğŸ”¹ **Installing and Configuring LangGraph**  
To run the above examples, install the required packages:  

```sh
pip install --quiet -U langgraph langchain-openai
```

### ğŸ”¹ **Set Up API Keys**  
```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

ğŸ’¡ **Use Case**: Securely store API keys for OpenAI without hardcoding them.  

---

# ğŸ”¹ **Summary**  
âœ… **What We Learned**  
- **Injected Arguments (`InjectedToolArg`)** hide certain tool arguments from the LLM.  
- **Injected State (`InjectedState`)** in LangGraph allows passing shared state dynamically.  
- **Use Cases** include **secure API access, AI chat memory, and dynamic context-aware tool execution**.  

âœ… **When to Use This Approach**  
| Scenario | Solution |
|---|---|
| Secure API keys & sensitive data | `InjectedToolArg` |
| Store past conversation history | `InjectedState` |
| Pass shared memory (e.g., cache) | `InjectedStore` |

ğŸ’¡ **Real-world applications**:  
- AI-powered **customer support chatbots**.  
- **Finance AI assistants** requiring secure API access.  
- **Medical AI advisors** that recall patient history.  

ğŸš€ **Next Steps**  
- Try integrating this in your own **LangGraph agent workflows**.  
- Experiment with different **state schemas** for your applications.  

ğŸ”¹ **Got questions? Letâ€™s discuss below!** ğŸš€

---

# ğŸš€ **Passing Runtime Values to Tools in LangChain and LangGraph**  

When building AI applications with **LangChain** and **LangGraph**, there are scenarios where tools need access to runtime values, such as **graph state, shared memory, or dynamic user input**. However, we donâ€™t want the **LLM to generate these values**â€”instead, they should be **injected at runtime**.  

This guide will explain:  
âœ… What **injected arguments** are  
âœ… How to **pass graph state** to tools  
âœ… How to define **tools and graphs**  
âœ… Real-world use cases  
âœ… **Code examples** with detailed explanations  

---

## ğŸ§© **What is InjectedState?**  

Sometimes, we need to provide **external values** to tools at runtime, but we donâ€™t want the LLM to generate these values itself. This is where `InjectedState` and `InjectedToolArg` come in:  

- `InjectedState`: Injects **graph state** (e.g., shared memory or context) into tools at runtime.  
- `InjectedToolArg`: Hides specific tool arguments from the LLM and requires manual input.  

ğŸ“Œ **Example Use Case:**  
Imagine you're building an AI **news summarizer** that retrieves real-time news articles. You donâ€™t want the LLM to **generate fake news**â€”instead, you want to inject **real articles** from a database.  

---

## ğŸ—ï¸ **Defining Tools with InjectedState**  

We'll define a **tool that retrieves relevant documents** (context) for answering a userâ€™s question.  

### ğŸ”¹ **Code Example: Define a Tool with InjectedState**  

```python
from typing import List, Annotated
from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState

@tool
def get_context(question: str, state: Annotated[dict, InjectedState]):
    """Get relevant context for answering the question."""
    return "\n\n".join(doc for doc in state["docs"])
```

### ğŸ“ **Explanation of the Code:**  

ğŸ”¹ `@tool`: This **registers** the function as a tool in LangChain.  
ğŸ”¹ `question: str`: The **LLM provides** the question as input.  
ğŸ”¹ `state: Annotated[dict, InjectedState]`:  
   - This **injects** the graph state (past messages, documents, or external data).  
   - The **LLM does not generate this input**â€”it comes from our system.  
ğŸ”¹ `state["docs"]`: Retrieves documents stored in the state.  
ğŸ”¹ `return "\n\n".join(doc for doc in state["docs"])`: Combines all documents into a single response.  

ğŸ“Œ **Key Takeaway:** `InjectedState` ensures that **external knowledge (documents, memory, or stored values)** is automatically provided to the tool.  

---

## ğŸ—ï¸ **Defining the Graph**  

Next, weâ€™ll define a **graph** using LangGraphâ€™s `ToolNode`, which automatically **injects state into tools**.  

### ğŸ”¹ **Code Example: Define a Graph**  

```python
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import ToolNode, create_react_agent
from langgraph.checkpoint.memory import MemorySaver

# Define the AI model
model = ChatOpenAI(model="gpt-4o", temperature=0)

# Define the tool list
tools = [get_context]

# Create a tool node
tool_node = ToolNode(tools)

# Create a memory checkpoint
checkpointer = MemorySaver()

# Define the graph
graph = create_react_agent(model, tools, state_schema=State, checkpointer=checkpointer)
```

### ğŸ“ **Explanation of the Code:**  

ğŸ”¹ `ChatOpenAI(model="gpt-4o", temperature=0)`:  
   - Uses OpenAIâ€™s **GPT-4o model** with a **temperature of 0** (more deterministic responses).  

ğŸ”¹ `tools = [get_context]`:  
   - Specifies **which tools** the agent will use.  

ğŸ”¹ `ToolNode(tools)`:  
   - Automatically injects **graph state** into tools.  

ğŸ”¹ `checkpointer = MemorySaver()`:  
   - Saves intermediate results to **memory** for tracking.  

ğŸ”¹ `create_react_agent(model, tools, state_schema=State, checkpointer=checkpointer)`:  
   - **Creates an AI agent** that can **reason and call tools** dynamically.  
   - Uses **ReAct (Reason + Act) framework** for intelligent decision-making.  

ğŸ“Œ **Key Takeaway:** The graph defines **how the AI agent interacts** with tools and handles state.  

---

## ğŸ› ï¸ **Using the Graph to Process a Question**  

Letâ€™s test our setup by asking the AI **for news about "FooBar" company**.  

### ğŸ”¹ **Code Example: Query the AI Agent**  

```python
docs = [
    "FooBar company just raised 1 Billion dollars!",
    "FooBar company was founded in 2019",
]

# Define inputs for the AI
inputs = {
    "messages": [{"type": "user", "content": "what's the latest news about FooBar"}],
    "docs": docs,  # Injecting external data
}

# Configuration settings
config = {"configurable": {"thread_id": "1"}}

# Stream the output from the AI agent
for chunk in graph.stream(inputs, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

### ğŸ“ **Explanation of the Code:**  

ğŸ”¹ `docs = [...]`:  
   - Defines **real-world news articles** that the AI will reference.  

ğŸ”¹ `inputs = { "messages": [...], "docs": docs }`:  
   - Passes the **userâ€™s query and real documents** to the agent.  

ğŸ”¹ `config = {"configurable": {"thread_id": "1"}}`:  
   - Helps track conversation **threads in memory**.  

ğŸ”¹ `for chunk in graph.stream(...)`:  
   - Streams the AIâ€™s response in **real-time**.  

ğŸ“Œ **Expected Output:**  

```
================================ Human Message ================================
what's the latest news about FooBar

================================ Ai Message ===================================
Tool Calls:
  get_context (call_UkqfR7z2cLJQjhatUpDeEa5H)
 Call ID: call_UkqfR7z2cLJQjhatUpDeEa5H
  Args:
    question: what's the latest news about FooBar

================================ Tool Message =================================
Name: get_context

FooBar company just raised 1 Billion dollars!

FooBar company was founded in 2019

================================ Ai Message ===================================
The latest news about FooBar is that the company has just raised 1 billion dollars.
```

ğŸ¯ **Final AI Response:**  
âœ… The AI retrieves **real news articles** instead of making up an answer.  

---

## ğŸŒ **Real-World Use Cases**  

ğŸš€ **Where Can This Be Used?**  

1ï¸âƒ£ **News Summarization** â€“ AI retrieves **real-time news** instead of hallucinating facts.  
2ï¸âƒ£ **Customer Support Bots** â€“ Pulls data from **FAQs and past support tickets**.  
3ï¸âƒ£ **Legal Research** â€“ AI retrieves **relevant laws and cases** instead of guessing.  
4ï¸âƒ£ **E-Commerce Chatbots** â€“ Suggests **products based on stock availability**.  

ğŸ“Œ **Key Benefit:** **Accurate responses** by injecting **real data** into AI tools.  

---

## ğŸ¯ **Final Takeaways**  

ğŸ”¹ `InjectedState` ensures **graph state is passed to tools** dynamically.  
ğŸ”¹ **ToolNode** automatically **injects external data** (documents, messages, context).  
ğŸ”¹ The AI agent **retrieves real data** instead of generating false information.  
ğŸ”¹ **Useful in real-world AI applications** like news summarization, customer support, and legal research.  

---

ğŸ’¡ **Want to build smarter AI applications?**  
Use **LangChain + LangGraph** to create **state-aware AI agents** that retrieve **real data** dynamically! ğŸš€

---

# ğŸš€ **Understanding Shared Memory (Store) in LangGraph for AI Agents**  

LangGraph allows us to build powerful AI agents that can remember past interactions by storing shared memory. In this guide, weâ€™ll break down how to use **InjectedStore** to pass shared memory across different users in an AI agent.  

## ğŸ§  **What is Shared Memory (Store) in LangGraph?**  
When building AI-powered chatbots or virtual assistants, it's essential to store and retrieve past information. LangGraph provides **InjectedStore**, which allows tools to access shared memory across multiple conversations or users.  

### âœ… **Real-World Use Case**  
Imagine building a **customer support chatbot** for an e-commerce website. Different users interact with the bot, and each user should only access their own order history. Using **InjectedStore**, we can store each user's data separately and retrieve the relevant information when needed.  

---

# ğŸ“Œ **Setting Up Shared Memory Using LangGraph Store**
## ğŸ”¹ **Step 1: Install Required Dependencies**  
Ensure you have the correct versions installed:  
```bash
pip install langchain-core langgraph
```
ğŸš€ **Note:** InjectedStore requires **langchain-core >= 0.3.8** and **LangGraph v0.2.34**.  

---

## ğŸ”¹ **Step 2: Initialize an In-Memory Store**  
First, we create a shared memory store using `InMemoryStore()`.  
This store will hold user-specific data.  

```python
from langgraph.store.memory import InMemoryStore

# Create an in-memory document store
doc_store = InMemoryStore()

# Store documents for different users
doc_store.put(("documents", "1"), "doc_0", {"doc": "FooBar company just raised 1 Billion dollars!"})
doc_store.put(("documents", "2"), "doc_1", {"doc": "FooBar company was founded in 2019"})
```
### ğŸ” **Explanation**
- We create a **doc_store** to hold documents.
- We use **namespaces** (`("documents", "1")`) to store data separately for each user.
- **User "1"** has financial news, while **User "2"** has company history.

---

## ğŸ”¹ **Step 3: Define the Tool to Retrieve Context**  
Next, we define a tool that will fetch the relevant document for each user.

```python
from langgraph.store.base import BaseStore
from langchain_core.runnables import RunnableConfig
from typing import List, Tuple
from typing_extensions import Annotated
from langgraph.prebuilt import InjectedStore
from langchain_core.tools import tool

@tool
def get_context(
    question: str,
    config: RunnableConfig,
    store: Annotated[BaseStore, InjectedStore()],
) -> Tuple[str, List[str]]:
    """Get relevant context for answering the question."""
    user_id = config.get("configurable", {}).get("user_id")  # Get user ID from config
    docs = [item.value["doc"] for item in store.search(("documents", user_id))]  # Fetch docs for the user
    return "\n\n".join(doc for doc in docs)
```
### ğŸ” **Explanation**
- **`@tool`** â†’ Registers `get_context` as a tool.
- **`config: RunnableConfig`** â†’ Stores user-specific configuration (like `user_id`).
- **`store: Annotated[BaseStore, InjectedStore()]`** â†’ Injects the shared memory store.
- **`user_id = config.get("configurable", {}).get("user_id")`** â†’ Retrieves the **user ID** from the request.
- **`store.search(("documents", user_id))`** â†’ Searches for documents related to the given user.

---

## ğŸ”¹ **Step 4: Define the Graph and Model**
Now, we set up the AI agent and graph.

```python
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import ToolNode, create_react_agent
from langgraph.checkpoint.memory import MemorySaver

# Define the AI model
model = ChatOpenAI(model="gpt-4o", temperature=0)

# Define tools
tools = [get_context]

# Create a tool node to manage state injection
tool_node = ToolNode(tools)

# Create a memory saver
checkpointer = MemorySaver()

# Create the agent with memory store
graph = create_react_agent(model, tools, checkpointer=checkpointer, store=doc_store)
```

### ğŸ” **Explanation**
- **`ChatOpenAI(model="gpt-4o", temperature=0)`** â†’ Defines an **LLM model**.
- **`ToolNode(tools)`** â†’ Handles **automatic state injection** for tools.
- **`MemorySaver()`** â†’ Saves intermediate conversation states.
- **`create_react_agent(...)`** â†’ Creates the **AI agent** with memory access.

---

## ğŸ”¹ **Step 5: Running the AI Agent with Different Users**
Now, letâ€™s test the agent with two different users.  

### ğŸ‘¤ **Test Case 1: User "1" asks about FooBar news**
```python
messages = [{"type": "user", "content": "what's the latest news about FooBar"}]
config = {"configurable": {"thread_id": "1", "user_id": "1"}}

for chunk in graph.stream({"messages": messages}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```
### ğŸ’¡ **Expected Output**
```
================================ Human Message ================================
what's the latest news about FooBar

================================ Ai Message ================================
Tool Calls:
  get_context (call_ocyHBpGgF3LPFOgRKURBfkGG)
  Args:
    question: latest news about FooBar

================================ Tool Message ================================
Name: get_context
FooBar company just raised 1 Billion dollars!

================================ Ai Message ================================
The latest news about FooBar is that the company has just raised 1 billion dollars.
```
ğŸš€ **User "1" successfully retrieves their relevant document!**  

---

### ğŸ‘¤ **Test Case 2: User "2" asks about FooBar news**
```python
messages = [{"type": "user", "content": "what's the latest news about FooBar"}]
config = {"configurable": {"thread_id": "2", "user_id": "2"}}

for chunk in graph.stream({"messages": messages}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```
### ğŸ’¡ **Expected Output**
```
================================ Human Message ================================
what's the latest news about FooBar

================================ Ai Message ================================
Tool Calls:
  get_context (call_zxO9KVlL8UxFQUMb8ETeHNvs)
  Args:
    question: latest news about FooBar

================================ Tool Message ================================
Name: get_context
FooBar company was founded in 2019

================================ Ai Message ================================
FooBar company was founded in 2019. If you need more specific or recent news, please let me know!
```
ğŸ¯ **User "2" gets a completely different response based on stored memory!**  

---

# ğŸ¯ **Key Takeaways**
âœ… **InjectedStore** allows AI tools to access shared memory, ensuring relevant information retrieval.  
âœ… AI tools can **store and retrieve** user-specific data dynamically.  
âœ… **Real-world applications** include chatbots, recommendation systems, and AI-powered customer support.  
âœ… Using **LangGraph's Store API**, we can manage multi-user memory efficiently.  

---

# ğŸ¬ **Final Thoughts**
This guide demonstrated **how to store and retrieve user-specific data in an AI agent using LangGraph's InjectedStore**. This technique is **essential** for building personalized AI systems like virtual assistants, chatbots, or multi-user AI applications.  

ğŸš€ Now you can apply this knowledge to **create AI-powered apps that remember users and provide relevant responses dynamically!** ğŸ‰