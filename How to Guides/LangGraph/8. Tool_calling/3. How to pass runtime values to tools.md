# 3. How to pass runtime values to tools

Here’s a deep and beginner-friendly explanation of how to **pass runtime values to tools** in **LangChain** and **LangGraph**, complete with real-world use cases, a step-by-step breakdown, and annotated code examples.  

---

# 🔹 Passing Runtime Values to Tools in LangChain and LangGraph  

## 📌 **Introduction**  
When working with **LangChain** or **LangGraph**, we often need to pass values dynamically at runtime instead of letting the LLM populate all tool parameters. This is crucial when:  
- A tool requires **sensitive values** that shouldn't be exposed to the LLM.  
- A tool needs **context** from past steps (e.g., message history, shared state).  
- We want to **inject external dependencies** like API keys or database connections.  

### 🛠 **Real-World Use Case**  
Imagine you're building an **AI customer support chatbot**. The chatbot must:  
1. **Retrieve past chat history** to provide relevant responses.  
2. **Access an internal knowledge base** that the LLM should not see directly.  
3. **Fetch user account details** securely without exposing them to the LLM.  

To achieve this, we can use **Injected Arguments** to pass runtime values dynamically.  

---

## 🔹 **Understanding Injected Arguments in LangChain**  
In **LangChain**, we can use `InjectedToolArg` to **hide specific arguments from the LLM** and populate them at runtime.  

### 🔹 **Key Concepts**  
1. **Injected Arguments (`InjectedToolArg`)**: Used to pass values manually at runtime instead of exposing them to the LLM.  
2. **RunnableConfig**: A configuration object that allows controlling execution.  
3. **Injected State (`InjectedState`)**: Used in **LangGraph** to pass values dynamically from graph state or memory.  

---

# 🚀 **Code Example 1: Using Injected Arguments in LangChain**  

```python
from typing import Annotated
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import InjectedToolArg

# Define a simple tool function
def my_tool(
    user_input: str,  # LLM-provided input
    secret_key: Annotated[str, InjectedToolArg],  # Injected argument
    config: RunnableConfig  # LangChain config (not seen by LLM)
):
    """Secure tool that uses an injected API key."""
    print(f"User input: {user_input}")
    print(f"Using API key: {secret_key}")  # This is injected securely
    return "Processed request"

# Example runtime execution
config = RunnableConfig()  # Create an empty config
my_tool("Hello, AI!", secret_key="MY_SECRET_KEY", config=config)
```

### ✅ **Code Explanation**  
1. **`user_input`** → Provided by the LLM.  
2. **`secret_key`** → Marked with `InjectedToolArg`, meaning it's not visible to the LLM but passed at runtime.  
3. **`RunnableConfig`** → Allows configuration of tool execution.  
4. **At runtime**, the secret key is securely injected.  

💡 **Use Case**: Securely pass API keys, authentication tokens, or internal data to tools **without exposing them to the LLM**.  

---

# 🚀 **Code Example 2: Passing State to Tools in LangGraph**  

In **LangGraph**, tools may need **past conversation history** or **shared memory**. We use **InjectedState** to pass such data at runtime.  

### 🔹 **Define a Shared Graph State**  
```python
from typing import List
from langgraph.prebuilt.chat_agent_executor import AgentState

# Define a shared state schema
class State(AgentState):
    messages: List[str]  # Stores past chat messages
```

### 🔹 **Pass State to a Tool**  
```python
from typing import Annotated
from langgraph.store.base import BaseStore
from langgraph.prebuilt import InjectedState, InjectedStore

# Tool function that uses injected state
async def my_tool(
    user_query: str,  # Provided by LLM
    store: Annotated[BaseStore, InjectedStore],  # Injected store
    state: Annotated[State, InjectedState],  # Injected shared state
    chat_history: Annotated[list, InjectedState("messages")]  # Extract only "messages" from state
):
    """Tool that uses past conversation history."""
    print(f"User asked: {user_query}")
    print(f"Past messages: {chat_history}")
    return "Here's a response based on chat history!"
```

### ✅ **Code Explanation**  
1. **`user_query`** → Comes from the LLM.  
2. **`store`** → Injected store (e.g., database, cache).  
3. **`state`** → Injected shared state (stores conversation memory).  
4. **`chat_history`** → Extracts only the "messages" field from state.  

💡 **Use Case**: AI chatbots that remember previous conversations **without needing the LLM to track state explicitly**.  

---

# 🔹 **Installing and Configuring LangGraph**  
To run the above examples, install the required packages:  

```sh
pip install --quiet -U langgraph langchain-openai
```

### 🔹 **Set Up API Keys**  
```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

💡 **Use Case**: Securely store API keys for OpenAI without hardcoding them.  

---

# 🔹 **Summary**  
✅ **What We Learned**  
- **Injected Arguments (`InjectedToolArg`)** hide certain tool arguments from the LLM.  
- **Injected State (`InjectedState`)** in LangGraph allows passing shared state dynamically.  
- **Use Cases** include **secure API access, AI chat memory, and dynamic context-aware tool execution**.  

✅ **When to Use This Approach**  
| Scenario | Solution |
|---|---|
| Secure API keys & sensitive data | `InjectedToolArg` |
| Store past conversation history | `InjectedState` |
| Pass shared memory (e.g., cache) | `InjectedStore` |

💡 **Real-world applications**:  
- AI-powered **customer support chatbots**.  
- **Finance AI assistants** requiring secure API access.  
- **Medical AI advisors** that recall patient history.  

🚀 **Next Steps**  
- Try integrating this in your own **LangGraph agent workflows**.  
- Experiment with different **state schemas** for your applications.  

🔹 **Got questions? Let’s discuss below!** 🚀

---

# 🚀 **Passing Runtime Values to Tools in LangChain and LangGraph**  

When building AI applications with **LangChain** and **LangGraph**, there are scenarios where tools need access to runtime values, such as **graph state, shared memory, or dynamic user input**. However, we don’t want the **LLM to generate these values**—instead, they should be **injected at runtime**.  

This guide will explain:  
✅ What **injected arguments** are  
✅ How to **pass graph state** to tools  
✅ How to define **tools and graphs**  
✅ Real-world use cases  
✅ **Code examples** with detailed explanations  

---

## 🧩 **What is InjectedState?**  

Sometimes, we need to provide **external values** to tools at runtime, but we don’t want the LLM to generate these values itself. This is where `InjectedState` and `InjectedToolArg` come in:  

- `InjectedState`: Injects **graph state** (e.g., shared memory or context) into tools at runtime.  
- `InjectedToolArg`: Hides specific tool arguments from the LLM and requires manual input.  

📌 **Example Use Case:**  
Imagine you're building an AI **news summarizer** that retrieves real-time news articles. You don’t want the LLM to **generate fake news**—instead, you want to inject **real articles** from a database.  

---

## 🏗️ **Defining Tools with InjectedState**  

We'll define a **tool that retrieves relevant documents** (context) for answering a user’s question.  

### 🔹 **Code Example: Define a Tool with InjectedState**  

```python
from typing import List, Annotated
from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState

@tool
def get_context(question: str, state: Annotated[dict, InjectedState]):
    """Get relevant context for answering the question."""
    return "\n\n".join(doc for doc in state["docs"])
```

### 📝 **Explanation of the Code:**  

🔹 `@tool`: This **registers** the function as a tool in LangChain.  
🔹 `question: str`: The **LLM provides** the question as input.  
🔹 `state: Annotated[dict, InjectedState]`:  
   - This **injects** the graph state (past messages, documents, or external data).  
   - The **LLM does not generate this input**—it comes from our system.  
🔹 `state["docs"]`: Retrieves documents stored in the state.  
🔹 `return "\n\n".join(doc for doc in state["docs"])`: Combines all documents into a single response.  

📌 **Key Takeaway:** `InjectedState` ensures that **external knowledge (documents, memory, or stored values)** is automatically provided to the tool.  

---

## 🏗️ **Defining the Graph**  

Next, we’ll define a **graph** using LangGraph’s `ToolNode`, which automatically **injects state into tools**.  

### 🔹 **Code Example: Define a Graph**  

```python
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import ToolNode, create_react_agent
from langgraph.checkpoint.memory import MemorySaver

# Define the AI model
model = ChatOpenAI(model="gpt-4o", temperature=0)

# Define the tool list
tools = [get_context]

# Create a tool node
tool_node = ToolNode(tools)

# Create a memory checkpoint
checkpointer = MemorySaver()

# Define the graph
graph = create_react_agent(model, tools, state_schema=State, checkpointer=checkpointer)
```

### 📝 **Explanation of the Code:**  

🔹 `ChatOpenAI(model="gpt-4o", temperature=0)`:  
   - Uses OpenAI’s **GPT-4o model** with a **temperature of 0** (more deterministic responses).  

🔹 `tools = [get_context]`:  
   - Specifies **which tools** the agent will use.  

🔹 `ToolNode(tools)`:  
   - Automatically injects **graph state** into tools.  

🔹 `checkpointer = MemorySaver()`:  
   - Saves intermediate results to **memory** for tracking.  

🔹 `create_react_agent(model, tools, state_schema=State, checkpointer=checkpointer)`:  
   - **Creates an AI agent** that can **reason and call tools** dynamically.  
   - Uses **ReAct (Reason + Act) framework** for intelligent decision-making.  

📌 **Key Takeaway:** The graph defines **how the AI agent interacts** with tools and handles state.  

---

## 🛠️ **Using the Graph to Process a Question**  

Let’s test our setup by asking the AI **for news about "FooBar" company**.  

### 🔹 **Code Example: Query the AI Agent**  

```python
docs = [
    "FooBar company just raised 1 Billion dollars!",
    "FooBar company was founded in 2019",
]

# Define inputs for the AI
inputs = {
    "messages": [{"type": "user", "content": "what's the latest news about FooBar"}],
    "docs": docs,  # Injecting external data
}

# Configuration settings
config = {"configurable": {"thread_id": "1"}}

# Stream the output from the AI agent
for chunk in graph.stream(inputs, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

### 📝 **Explanation of the Code:**  

🔹 `docs = [...]`:  
   - Defines **real-world news articles** that the AI will reference.  

🔹 `inputs = { "messages": [...], "docs": docs }`:  
   - Passes the **user’s query and real documents** to the agent.  

🔹 `config = {"configurable": {"thread_id": "1"}}`:  
   - Helps track conversation **threads in memory**.  

🔹 `for chunk in graph.stream(...)`:  
   - Streams the AI’s response in **real-time**.  

📌 **Expected Output:**  

```
================================ Human Message ================================
what's the latest news about FooBar

================================ Ai Message ===================================
Tool Calls:
  get_context (call_UkqfR7z2cLJQjhatUpDeEa5H)
 Call ID: call_UkqfR7z2cLJQjhatUpDeEa5H
  Args:
    question: what's the latest news about FooBar

================================ Tool Message =================================
Name: get_context

FooBar company just raised 1 Billion dollars!

FooBar company was founded in 2019

================================ Ai Message ===================================
The latest news about FooBar is that the company has just raised 1 billion dollars.
```

🎯 **Final AI Response:**  
✅ The AI retrieves **real news articles** instead of making up an answer.  

---

## 🌍 **Real-World Use Cases**  

🚀 **Where Can This Be Used?**  

1️⃣ **News Summarization** – AI retrieves **real-time news** instead of hallucinating facts.  
2️⃣ **Customer Support Bots** – Pulls data from **FAQs and past support tickets**.  
3️⃣ **Legal Research** – AI retrieves **relevant laws and cases** instead of guessing.  
4️⃣ **E-Commerce Chatbots** – Suggests **products based on stock availability**.  

📌 **Key Benefit:** **Accurate responses** by injecting **real data** into AI tools.  

---

## 🎯 **Final Takeaways**  

🔹 `InjectedState` ensures **graph state is passed to tools** dynamically.  
🔹 **ToolNode** automatically **injects external data** (documents, messages, context).  
🔹 The AI agent **retrieves real data** instead of generating false information.  
🔹 **Useful in real-world AI applications** like news summarization, customer support, and legal research.  

---

💡 **Want to build smarter AI applications?**  
Use **LangChain + LangGraph** to create **state-aware AI agents** that retrieve **real data** dynamically! 🚀

---

# 🚀 **Understanding Shared Memory (Store) in LangGraph for AI Agents**  

LangGraph allows us to build powerful AI agents that can remember past interactions by storing shared memory. In this guide, we’ll break down how to use **InjectedStore** to pass shared memory across different users in an AI agent.  

## 🧠 **What is Shared Memory (Store) in LangGraph?**  
When building AI-powered chatbots or virtual assistants, it's essential to store and retrieve past information. LangGraph provides **InjectedStore**, which allows tools to access shared memory across multiple conversations or users.  

### ✅ **Real-World Use Case**  
Imagine building a **customer support chatbot** for an e-commerce website. Different users interact with the bot, and each user should only access their own order history. Using **InjectedStore**, we can store each user's data separately and retrieve the relevant information when needed.  

---

# 📌 **Setting Up Shared Memory Using LangGraph Store**
## 🔹 **Step 1: Install Required Dependencies**  
Ensure you have the correct versions installed:  
```bash
pip install langchain-core langgraph
```
🚀 **Note:** InjectedStore requires **langchain-core >= 0.3.8** and **LangGraph v0.2.34**.  

---

## 🔹 **Step 2: Initialize an In-Memory Store**  
First, we create a shared memory store using `InMemoryStore()`.  
This store will hold user-specific data.  

```python
from langgraph.store.memory import InMemoryStore

# Create an in-memory document store
doc_store = InMemoryStore()

# Store documents for different users
doc_store.put(("documents", "1"), "doc_0", {"doc": "FooBar company just raised 1 Billion dollars!"})
doc_store.put(("documents", "2"), "doc_1", {"doc": "FooBar company was founded in 2019"})
```
### 🔍 **Explanation**
- We create a **doc_store** to hold documents.
- We use **namespaces** (`("documents", "1")`) to store data separately for each user.
- **User "1"** has financial news, while **User "2"** has company history.

---

## 🔹 **Step 3: Define the Tool to Retrieve Context**  
Next, we define a tool that will fetch the relevant document for each user.

```python
from langgraph.store.base import BaseStore
from langchain_core.runnables import RunnableConfig
from typing import List, Tuple
from typing_extensions import Annotated
from langgraph.prebuilt import InjectedStore
from langchain_core.tools import tool

@tool
def get_context(
    question: str,
    config: RunnableConfig,
    store: Annotated[BaseStore, InjectedStore()],
) -> Tuple[str, List[str]]:
    """Get relevant context for answering the question."""
    user_id = config.get("configurable", {}).get("user_id")  # Get user ID from config
    docs = [item.value["doc"] for item in store.search(("documents", user_id))]  # Fetch docs for the user
    return "\n\n".join(doc for doc in docs)
```
### 🔍 **Explanation**
- **`@tool`** → Registers `get_context` as a tool.
- **`config: RunnableConfig`** → Stores user-specific configuration (like `user_id`).
- **`store: Annotated[BaseStore, InjectedStore()]`** → Injects the shared memory store.
- **`user_id = config.get("configurable", {}).get("user_id")`** → Retrieves the **user ID** from the request.
- **`store.search(("documents", user_id))`** → Searches for documents related to the given user.

---

## 🔹 **Step 4: Define the Graph and Model**
Now, we set up the AI agent and graph.

```python
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import ToolNode, create_react_agent
from langgraph.checkpoint.memory import MemorySaver

# Define the AI model
model = ChatOpenAI(model="gpt-4o", temperature=0)

# Define tools
tools = [get_context]

# Create a tool node to manage state injection
tool_node = ToolNode(tools)

# Create a memory saver
checkpointer = MemorySaver()

# Create the agent with memory store
graph = create_react_agent(model, tools, checkpointer=checkpointer, store=doc_store)
```

### 🔍 **Explanation**
- **`ChatOpenAI(model="gpt-4o", temperature=0)`** → Defines an **LLM model**.
- **`ToolNode(tools)`** → Handles **automatic state injection** for tools.
- **`MemorySaver()`** → Saves intermediate conversation states.
- **`create_react_agent(...)`** → Creates the **AI agent** with memory access.

---

## 🔹 **Step 5: Running the AI Agent with Different Users**
Now, let’s test the agent with two different users.  

### 👤 **Test Case 1: User "1" asks about FooBar news**
```python
messages = [{"type": "user", "content": "what's the latest news about FooBar"}]
config = {"configurable": {"thread_id": "1", "user_id": "1"}}

for chunk in graph.stream({"messages": messages}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```
### 💡 **Expected Output**
```
================================ Human Message ================================
what's the latest news about FooBar

================================ Ai Message ================================
Tool Calls:
  get_context (call_ocyHBpGgF3LPFOgRKURBfkGG)
  Args:
    question: latest news about FooBar

================================ Tool Message ================================
Name: get_context
FooBar company just raised 1 Billion dollars!

================================ Ai Message ================================
The latest news about FooBar is that the company has just raised 1 billion dollars.
```
🚀 **User "1" successfully retrieves their relevant document!**  

---

### 👤 **Test Case 2: User "2" asks about FooBar news**
```python
messages = [{"type": "user", "content": "what's the latest news about FooBar"}]
config = {"configurable": {"thread_id": "2", "user_id": "2"}}

for chunk in graph.stream({"messages": messages}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```
### 💡 **Expected Output**
```
================================ Human Message ================================
what's the latest news about FooBar

================================ Ai Message ================================
Tool Calls:
  get_context (call_zxO9KVlL8UxFQUMb8ETeHNvs)
  Args:
    question: latest news about FooBar

================================ Tool Message ================================
Name: get_context
FooBar company was founded in 2019

================================ Ai Message ================================
FooBar company was founded in 2019. If you need more specific or recent news, please let me know!
```
🎯 **User "2" gets a completely different response based on stored memory!**  

---

# 🎯 **Key Takeaways**
✅ **InjectedStore** allows AI tools to access shared memory, ensuring relevant information retrieval.  
✅ AI tools can **store and retrieve** user-specific data dynamically.  
✅ **Real-world applications** include chatbots, recommendation systems, and AI-powered customer support.  
✅ Using **LangGraph's Store API**, we can manage multi-user memory efficiently.  

---

# 🎬 **Final Thoughts**
This guide demonstrated **how to store and retrieve user-specific data in an AI agent using LangGraph's InjectedStore**. This technique is **essential** for building personalized AI systems like virtual assistants, chatbots, or multi-user AI applications.  

🚀 Now you can apply this knowledge to **create AI-powered apps that remember users and provide relevant responses dynamically!** 🎉