# 2. How to handle tool calling errors

Sure! Here’s a well-structured, beginner-friendly, and visually appealing explanation of **how to handle tool-calling errors in LLMs (Large Language Models)** with **real-world examples, code snippets, and detailed explanations**.

---

# 🌟 **Handling Tool-Calling Errors in LLMs**
LLMs (Large Language Models) like **Claude** or **GPT** can call external tools to fetch data or perform actions. However, **errors** can occur when the model:
- Calls a tool incorrectly.
- Passes invalid arguments.
- Encounters restrictions in the tool.

Handling these errors properly ensures **smooth application performance** and improves **user experience**.

---

## 🎯 **Why Do Tool-Calling Errors Happen?**
### ✅ Common Causes:
1. **Incorrect Input Format** – The model might pass a string instead of a list.
2. **Missing Required Fields** – Not providing necessary data.
3. **Tool Constraints** – The tool may expect specific values (e.g., capitalized names).
4. **Multiple Tools at Once** – Too many tools can confuse the model.

### 🚀 **Real-World Example**
Imagine a **weather chatbot** that calls an API to fetch weather updates:
- If the **city name is incorrect**, the API may return an error.
- If the **location is missing**, the tool may fail.

We need to **handle such errors gracefully** to ensure a smooth user experience.

---

# 🛠 **Implementing Error Handling**
Let's build a **LangGraph-based chatbot** that can:
✔ Call tools.  
✔ Detect errors.  
✔ Retry with corrected input.  

---

## 🏗 **Step 1: Install Required Libraries**
```python
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_anthropic
```
🔹 **What this does?**  
- Installs **LangGraph** for workflow management.  
- Installs **LangChain Anthropic** to use the Claude model.

---

## 🔑 **Step 2: Set API Keys**
```python
import os
import getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
```
🔹 **What this does?**  
- **Stores API Keys securely** (so we don’t hardcode them).  
- Uses `getpass.getpass()` to prompt the user for the API key.

---

## ☀ **Step 3: Define a Tool with Error Handling**
We'll create a **weather tool** that expects a **properly capitalized city name**.

```python
from langchain_core.tools import tool

@tool
def get_weather(location: str):
    """Call to get the current weather."""
    if location == "san francisco":
        raise ValueError("Input queries must be proper nouns")
    elif location == "San Francisco":
        return "It's 60 degrees and foggy."
    else:
        raise ValueError("Invalid input.")
```
🔹 **What this does?**  
✔ Defines a **tool named `get_weather()`**.  
✔ Throws an error if the location is not capitalized (`"san francisco"` fails).  
✔ Returns weather data if the input is correct.

---

## 🏗 **Step 4: Build a Chatbot with Tool Handling**
### 🔄 **Using LangGraph for Processing**
```python
from typing import Literal
from langchain_anthropic import ChatAnthropic
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode

tool_node = ToolNode([get_weather])  # 🔹 Attach weather tool

model_with_tools = ChatAnthropic(
    model="claude-3-haiku-20240307", temperature=0
).bind_tools([get_weather])  # 🔹 Bind model with tool
```
🔹 **What this does?**  
- Loads **Claude-3-Haiku** model.  
- Binds it with the `get_weather` tool.  
- Uses **`ToolNode`** to manage tool execution.

---

### 🔍 **Step 5: Define Conditional Flow**
We define **when the model should continue or stop**.

```python
def should_continue(state: MessagesState):
    messages = state["messages"]
    last_message = messages[-1]
    if last_message.tool_calls:
        return "tools"  # Continue if tool call exists
    return END  # Stop otherwise
```
🔹 **What this does?**  
- Checks if the model **called a tool**.  
- If yes → Calls the tool.  
- If no → Ends the process.

---

### 🚀 **Step 6: Build the Workflow**
```python
def call_model(state: MessagesState):
    messages = state["messages"]
    response = model_with_tools.invoke(messages)  # 🔹 Calls the AI model
    return {"messages": [response]}

workflow = StateGraph(MessagesState)

workflow.add_node("agent", call_model)  # 🔹 Agent Node
workflow.add_node("tools", tool_node)  # 🔹 Tool Execution Node

workflow.add_edge(START, "agent")  # 🔹 Start with AI Agent
workflow.add_conditional_edges("agent", should_continue, ["tools", END])
workflow.add_edge("tools", "agent")  # 🔹 Loop Between Agent & Tools

app = workflow.compile()  # 🔹 Compile the Workflow
```
🔹 **What this does?**  
✔ Defines **workflow logic**.  
✔ Calls tools **only when needed**.  
✔ Loops **until the correct answer is found**.

---

## 🛠 **Step 7: Test the Error Handling**
Now, let's test if the chatbot can **recover from tool errors**.

```python
response = app.invoke(
    {"messages": [("human", "what is the weather in san francisco?")]},
)

for message in response["messages"]:
    string_representation = f"{message.type.upper()}: {message.content}\n"
    print(string_representation)
```
### 🔍 **Expected Output**
```
HUMAN: what is the weather in san francisco?

AI: [{'id': 'toolu_01K5tXKVRbETcs7Q8U9PHy96', 'input': {'location': 'san francisco'}, 'name': 'get_weather', 'type': 'tool_use'}]

TOOL: Error: ValueError('Input queries must be proper nouns')
 Please fix your mistakes.

AI: [{'text': 'Apologies, it looks like there was an issue with the weather lookup. Let me try that again with the proper format:', 'type': 'text'}, {'id': 'toolu_01KSCsme3Du2NBazSJQ1af4b', 'input': {'location': 'San Francisco'}, 'name': 'get_weather', 'type': 'tool_use'}]

TOOL: It's 60 degrees and foggy.

AI: The current weather in San Francisco is 60 degrees and foggy.
```
🔹 **What this proves?**  
✔ The chatbot **corrected its mistake**.  
✔ It **retried with the right format** (`San Francisco`).  
✔ It **returned the correct weather data**.

---

## 🎯 **Advanced Error Handling: Custom Fallbacks**
We can **customize error handling** for complex tools.

### 🔹 **Example: A Haiku Generator with Strict Inputs**
```python
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field

class HaikuRequest(BaseModel):
    topic: list[str] = Field(
        max_length=3, min_length=3,
    )

@tool
def master_haiku_generator(request: HaikuRequest):
    """Generates a haiku based on topics."""
    model = ChatAnthropic(model="claude-3-haiku-20240307", temperature=0)
    chain = model | StrOutputParser()
    topics = ", ".join(request.topic)
    haiku = chain.invoke(f"Write a haiku about {topics}")
    return haiku
```
🔹 **What this does?**  
✔ Expects **exactly 3 words** as topics.  
✔ Returns a **haiku** if the format is correct.  
✔ Handles **invalid inputs properly**.

---

# 🚀 **Conclusion**
Handling tool errors is **crucial for building reliable AI applications**.  
We used:
✅ **Tool constraints** – Prevent errors before they happen.  
✅ **Retry logic** – Automatically correct mistakes.  
✅ **LangGraph** – To control execution flow.  

By following these strategies, we ensure **a smooth, human-like AI experience**! 🚀

---

# 🚨 How to Handle Tool Calling Errors in LangGraph

When building AI systems that call external tools, errors are inevitable. For example, your AI may pass bad inputs to a tool or try to call a non-existent tool. In this guide, we'll dive deep into how to handle these errors gracefully using LangGraph and LangChain. We'll cover:

- 🔧 **Setting up the environment**  
- 🛠 **Using a prebuilt ToolNode with error handling**  
- 🎯 **Implementing a ReAct agent that handles tool call errors**  
- 🤖 **Custom strategies and fallback mechanisms**  

We'll explain each code block in detail with real-world examples so even beginners can follow along.

---

## 🔧 Setup: Installing Packages and Setting API Keys

Before diving into error handling, we need to install the necessary packages and set up our API keys securely.

```python
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_anthropic
```

```python
import os
import getpass

# Function to set environment variables securely.
def _set_env(var: str):
    if not os.environ.get(var):
        # Prompt the user to enter the API key if it's not already set.
        os.environ[var] = getpass.getpass(f"{var}: ")

# Set the Anthropic API key for accessing the model.
_set_env("ANTHROPIC_API_KEY")
```

**Explanation:**
- **Installation:** The pip command installs `langgraph` and `langchain_anthropic` quietly.
- **Secure API Key:** We use a helper function `_set_env` to prompt and set our API key if it’s not already in our environment.

---

## 🛠 Using the Prebuilt ToolNode with Error Handling

LLMs (Large Language Models) may sometimes call tools with incorrect inputs. We simulate this by creating a **mock weather tool** that deliberately raises errors when the input is not formatted correctly.

```python
from langchain_core.tools import tool

@tool
def get_weather(location: str):
    """Call to get the current weather."""
    # Simulate a scenario where the input is rejected if not properly capitalized.
    if location == "san francisco":
        raise ValueError("Input queries must be proper nouns")
    elif location == "San Francisco":
        return "It's 60 degrees and foggy."
    else:
        raise ValueError("Invalid input.")
```

**Explanation:**
- **`@tool` Decorator:** This marks `get_weather` as a callable tool.
- **Error Simulation:**  
  - If `location` is `"san francisco"` (all lowercase), it raises an error.
  - If it’s `"San Francisco"`, it returns the weather.
  - Any other input also raises an error.

---

## 🎯 Building a ReAct Agent to Handle Tool Call Errors

Next, we build a simple ReAct agent using LangGraph's **StateGraph**. The agent will:
1. **Invoke the model** to generate a response.
2. **Call the tool** using our prebuilt ToolNode.
3. **Handle errors** by capturing them and allowing the model to try again.

### Step 1: Create the Graph Nodes

```python
from typing import Literal
from langchain_anthropic import ChatAnthropic
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode

# Create a ToolNode instance with the get_weather tool.
tool_node = ToolNode([get_weather])

# Bind the tool to our Anthropic chat model.
model_with_tools = ChatAnthropic(
    model="claude-3-haiku-20240307", temperature=0
).bind_tools([get_weather])
```

**Explanation:**
- **ToolNode Instance:** We initialize `tool_node` with our `get_weather` tool.
- **Model Binding:** We bind our chat model to the `get_weather` tool so that it can generate tool call instructions.

### Step 2: Define Helper Functions

```python
# Check if the last AI message contains a tool call.
def should_continue(state: MessagesState):
    messages = state["messages"]
    last_message = messages[-1]
    if last_message.tool_calls:
        return "tools"  # Continue to the tools node if there is a tool call.
    return END        # Otherwise, end the workflow.

# Invoke the model to generate a response based on current messages.
def call_model(state: MessagesState):
    messages = state["messages"]
    response = model_with_tools.invoke(messages)
    return {"messages": [response]}
```

**Explanation:**
- **`should_continue`:**  
  - Inspects the latest message.
  - If a tool call is present, the workflow should move to the "tools" node.
- **`call_model`:**  
  - Passes the current state (list of messages) to the model.
  - Wraps the model's response in a dictionary under the `"messages"` key.

### Step 3: Create the Workflow Graph

```python
# Build the workflow graph using StateGraph.
workflow = StateGraph(MessagesState)

# Add the model (agent) and tools nodes.
workflow.add_node("agent", call_model)
workflow.add_node("tools", tool_node)

# Define the flow: Start -> Agent -> (if tool call exists) Tools -> Agent -> End.
workflow.add_edge(START, "agent")
workflow.add_conditional_edges("agent", should_continue, ["tools", END])
workflow.add_edge("tools", "agent")

# Compile the workflow into an executable app.
app = workflow.compile()
```

**Explanation:**
- **Graph Nodes:**  
  - `"agent"` node: where the AI model is called.
  - `"tools"` node: where the ToolNode executes the tool calls.
- **Edges:**  
  - Direct the flow from start to agent.
  - Use conditional edges based on whether the last message includes a tool call.
  - Cycle back from tools to agent until the process ends.
- **Compilation:** The workflow is compiled into an application that can be invoked.

### Step 4: Test the Error Handling

We now simulate a scenario where the model calls the tool with a bad input ("san francisco" in lowercase), triggering an error.

```python
# Invoke the app with a problematic query.
response = app.invoke(
    {"messages": [("human", "what is the weather in san francisco?")]}
)

# Iterate over the response messages and print them.
for message in response["messages"]:
    string_representation = f"{message.type.upper()}: {message.content}\n"
    print(string_representation)
```

**Expected Output:**

```plaintext
HUMAN: what is the weather in san francisco?

AI: [{'id': 'toolu_01K5tXKVRbETcs7Q8U9PHy96', 'input': {'location': 'san francisco'}, 'name': 'get_weather', 'type': 'tool_use'}]

TOOL: Error: ValueError('Input queries must be proper nouns')
 Please fix your mistakes.

AI: [{'text': 'Apologies, it looks like there was an issue with the weather lookup. Let me try that again with the proper format:', 'type': 'text'}, {'id': 'toolu_01KSCsme3Du2NBazSJQ1af4b', 'input': {'location': 'San Francisco'}, 'name': 'get_weather', 'type': 'tool_use'}]

TOOL: It's 60 degrees and foggy.

AI: The current weather in San Francisco is 60 degrees and foggy.
```

**Explanation:**
- The human query uses "san francisco" in lowercase.
- The model’s initial tool call fails because of our input restriction.
- The built-in error handling in ToolNode captures the error and returns it to the model.
- The model then adjusts the input (using "San Francisco") and successfully retrieves the weather.

---

## 🤖 Custom Strategies and Fallback Mechanisms

Sometimes, default error handling isn’t enough. You may need **custom fallbacks** or **advanced error-handling logic**. For example, consider a tool that generates a haiku from provided topics. The tool requires exactly three topics—a requirement that can easily trip up a model.

### Custom Tool with Input Validation

```python
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field

# Define a Pydantic model to enforce exactly 3 topics.
class HaikuRequest(BaseModel):
    topic: list[str] = Field(
        max_length=3,
        min_length=3,
    )

@tool
def master_haiku_generator(request: HaikuRequest):
    """Generates a haiku based on the provided topics."""
    # Initialize a chat model.
    model = ChatAnthropic(model="claude-3-haiku-20240307", temperature=0)
    # Use an output parser to convert the response to a string.
    chain = model | StrOutputParser()
    topics = ", ".join(request.topic)
    haiku = chain.invoke(f"Write a haiku about {topics}")
    return haiku
```

**Explanation:**
- **HaikuRequest:**  
  - Enforces that the `topic` list must have exactly 3 items.
- **`master_haiku_generator`:**  
  - Uses a chat model chained with an output parser to generate a haiku.
  - Joins the topics into a single string for the prompt.

### Building a More Robust Workflow with Fallbacks

In cases where tool calls repeatedly fail, you might want to:
1. **Remove failed attempts** to clean up the conversation.
2. **Fallback to a more capable model** for better handling.

#### Custom Node to Call Tools with Error Capture

```python
import json
from langchain_core.messages import AIMessage, ToolMessage
from langgraph.graph import RemoveMessage

# Custom node to execute tool calls with manual error handling.
def call_tool(state: MessagesState):
    # Map tool names to actual functions.
    tools_by_name = {master_haiku_generator.name: master_haiku_generator}
    messages = state["messages"]
    last_message = messages[-1]
    output_messages = []
    
    # Iterate through each tool call in the last message.
    for tool_call in last_message.tool_calls:
        try:
            # Try to invoke the tool with the provided arguments.
            tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
            output_messages.append(
                ToolMessage(
                    content=json.dumps(tool_result),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        except Exception as e:
            # On failure, capture the error in the message.
            output_messages.append(
                ToolMessage(
                    content="",
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                    additional_kwargs={"error": e},
                )
            )
    return {"messages": output_messages}
```

**Explanation:**
- **Mapping Tools:**  
  - Creates a dictionary mapping tool names to their function implementations.
- **Error Handling:**  
  - Wraps each tool call in a `try` block.
  - If an error occurs, it attaches the error details in `additional_kwargs` of the `ToolMessage`.

#### Fallback Mechanism: Removing Failed Attempts and Retrying with a Better Model

```python
# Fallback: Remove messages corresponding to failed tool calls.
def remove_failed_tool_call_attempt(state: MessagesState):
    messages = state["messages"]
    # Find the last AI message to remove subsequent messages.
    last_ai_message_index = next(
        i for i, msg in reversed(list(enumerate(messages))) if isinstance(msg, AIMessage)
    )
    messages_to_remove = messages[last_ai_message_index:]
    # Create RemoveMessage objects for each message to be cleared.
    return {"messages": [RemoveMessage(id=m.id) for m in messages_to_remove]}

# Fallback: Use a more powerful model if errors persist.
better_model = ChatAnthropic(model="claude-3-5-sonnet-20240620", temperature=0)
better_model_with_tools = better_model.bind_tools([master_haiku_generator])

def call_fallback_model(state: MessagesState):
    messages = state["messages"]
    response = better_model_with_tools.invoke(messages)
    return {"messages": [response]}
```

**Explanation:**
- **`remove_failed_tool_call_attempt`:**  
  - Identifies and removes the latest batch of messages starting from the last AI response. This "cleans" the conversation state.
- **Fallback Model:**  
  - A more capable model (`claude-3-5-sonnet-20240620`) is used to handle the task if the primary model fails repeatedly.
- **`call_fallback_model`:**  
  - Invokes the fallback model to generate a corrected tool call response.

#### Integrating Custom Nodes into the Workflow

```python
# Create a new workflow graph incorporating fallback logic.
workflow = StateGraph(MessagesState)

# Add nodes: agent (model), tools (custom call_tool), remove_failed, and fallback.
workflow.add_node("agent", call_model)
workflow.add_node("tools", call_tool)
workflow.add_node("remove_failed_tool_call_attempt", remove_failed_tool_call_attempt)
workflow.add_node("fallback_agent", call_fallback_model)

# Define the workflow's edges.
workflow.add_edge(START, "agent")
workflow.add_conditional_edges("agent", should_continue, ["tools", END])
workflow.add_conditional_edges("tools", 
    lambda state: "remove_failed_tool_call_attempt" if any(
        isinstance(msg, ToolMessage) and msg.additional_kwargs.get("error") is not None 
        for msg in state["messages"]) else "agent"
)
workflow.add_edge("remove_failed_tool_call_attempt", "fallback_agent")
workflow.add_edge("fallback_agent", "tools")

# Compile the final workflow.
app = workflow.compile()
```

**Explanation:**
- **Nodes Added:**  
  - `"agent"`: Calls the primary model.
  - `"tools"`: Uses our custom tool call handler with error capture.
  - `"remove_failed_tool_call_attempt"`: Clears failed tool call attempts.
  - `"fallback_agent"`: Invokes a fallback model for a second try.
- **Conditional Edges:**  
  - The workflow conditionally moves to remove failed messages if any tool call error is detected.
  - Then, it falls back to the stronger model and retries the tool call.
- **Workflow Compilation:**  
  - The complete workflow is compiled into an executable app.

---

## 🌍 Real-World Use Case

Imagine you're building an **AI-powered creative assistant** that writes haikus on demand. Due to the nuanced nature of poetry, the model might sometimes call the haiku generator with incomplete or improperly formatted input. By using the above error handling and fallback strategies, the assistant:
- Catches and reports input errors.
- Automatically corrects or refines its input.
- Ultimately delivers a well-formulated haiku.

This ensures a smoother user experience and more robust error recovery in live applications.

---

## 🚀 Summary

- **LLMs and Tool Calling:**  
  - Language models may produce errors when calling external tools due to input mismatches.
- **Prebuilt Error Handling:**  
  - LangGraph's ToolNode has built-in error handling to capture and return errors.
- **Custom Strategies:**  
  - Create custom nodes to capture errors, remove failed attempts, and fall back to a more capable model.
- **Real-World Impact:**  
  - Enhances robustness in applications like creative assistants, smart customer service bots, and more.

By integrating these techniques into your AI workflows, you ensure that even if errors occur, your system can recover gracefully and deliver reliable results.

Happy coding! If you have any questions or need further clarification, feel free to ask.