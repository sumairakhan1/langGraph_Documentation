# 6. How to handle large numbers of tools

Got it! I'll break down the concept of handling large numbers of tools in an easy-to-understand way, using clear explanations, real-world applications, and step-by-step code walkthroughs.  

---

# üõ†Ô∏è Handling Large Numbers of Tools in AI Models

When working with AI models, we often need to use **tools** (external functions, APIs, or services) to enhance the model‚Äôs capabilities. However, when the number of available tools grows, it becomes **inefficient** to let the AI model consider all tools at once.  

## üìå Why Limit the Tools an AI Model Uses?  
If we allow the model to access **too many tools**, it can cause:  
‚úÖ **Increased token usage** ‚Äì More tools mean more computation, leading to higher costs.  
‚úÖ **Slower response time** ‚Äì The model takes longer to decide which tool to use.  
‚úÖ **Higher error rate** ‚Äì The model might pick an incorrect tool due to too many choices.  

To solve this, we **dynamically select** a **subset of relevant tools** based on the user‚Äôs input.  

---

# üèóÔ∏è How Do We Efficiently Select Tools?  
To optimize tool selection, we follow these steps:  
1Ô∏è‚É£ **Store all tools** in a **registry** with descriptions.  
2Ô∏è‚É£ **Convert descriptions into embeddings** (vector representation).  
3Ô∏è‚É£ **Use similarity search** to find tools based on user queries.  
4Ô∏è‚É£ **Limit the AI‚Äôs access** to only the most relevant tools.  

This approach works similarly to **Google Search** ‚Äì instead of searching everything on the internet, we retrieve **only the most relevant pages** based on the query.  

---

# üõ†Ô∏è Step-by-Step Implementation  

### üìå 1. Install Required Libraries  
We use `langgraph`, `langchain_openai`, and `numpy` to build our tool selection system.  

```python
!pip install --quiet -U langgraph langchain_openai numpy
```

---

### üìå 2. Set Up API Keys  
We need an **API key** to use OpenAI‚Äôs tools. This code sets environment variables securely.  

```python
import getpass
import os

def _set_env(var: str):
    """Prompt user for API key if not already set"""
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

üîπ **What This Does:**  
‚úîÔ∏è If the API key isn‚Äôt already set, it **asks the user to enter it** securely.  
‚úîÔ∏è Stores it in an **environment variable** so we don‚Äôt need to enter it repeatedly.  

---

### üìå 3. Create a Tool Registry  
Let‚Äôs assume we have tools for different companies in the **S&P 500**. Each tool fetches a company‚Äôs revenue based on the year provided.  

```python
import re
import uuid
from langchain_core.tools import StructuredTool

def create_tool(company: str) -> dict:
    """Create a placeholder tool for a company"""
    # Format company name (remove spaces and special characters)
    formatted_company = re.sub(r"[^\w\s]", "", company).replace(" ", "_")

    def company_tool(year: int) -> str:
        """Placeholder function returning static revenue information"""
        return f"{company} had revenues of $100 in {year}."

    return StructuredTool.from_function(
        company_tool,
        name=formatted_company,
        description=f"Information about {company}",
    )

# List of companies (example)
s_and_p_500_companies = [
    "3M", "Abbott", "Accenture", "Advanced Micro Devices", "Yum! Brands"
]

# Create a tool for each company and store in a registry
tool_registry = {
    str(uuid.uuid4()): create_tool(company) for company in s_and_p_500_companies
}
```

üîπ **Explanation:**  
‚úîÔ∏è We define a function `create_tool(company)` to generate a tool for each company.  
‚úîÔ∏è It returns **dummy revenue data** for the given company and year.  
‚úîÔ∏è The tools are stored in a **dictionary** where each tool has a **unique ID**.  

---

### üìå 4. Store Tools in a Searchable Vector Database  
We convert the tool descriptions into **embeddings** (numerical representations) and store them in a **vector database**.  

```python
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings

# Convert tool descriptions into documents
tool_documents = [
    Document(
        page_content=tool.description,
        id=id,
        metadata={"tool_name": tool.name},
    )
    for id, tool in tool_registry.items()
]

# Store tool descriptions as embeddings
vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())
document_ids = vector_store.add_documents(tool_documents)
```

üîπ **Explanation:**  
‚úîÔ∏è Each tool‚Äôs description is **converted into a document**.  
‚úîÔ∏è We use **embeddings** to represent descriptions in a **searchable format**.  
‚úîÔ∏è These embeddings are stored in an **in-memory vector store** for **fast lookup**.  

---

### üìå 5. Implement Tool Selection Using Semantic Search  
When a user asks a question, we **find the most relevant tools** by searching their descriptions.  

```python
def select_tools(state):
    """Retrieve relevant tools based on user query"""
    last_user_message = state["messages"][-1]  # Get last user input
    query = last_user_message.content  # Extract query text

    # Search for similar tool descriptions
    tool_documents = vector_store.similarity_search(query)

    # Return a list of matching tool IDs
    return {"selected_tools": [document.id for document in tool_documents]}
```

üîπ **Explanation:**  
‚úîÔ∏è Extracts **the latest user message**.  
‚úîÔ∏è Uses **semantic search** to find the **most relevant tools**.  
‚úîÔ∏è Returns the **IDs of the best-matching tools**.  

---

### üìå 6. Set Up the AI Agent  
Now we build an **AI agent** that only accesses the selected tools.  

```python
from typing import Annotated
from langchain_openai import ChatOpenAI
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

# Define state structure
class State(TypedDict):
    messages: Annotated[list, add_messages]
    selected_tools: list[str]

# Initialize AI model and tool registry
builder = StateGraph(State)
tools = list(tool_registry.values())
llm = ChatOpenAI()

def agent(state: State):
    """AI agent function"""
    # Get selected tools from the registry
    selected_tools = [tool_registry[id] for id in state["selected_tools"]]

    # Bind tools to AI model
    llm_with_tools = llm.bind_tools(selected_tools)

    # Invoke model and return new messages
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# Build the execution graph
builder.add_node("agent", agent)
builder.add_node("select_tools", select_tools)
tool_node = ToolNode(tools=tools)

builder.add_node("tools", tool_node)
builder.add_conditional_edges("agent", tools_condition, path_map=["tools", "__end__"])
builder.add_edge("tools", "agent")
builder.add_edge("select_tools", "agent")
builder.add_edge(START, "select_tools")

# Compile execution graph
graph = builder.compile()
```

üîπ **Explanation:**  
‚úîÔ∏è The AI agent **binds the selected tools** and answers using **only those tools**.  
‚úîÔ∏è It uses a **graph structure** to control execution flow.  
‚úîÔ∏è The AI first **selects tools**, then **invokes the agent**, which **calls the tools** if needed.  

---

# üéØ Real-World Applications  
This method is used in:  
‚úÖ **Customer Support Chatbots** ‚Äì Selecting relevant **help desk** tools based on the query.  
‚úÖ **AI Assistants** ‚Äì Choosing appropriate **APIs** for finance, weather, or stocks.  
‚úÖ **Healthcare** ‚Äì Selecting medical tools based on **symptoms** entered by users.  

---

# ‚úÖ Conclusion  
By dynamically **retrieving and binding** tools based on user queries, we:  
‚úîÔ∏è **Reduce token usage** üí∞  
‚úîÔ∏è **Improve response speed** ‚ö°  
‚úîÔ∏è **Enhance accuracy** üéØ  

This method makes AI **smarter and more efficient** in handling a **large number of tools**! üöÄ

---

# üõ†Ô∏è Handling Large Numbers of Tools in AI Models

When dealing with AI models, particularly those with many tools, the challenge of selecting the appropriate tool becomes increasingly complex. This task can involve dynamically adjusting which tools the model can use depending on the context, which helps reduce errors and token consumption. Let's break down this process and understand how it can be applied in a real-world scenario.

---

### üéØ **Real-World Use Case: Virtual Assistant for Business Insights**

Imagine you‚Äôre building a **virtual assistant** for a business. The assistant needs to provide company-specific insights (like revenue) based on various parameters like year, region, etc. You can integrate **tools** that provide data about companies (like **Advanced Micro Devices (AMD)**, **Accenture**, etc.). However, as the number of companies grows, the assistant will need to decide which tools to use, depending on the user's query, to avoid unnecessary computation and improve efficiency.

---

### üß© **What We Are Doing: Dynamically Selecting Tools**

We will design a system where the AI model dynamically selects the most relevant tools to use for a given query, such as fetching company revenue data for a specific year. This is achieved by:

- **Tool registry**: A collection of tools (each representing a company or resource).
- **Vector store**: A storage system that helps us find the tools relevant to a user's query using **semantic search**.
- **State management**: We track the AI's progress in the conversation and manage the selected tools accordingly.

---

### üöÄ **How It Works: Code Explanation**

Let‚Äôs walk through the provided code and explain each part. The goal is to dynamically choose the correct tool based on the user's query.

---

#### 1. **Setting up the environment**

Before we start coding, we need to set up our environment and install required packages.

```python
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_openai numpy
```

- This installs the necessary libraries such as **LangGraph**, **LangChain** (for large language models), and **Numpy** (for numerical calculations).
  
---

#### 2. **Creating the Tools**

We define a function to create tools for each company. Each tool fetches company-specific data based on the **year** provided by the user.

```python
import re
import uuid
from langchain_core.tools import StructuredTool

def create_tool(company: str) -> dict:
    """Create schema for a placeholder tool."""
    formatted_company = re.sub(r"[^\w\s]", "", company).replace(" ", "_")

    def company_tool(year: int) -> str:
        return f"{company} had revenues of $100 in {year}."

    return StructuredTool.from_function(
        company_tool,
        name=formatted_company,
        description=f"Information about {company}",
    )
```

- **`create_tool()`**: This function generates a tool for each company using a schema. It removes non-alphanumeric characters from the company name and replaces spaces with underscores.
- **`company_tool(year: int)`**: This is a placeholder function that simulates fetching revenue data for a given year.
- **`StructuredTool.from_function()`**: This converts the function into a structured tool that can be used by the model.

Example: For the company **"AMD"**, the tool will return:
```plaintext
"AMD had revenues of $100 in 2022."
```

---

#### 3. **Creating a Tool Registry**

We then generate a registry (a dictionary) that associates each company with its respective tool.

```python
s_and_p_500_companies = [
    "3M", "A.O. Smith", "Abbott", "Accenture", "Advanced Micro Devices", "Yum! Brands",
    "Zebra Technologies", "Zimmer Biomet", "Zoetis",
]

tool_registry = {
    str(uuid.uuid4()): create_tool(company) for company in s_and_p_500_companies
}
```

- We use **UUIDs** to uniquely identify each tool and store them in the `tool_registry`.
- The registry ensures that each company has its own unique tool for fetching data.

---

#### 4. **Tool Selection via Semantic Search**

Now, we need to decide which tools to select based on the user‚Äôs query. This is done by embedding each tool‚Äôs description and performing a semantic search.

```python
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings

tool_documents = [
    Document(page_content=tool.description, id=id, metadata={"tool_name": tool.name})
    for id, tool in tool_registry.items()
]

vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())
document_ids = vector_store.add_documents(tool_documents)
```

- **`tool_documents`**: We create a list of documents, where each document contains the description of a tool.
- **`InMemoryVectorStore`**: This stores the tool descriptions and their embeddings.
- **`OpenAIEmbeddings`**: Embeddings are used to convert textual data into numerical vectors for semantic search.

When a user asks, "Can you give me some information about AMD in 2022?", the query is compared to the tool descriptions using semantic search to determine the best matching tools.

---

#### 5. **State Management and Tool Selection Logic**

The system manages the state of the conversation, tracks the messages, and selects the relevant tools for the query.

```python
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

# Define the state structure using TypedDict.
class State(TypedDict):
    messages: Annotated[list, add_messages]
    selected_tools: list[str]
```

- **`State`**: This is where we store conversation history (`messages`) and the selected tools (`selected_tools`).
- **`StateGraph`**: The graph that represents the state transitions in the conversation.

---

#### 6. **Tool Selection Based on User Query**

Now, the `select_tools()` function is responsible for selecting the tools based on the user's query.

```python
def select_tools(state: State):
    last_message = state["messages"][-1]
    query = last_message.content  # Get the user's query

    # Use semantic search to find the best matching tools
    tool_documents = vector_store.similarity_search(query)
    
    selected_tools = [document.id for document in tool_documents]
    return {"selected_tools": selected_tools}
```

- **`select_tools(state)`**: This function checks the user's last message and performs semantic search to find tools that match the query.
- **`similarity_search(query)`**: This searches the tool documents to find the ones most relevant to the user's query.

Example: For the query **"Can you give me some information about AMD in 2022?"**, the system will choose tools related to **AMD**.

---

#### 7. **Executing the Graph and Selecting Tools**

Finally, we invoke the graph and see the tool selection in action:

```python
result = graph.invoke({"messages": [("user", user_input)]})

for message in result["messages"]:
    message.pretty_print()
```

- **`graph.invoke()`**: This executes the graph and processes the user‚Äôs message.
- **`message.pretty_print()`**: This prints the AI‚Äôs response in a human-readable format.

---

### üõ†Ô∏è **Improvements and Optimizations**

- **Repeating Tool Selection**: If the initial selection is incorrect, we can modify the tool selection logic to retry, improving accuracy.
- **Optimizing Tool Retrieval**: Grouping tools or using a more sophisticated query mechanism can help optimize the search process.

---

### üí° **Conclusion**

Handling large numbers of tools involves:

- **Tool Registry**: Store tools dynamically for each resource (e.g., companies).
- **Semantic Search**: Use embeddings and vector stores to match user queries to relevant tools.
- **State Management**: Track and manage the state of the conversation to select the appropriate tool.

In a real-world scenario, this approach can be used to build systems that dynamically select the right tool based on user input, making the system efficient and responsive.

