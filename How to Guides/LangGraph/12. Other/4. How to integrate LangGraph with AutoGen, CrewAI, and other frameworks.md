# 4. How to integrate LangGraph with AutoGen, CrewAI, and other frameworks

# Integrating LangGraph with AutoGen, CrewAI, and Other Frameworks ü§ñ

LangGraph is a powerful framework designed for creating agent-based and multi-agent systems. When combined with frameworks like **AutoGen**, **CrewAI**, and others, it becomes an incredibly versatile tool for building intelligent applications. Here, we will explain how to integrate LangGraph with AutoGen in detail. We will also provide real-world examples, code explanations, and more to ensure beginners can easily follow along.

---

### üß© Why Integrate LangGraph with Other Frameworks?

LangGraph allows you to create systems that use multiple agents, each possibly built with different frameworks. This provides flexibility and scalability. You might want to integrate LangGraph with frameworks like AutoGen to achieve:

1. **Multi-agent Systems**: Use agents from different frameworks to interact with each other.
2. **Feature Enrichment**: Add advanced features like **memory**, **streaming**, **persistence**, and more to enhance your agents' capabilities.
3. **Flexibility**: Easily integrate different agents into one system without worrying about compatibility issues.

**Real-World Example**: Imagine you are building a smart home assistant system where one agent controls the lighting (using CrewAI), another manages the HVAC system (using AutoGen), and LangGraph acts as the orchestrator, managing communication and decision-making across these agents.

---

### üöÄ How to Integrate LangGraph with AutoGen

To start integrating LangGraph with AutoGen, follow these steps:

#### 1Ô∏è‚É£ Install Required Libraries

First, ensure you have the required libraries installed. Use the following command to install **LangGraph** and **AutoGen**:

```bash
%pip install autogen langgraph
```

This will allow you to access all the functions needed for this integration.

#### 2Ô∏è‚É£ Define the AutoGen Agent

Next, you'll define an **AutoGen agent**. This agent will respond to user inputs and interact with LangGraph. We‚Äôll also define a **UserProxyAgent** for user interactions.

```python
import autogen
import os

# Configuring the AutoGen agent
config_list = [{"model": "gpt-4o", "api_key": os.environ["OPENAI_API_KEY"]}]
llm_config = {
    "timeout": 600,
    "cache_seed": 42,
    "config_list": config_list,
    "temperature": 0,
}

# Define the AutoGen assistant and user proxy agents
autogen_agent = autogen.AssistantAgent(
    name="assistant",
    llm_config=llm_config,
)

user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={
        "work_dir": "web",
        "use_docker": False,
    },  # Set use_docker=True if available
    llm_config=llm_config,
    system_message="Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE.",
)
```

- **Purpose of the Code**:
  - We configure the **AutoGen Assistant Agent** to use the GPT-4 model for generating responses.
  - **UserProxyAgent** is set up to handle user inputs and determine whether a task is complete by checking the content for a "TERMINATE" message.
  - We also configure execution parameters like **timeout** and **temperature**.

#### 3Ô∏è‚É£ Create a LangGraph Chatbot

Now, let's define a **LangGraph** state machine that will call the **AutoGen Agent**.

```python
from langchain_core.messages import convert_to_openai_messages
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.memory import MemorySaver

# Define the function to interact with the AutoGen agent
def call_autogen_agent(state: MessagesState):
    # Convert the messages to OpenAI-style format
    messages = convert_to_openai_messages(state["messages"])
    
    # Initiate chat with AutoGen agent
    response = user_proxy.initiate_chat(
        autogen_agent,
        message=messages[-1],  # Pass the latest message
        carryover=messages[:-1],  # Provide previous messages as context
    )
    
    # Get the response content from the agent
    content = response.chat_history[-1]["content"]
    return {"messages": {"role": "assistant", "content": content}}

# Add memory-saving functionality for storing conversation history
checkpointer = MemorySaver()

# Create the LangGraph state graph
builder = StateGraph(MessagesState)
builder.add_node(call_autogen_agent)
builder.add_edge(START, "call_autogen_agent")
graph = builder.compile(checkpointer=checkpointer)
```

- **Explanation**:
  - The function **`call_autogen_agent`** sends a message to the **AutoGen agent** and retrieves the response.
  - **`convert_to_openai_messages`** is used to format the messages appropriately.
  - **`MemorySaver`** ensures the conversation is stored for future use, enabling **short-term memory**.

#### 4Ô∏è‚É£ Visualize the Graph

To visualize how the graph looks and understand the flow of nodes:

```python
from IPython.display import display, Image

# Display the graph as a visual diagram
display(Image(graph.get_graph().draw_mermaid_png()))
```

- **Purpose**: This line allows us to visually check the flow of our **LangGraph**.

#### 5Ô∏è‚É£ Run the Graph

Now, we will run the graph with a user message to see how it works in practice. We'll send a user request and print the assistant's response.

```python
# Define the configuration for running the graph
config = {"configurable": {"thread_id": "1"}}  # Use a thread ID to persist conversation history

# Pass a message and get the response
for chunk in graph.stream(
    {
        "messages": [
            {
                "role": "user",
                "content": "Find numbers between 10 and 30 in Fibonacci sequence",
            }
        ]
    },
    config,
):
    print(chunk)
```

**Real-World Example**: In a customer support bot, you can integrate LangGraph with AutoGen to handle queries about product availability, providing responses like the one in the Fibonacci sequence example, and storing previous chat history for better interaction continuity.

#### 6Ô∏è‚É£ Handling Code Execution

You can also execute code dynamically through the agent. In the Fibonacci example, if the user asks for code execution (like multiplying numbers), the agent will respond with Python code and execute it:

```python
# Fibonacci Sequence Python Code
def fibonacci_sequence():
    a, b = 0, 1
    while a <= 30:
        if 10 <= a <= 30:
            print(a)
        a, b = b, a + b

fibonacci_sequence()  # This will print numbers between 10 and 30
```

---

### üõ†Ô∏è Final Thoughts

Integrating **LangGraph** with frameworks like **AutoGen** can enable complex, intelligent systems that manage conversations and tasks seamlessly. The approach we've shown can easily be adapted to work with other frameworks (like **CrewAI**) by modifying the agent interaction logic.

#### Key Benefits:
- **Flexibility**: Use agents from various frameworks in one system.
- **Memory**: Store and recall conversation history for continuity.
- **Persistence**: Ensure the agent can continue its task over multiple interactions.

---

By following the above steps, you can build complex, multi-agent applications tailored to your specific needs, such as chatbots, virtual assistants, and more.

---

# How to Integrate LangGraph with AutoGen and Other Frameworks üöÄ

In this guide, we'll learn how to integrate **LangGraph** with agent frameworks like **AutoGen** (and others such as CrewAI) to build powerful multi-agent applications. We'll cover everything from setup to code examples, detailed explanations, and even a real-world use case!

---

## 1. What is LangGraph? ü§ñ

**LangGraph** is a framework for building agentic and multi-agent systems. It lets you combine different agents‚Äîeach potentially built with a different framework‚Äîinto one unified system. This integration brings benefits such as:
- **Persistence:** Storing conversation history.
- **Streaming:** Receiving real-time responses.
- **Memory Management:** Maintaining context with short- and long-term memory.

---

## 2. Why Integrate LangGraph with Other Frameworks? üîó

Integrating LangGraph with frameworks like **AutoGen** offers these advantages:
- **Multi-Agent Coordination:** Create systems where each agent is specialized (e.g., one for language generation and another for task management).
- **Enhanced Features:** Leverage LangGraph‚Äôs persistence and memory features to maintain conversation context.
- **Modular Development:** Build and test agents independently before combining them into one system.

### Real-World Example: Customer Support Chatbot üí¨

Imagine a customer support system where:
- **Agent 1 (AutoGen):** Provides detailed answers to customer queries.
- **Agent 2 (CrewAI):** Manages order tracking and technical troubleshooting.
- **LangGraph:** Orchestrates the conversation, ensuring context is maintained and the right agent responds at the right time.

---

## 3. Setting Up the Environment üõ†Ô∏è

Before integrating, install the required packages:

```bash
%pip install autogen langgraph
```

### Environment Setup Code

```python
import getpass
import os

# Helper function to set environment variables securely
def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

# Securely set your OpenAI API key
_set_env("OPENAI_API_KEY")
```

**Explanation:**
- **getpass & os:** Used to securely handle environment variables.
- **_set_env:** Checks if a variable is set; if not, it prompts you to enter it (e.g., your API key).

---

## 4. Defining the AutoGen Agent üéØ

Next, we define an AutoGen agent, which will handle the generation of responses.

### Code Example

```python
import autogen
import os

# Configuration for using GPT-4 with AutoGen
config_list = [{"model": "gpt-4o", "api_key": os.environ["OPENAI_API_KEY"]}]

# LLM configuration parameters
llm_config = {
    "timeout": 600,            # Timeout for API calls (in seconds)
    "cache_seed": 42,          # Seed for caching to ensure consistent responses
    "config_list": config_list, # Use our GPT-4 configuration
    "temperature": 0,          # 0 makes the output deterministic
}

# Create the assistant agent responsible for generating responses
autogen_agent = autogen.AssistantAgent(
    name="assistant",
    llm_config=llm_config,
)

# Create a user proxy agent that simulates user input and manages interactions
user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",  # Disable direct human input
    max_consecutive_auto_reply=10,
    # Function to determine when the task is complete
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    # Config for code execution (e.g., running generated code)
    code_execution_config={
        "work_dir": "web",
        "use_docker": False,   # Use Docker for safe code execution if available
    },
    llm_config=llm_config,
    system_message="Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or explain why the task is not solved yet.",
)
```

**Explanation:**
- **config_list & llm_config:** Define how the language model (GPT-4) should work, including parameters like timeout and temperature.
- **autogen_agent:** This agent uses AutoGen to generate answers.
- **user_proxy:** Acts as a proxy for user interactions. It simulates a user, carries over conversation history, and determines when a conversation should terminate (using a termination message).

---

## 5. Creating the LangGraph Chatbot Graph üîÑ

Now, we build the LangGraph graph that ties everything together. This graph will manage the conversation flow, call our AutoGen agent, and maintain memory.

### Code Example

```python
from typing import Literal, TypedDict
from langchain_core.messages import convert_to_openai_messages
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.memory import MemorySaver

# Function to call the AutoGen agent
def call_autogen_agent(state: MessagesState):
    # Convert conversation history to OpenAI's message format
    messages = convert_to_openai_messages(state["messages"])
    
    # Call the user_proxy, sending the latest message and previous history as context
    response = user_proxy.initiate_chat(
        autogen_agent,
        message=messages[-1],   # Latest user message
        carryover=messages[:-1] # Previous messages for context
    )
    
    # Extract the final response from the agent's chat history
    content = response.chat_history[-1]["content"]
    return {"messages": {"role": "assistant", "content": content}}

# MemorySaver saves the conversation history for persistence
checkpointer = MemorySaver()

# Build the state graph with the conversation state type (MessagesState)
builder = StateGraph(MessagesState)
builder.add_node(call_autogen_agent)       # Add our agent-calling function as a node
builder.add_edge(START, "call_autogen_agent")  # Define the starting point of the conversation
graph = builder.compile(checkpointer=checkpointer)  # Compile the graph with persistence enabled
```

**Explanation:**
- **convert_to_openai_messages:** Converts messages to a format that the OpenAI API understands.
- **call_autogen_agent:** 
  - **Input:** Takes the current conversation state.
  - **Processing:** Converts messages, calls the AutoGen agent using the latest message, and carries over previous messages for context.
  - **Output:** Returns the agent‚Äôs response.
- **MemorySaver:** Provides persistence by saving conversation history.
- **StateGraph:** Manages the nodes and edges (i.e., the conversation flow). Here, we add our node (the function) and define the starting edge.
- **graph.compile:** Finalizes the graph for execution, including memory features.

### Visualizing the Graph

To view a diagram of your graph structure, you can use:

```python
from IPython.display import display, Image
display(Image(graph.get_graph().draw_mermaid_png()))
```

This code generates a visual representation of your LangGraph, making it easier to understand how your agents are connected.

---

## 6. Running the Graph üöÄ

Let's run the graph with an example conversation. In this case, we'll ask the agent to find Fibonacci numbers between 10 and 30.

### Code Example

```python
# Configuration with a thread ID to maintain conversation history
config = {"configurable": {"thread_id": "1"}}

# Stream responses from the graph by sending an initial message
for chunk in graph.stream(
    {
        "messages": [
            {
                "role": "user",
                "content": "Find numbers between 10 and 30 in fibonacci sequence",
            }
        ]
    },
    config,
):
    print(chunk)
```

**Explanation:**
- **config:** Sets a unique thread ID. This ensures that the conversation history is preserved for later interactions.
- **graph.stream:** Begins the conversation. It takes the initial user message, streams responses from the agent, and prints them chunk by chunk.

---

## 7. Continuing the Conversation üîÑ

Thanks to LangGraph's persistence, you can continue the conversation seamlessly. For instance, after receiving the Fibonacci numbers, you can ask the agent to perform further calculations.

### Code Example

```python
for chunk in graph.stream(
    {
        "messages": [
            {
                "role": "user",
                "content": "Multiply the last number by 3",
            }
        ]
    },
    config,
):
    print(chunk)
```

**Explanation:**
- **Continuation:** Uses the same `thread_id` so that the agent remembers the previous conversation.
- **New Query:** The new message instructs the agent to use the context (Fibonacci numbers) to compute further results.

---

## 8. Real-World Use Case: Workflow Automation in Business üè¢

Imagine a business environment where multiple agents are responsible for different tasks:
- **Customer Support:** One agent handles general queries using AutoGen.
- **Task Management:** Another agent (built with CrewAI) manages appointments and deadlines.
- **Orchestration:** LangGraph ensures that context (like previous customer interactions) is maintained across agents.

**Benefits:**
- **Seamless Communication:** Agents can exchange information and context without losing track.
- **Increased Efficiency:** Automated workflows reduce manual intervention.
- **Scalability:** Easily add or replace agents as business needs evolve.

---

## Conclusion üéâ

By integrating LangGraph with frameworks like AutoGen (and others such as CrewAI), you can build sophisticated multi-agent systems that:
- **Maintain context and history**
- **Streamline interactions**
- **Enable modular, scalable development**

This step-by-step guide‚Äîwith code examples and detailed explanations‚Äîshould help beginners understand and implement this powerful integration. Happy coding and enjoy building your multi-agent applications!