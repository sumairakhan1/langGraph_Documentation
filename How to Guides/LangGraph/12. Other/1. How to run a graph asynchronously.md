# 1. How to run a graph asynchronously

# üöÄ How to Run a Graph Asynchronously

Asynchronous programming can greatly improve performance by handling IO-bound tasks concurrently. This means your application doesn't need to wait for a task, like making a web request, to finish before it continues executing other tasks. In this guide, we'll take a synchronous graph and convert it into an asynchronous graph.

---

## üîß Key Concepts

### **What is Async Programming?**

In synchronous programming, tasks are executed one by one, blocking the execution of the program until the task is completed. Asynchronous programming allows tasks to run concurrently, meaning other tasks can be processed while waiting for one task to finish. This is particularly useful for IO-bound tasks like API calls or reading from files.

For example, imagine you need to make 10 API calls to gather information. Without async programming, each call would have to wait for the previous one to complete. With async programming, all 10 calls can happen concurrently, significantly reducing the overall time.

---

## üõ†Ô∏è Setting Up the Environment

To work with async graphs, you'll need to install the required libraries:

```bash
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_anthropic
```

Once the packages are installed, you can set up your API keys for the LLM (Large Language Model):

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
```

---

## üîë Understanding the State Graph

### **What is a State Graph?**

In the LangGraph library, a **StateGraph** is the main structure where each node represents a task, and the state flows between nodes. The state can be anything: data, messages, or results of computations.

We will track messages as our state in this example.

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]
```

Here, `State` holds a list of messages, and `add_messages` ensures the messages are appended correctly.

---

## üß∞ Setting Up the Tools

You can define tools that your graph will use. In this example, we create a placeholder search engine:

```python
from langchain_core.tools import tool

@tool
def search(query: str):
    """Call to surf the web."""
    return ["The answer to your question lies within."]
```

This tool simulates web searching, and the results are returned as a list of strings. Tools are used to execute actions within nodes.

---

## üßë‚Äçüíª Loading the Model

Next, we load a chat model that will work with the messages in our state. This is done using `langchain_anthropic`:

```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-haiku-20240307")
model = model.bind_tools(tools)
```

The model is bound with tools, meaning it can now execute the defined tools (like the search function) during its operation.

---

## üßë‚Äçüî¨ Defining Nodes and Conditional Logic

### **Async Functions**

We define each node as an asynchronous function. For example, `call_model` is an async function that invokes the model and gets a response:

```python
async def call_model(state: State):
    messages = state["messages"]
    response = await model.ainvoke(messages)  # Asynchronously calling the model
    return {"messages": [response]}  # Adding response to the messages
```

The `await` keyword allows us to wait for the model's response without blocking other tasks.

### **Conditional Edges**

To control the flow of the graph, we define a conditional function `should_continue`. This function decides whether to continue or stop based on the state:

```python
def should_continue(state: State) -> Literal["end", "continue"]:
    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls:
        return "end"  # Stop if no tool calls were made
    else:
        return "continue"  # Continue if tool calls were made
```

This function checks if the last message in the state has any tool calls, determining whether to continue or end the process.

---

## üèóÔ∏è Defining the Graph

Now we bring everything together by defining the graph and adding nodes and edges:

```python
from langgraph.graph import END, StateGraph, START

workflow = StateGraph(State)

workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)

workflow.add_edge(START, "agent")  # Start the graph with the "agent" node

# Conditional edge: Decide whether to continue or end
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "action",  # Continue to "action" if "continue"
        "end": END,  # End if "end"
    },
)

workflow.add_edge("action", "agent")  # After "action", go back to "agent"

app = workflow.compile()  # Compile the graph
```

### **Explanation of Code:**

1. **Add Nodes**: We add two nodes: `agent` (which calls the model) and `action` (which executes the tool).
2. **Add Edges**: We define the flow between nodes. First, the graph starts with the `agent` node. Then, based on the output of `should_continue`, it decides whether to continue to the `action` node or stop.
3. **Compile**: The `compile()` method generates the final asynchronous graph that can be run.

---

## üèÅ Visualizing the Graph

Finally, we visualize the graph using the `get_graph()` method, which shows the flow of the graph in a visual format:

```python
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))
```

This will display the graph as a diagram, helping you understand how data flows between nodes.

---

## üéØ Real-World Example

### **Use Case: Chatbot with External APIs**

Imagine you're building a chatbot that can fetch information from multiple external APIs. Without async programming, your chatbot would need to wait for each API call to finish before making the next one. With async programming, the chatbot can make several API calls concurrently, significantly improving its response time.

For example:
- **User asks a question**.
- The bot sends requests to several APIs for data.
- The bot waits for all responses at the same time without blocking.
- Once all responses are received, the bot aggregates and sends the final response.

---

## üìù Conclusion

By using async programming in a graph like LangGraph, you can efficiently manage multiple tasks concurrently, improving performance and scalability in real-time applications. Asynchronous execution allows the graph to handle several operations simultaneously, such as making multiple API requests or processing messages concurrently.

This pattern is essential for building complex applications like chatbots, web crawlers, or any system that requires handling multiple tasks in parallel without waiting for one to complete before starting another.

# LangChain: A Deep Dive into Streaming and Real-Time Interaction

LangChain offers a robust framework for handling AI-based conversational models, making it easier to build powerful applications that can engage in dynamic conversations, fetch data, and even stream outputs in real time. In this detailed explanation, we'll explore the streaming capabilities of LangChain, how it works, and where you can use it in real-world scenarios.

### üé¨ **What is LangChain Streaming?**

LangChain allows us to stream outputs from various nodes, including AI models and external tools, as they are produced. Instead of waiting for the final result, you can observe intermediate results, which is especially useful in real-time applications such as chatbots or virtual assistants.

### üí° **Real-World Use Case of Streaming in LangChain**

Consider an AI-based weather assistant. Instead of waiting for the AI to process the entire query and provide a final answer, streaming allows us to see intermediate outputs, such as tool calls and partial results. This gives the user an interactive experience where they can view data as it's being processed. For example, in a weather app, the user can start receiving weather updates, and they can interact with the app as more details are processed.

### üîß **Streaming Node Output**

The simplest form of streaming in LangChain is **streaming node output**. Each node's output is produced in real-time, and LangChain allows us to capture these outputs as they happen.

#### üñ• **Code Example: Streaming Node Output**

Let's go through a simple example to see how streaming works.

```python
from langchain_core.messages import HumanMessage

# Input to the application
inputs = {"messages": [HumanMessage(content="what is the weather in sf")]}

# Streaming output from the app, with updates from each node
async for output in app.astream(inputs, stream_mode="updates"):
    # Loop through each node's output
    for key, value in output.items():
        print(f"Output from node '{key}':")
        print("---")
        print(value["messages"][-1].pretty_print())  # Print the most recent message from the node
    print("\n---\n")
```

#### **Code Explanation:**

- **HumanMessage(content="what is the weather in sf")**: We start by sending a human message asking about the weather in San Francisco.
  
- **app.astream(inputs, stream_mode="updates")**: This initiates the streaming process. The `stream_mode="updates"` ensures that each node's output is returned incrementally as soon as it is available.

- **for key, value in output.items()**: The output of each node is processed here. For every node in the LangChain pipeline, we print the output.

- **value["messages"][-1].pretty_print()**: This line fetches the latest message produced by the node and formats it for readability.

#### üåê **Real-World Example of This Streaming**

- **Weather Assistant**: In the real world, as you ask a weather assistant about the forecast in San Francisco, you might first get preliminary data like "Fetching weather data..." followed by detailed information like temperature, wind speed, and humidity. Streaming shows these as they are obtained instead of waiting for the entire response.

### üßë‚Äçüíª **Streaming LLM Tokens**

LangChain also supports streaming Large Language Model (LLM) tokens as they are generated, providing real-time feedback on how the model is generating the response.

#### üñ• **Code Example: Streaming LLM Tokens**

```python
inputs = {"messages": [HumanMessage(content="what is the weather in sf")]}

# Using streaming logs to capture token-by-token generation by the model
async for output in app.astream_log(inputs, include_types=["llm"]):
    for op in output.ops:
        if op["path"] == "/streamed_output/-":
            # Output from the model's streaming process
            print("Partial output:", op["value"].get("text", ""))
        elif op["path"].startswith("/logs/") and op["path"].endswith("/streamed_output/-"):
            # Display tokens as they are being produced
            try:
                content = op["value"].content[0]
                print(content.get("text", ""), end="|")
            except:
                pass
```

#### **Code Explanation:**

- **app.astream_log(inputs, include_types=["llm"])**: This line captures the logs of the LLM model, specifically focusing on the streaming tokens produced by the model.

- **op["value"].get("text", "")**: The token content is extracted and displayed as the model produces it. 

- **print(content.get("text", ""), end="|")**: As the model generates output, it's printed on the screen as individual tokens (words, sentences, or partial sentences).

#### üåç **Real-World Example of LLM Token Streaming**

- **Interactive Content Generation**: Imagine you are building a chatbot that generates responses dynamically based on user input. By streaming tokens, the user can see the bot's response being built word-by-word or sentence-by-sentence. This can make the bot feel more alive and interactive, enhancing the user experience.

### ‚ö° **Advantages of Streaming in LangChain**

1. **Real-Time Interaction**: Users get to see intermediate outputs, making the process faster and more engaging.
2. **Efficient Resource Use**: Instead of waiting for the full output, you can fetch results incrementally, reducing overall latency.
3. **Enhanced User Experience**: Whether it's a chatbot or a virtual assistant, streaming ensures that the user receives updates as soon as they are available.

### üöÄ **Summary**

LangChain's streaming capabilities unlock real-time interactions by allowing the streaming of both node outputs and LLM tokens. With streaming, users can engage with the system as it processes their requests, resulting in a more dynamic and efficient user experience. This feature is perfect for real-time applications such as chatbots, weather assistants, or any app where timely responses matter.

---

Feel free to reach out if you need further clarification or more examples on how streaming works in LangChain!