# 5. How to integrate LangGraph (functional API) with AutoGen, CrewAI, and other frameworks

# Integrating LangGraph with AutoGen, CrewAI, and Other Frameworks

LangGraph is a powerful framework designed for creating agent-based applications. It can easily integrate with other agent frameworks such as AutoGen, CrewAI, and many others. The purpose of integrating LangGraph with other frameworks is to:

- **Create Multi-Agent Systems**: Combining agents built using different frameworks.
- **Leverage LangGraph Features**: Such as memory persistence, short and long-term memory, and streaming.

This tutorial will walk you through how to integrate LangGraph with AutoGen to create a chatbot, but the approach is applicable to other frameworks as well.

## üõ†Ô∏è Setup

First, let's start by setting up the necessary libraries. You can install LangGraph and AutoGen using the following command:

```bash
%pip install autogen langgraph
```

This will install both the `AutoGen` and `LangGraph` libraries.

### Setting Environment Variables

LangGraph requires API keys for services like OpenAI. Here's how to set up your environment variables:

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

- This code checks if an environment variable (like `OPENAI_API_KEY`) is set. If not, it prompts you to enter it.

## üîß Define AutoGen Agent

Next, we define our AutoGen agent. The AutoGen agent will interact with LangGraph and handle various tasks, such as responding to user queries. Here's the configuration for the AutoGen agent:

```python
import autogen
import os

config_list = [{"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY"]}]

llm_config = {
    "timeout": 600,
    "cache_seed": 42,
    "config_list": config_list,
    "temperature": 0,
}

autogen_agent = autogen.AssistantAgent(
    name="assistant",
    llm_config=llm_config,
)

user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={
        "work_dir": "web",
        "use_docker": False,
    },
    llm_config=llm_config,
    system_message="Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.",
)
```

### Explanation:

- **`AssistantAgent`**: This is the core agent of AutoGen that will respond to user queries.
- **`UserProxyAgent`**: This agent acts as the intermediary between the user and the assistant, helping manage interactions and controlling the flow of messages.

## üîÑ Create Workflow with LangGraph

Now, let's create a LangGraph workflow that integrates with the AutoGen agent. This workflow will handle the logic for interacting with the assistant.

```python
from typing import List
from langchain_core.messages import convert_to_openai_messages, BaseMessage
from langgraph.func import entrypoint, task
from langgraph.graph import add_messages
from langgraph.checkpoint.memory import MemorySaver

@task
def call_autogen_agent(messages: List[BaseMessage]):
    # Convert messages to OpenAI style
    messages = convert_to_openai_messages(messages)
    
    # Call the AutoGen agent to generate a response
    response = user_proxy.initiate_chat(
        autogen_agent,
        message=messages[-1],
        carryover=messages[:-1],
    )
    
    # Get the final response from the agent
    content = response.chat_history[-1]["content"]
    return {"role": "assistant", "content": content}

# Add short-term memory for storing conversation history
checkpointer = MemorySaver()

@entrypoint(checkpointer=checkpointer)
def workflow(messages: List[BaseMessage], previous: List[BaseMessage]):
    messages = add_messages(previous or [], messages)
    response = call_autogen_agent(messages).result()
    return entrypoint.final(value=response, save=add_messages(messages, response))
```

### Explanation:

1. **`@task` Decorator**: This turns the `call_autogen_agent` function into a LangGraph task, which means it will be executed as part of the graph.
2. **`convert_to_openai_messages`**: Converts the messages into a format that AutoGen understands.
3. **`MemorySaver`**: This is used to save the conversation history (short-term memory) so that the assistant can use past context in future interactions.

## üèÉ‚Äç‚ôÄÔ∏è Run the Workflow

Now that we have everything set up, let's run the LangGraph workflow. This will simulate a conversation with the AutoGen assistant.

```python
# Pass the thread ID to persist agent outputs for future interactions
config = {"configurable": {"thread_id": "1"}}

for chunk in workflow.stream(
    [
        {
            "role": "user",
            "content": "Find numbers between 10 and 30 in the Fibonacci sequence",
        }
    ],
    config,
):
    print(chunk)
```

### Explanation:

- **`workflow.stream()`**: This runs the workflow and streams the result.
- **`thread_id`**: This ensures that LangGraph persists conversation history for the thread, enabling the agent to remember previous interactions.

## üöÄ Example Interaction

When you run the workflow, the assistant will process the user's request and generate a response. Here is what a conversation might look like:

### User: "Find numbers between 10 and 30 in the Fibonacci sequence"

The assistant will respond with the Fibonacci numbers between 10 and 30 and execute the code:

```python
def fibonacci_sequence():
    a, b = 0, 1
    while a <= 30:
        if 10 <= a <= 30:
            print(a)
        a, b = b, a + b

fibonacci_sequence()
```

### Output:

```plaintext
13
21
```

Now, the assistant will proceed with the next user input: "Multiply the last number by 3"

### Assistant: "The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives: 63."

## üîÑ Re-using Conversation History

One of the key features of LangGraph is that you can continue conversations even after they end. By using the `thread_id`, the assistant will remember past interactions, making future conversations smoother.

---

## üåç Real-World Use Case: Multi-Agent Systems

In real-world applications, you might use LangGraph to build a multi-agent system. For example, in a **customer support system**:

- One agent (AutoGen) could handle general queries.
- Another agent (e.g., CrewAI) could handle more specialized tasks like technical troubleshooting.
- LangGraph would allow you to integrate these agents into a seamless workflow where each agent communicates with the user, and the system remembers past interactions.

This integration allows for more efficient collaboration between different agents, each specialized in a specific domain.

---

### Conclusion

LangGraph makes it easy to integrate with frameworks like AutoGen and others. By combining the features of multiple agents, such as memory, persistence, and context-sharing, you can build powerful and complex multi-agent systems. The code examples above provide a basic framework to get started, and you can expand them to suit your needs, such as adding long-term memory or working with additional frameworks like CrewAI.

# o3 mini

# How to Integrate LangGraph (Functional API) with AutoGen, CrewAI, and Other Frameworks üöÄ

In this guide, we'll walk through how to integrate **LangGraph** using its functional API with agent frameworks like **AutoGen** (and potentially others like CrewAI). We‚Äôll explain each piece of code step by step, so even beginners can follow along. Plus, we'll discuss a real-world example of how this integration can be used.

---

## 1. What is LangGraph and Why Integrate? üîó

**LangGraph** is a powerful framework that helps build multi-agent and agentic applications. By integrating LangGraph with frameworks like **AutoGen**, you can:
- **Create Multi-Agent Systems:** Different agents (each built using their own framework) work together.
- **Enhance Functionality:** Add features like persistence, streaming, and memory (both short-term and long-term) to your agents.
- **Modularize Your Workflow:** Each agent can be built and maintained independently, then connected into one cohesive system.

### Real-World Example: Automated Customer Support Chatbot üí¨

Imagine a customer support system where:
- **AutoGen** handles natural language understanding and generating responses.
- **CrewAI** manages task assignments and order tracking.
- **LangGraph** orchestrates the overall workflow, maintaining conversation history and coordinating between agents.

This setup can help create a smooth, interactive support experience while leveraging the strengths of each framework.

---

## 2. Setting Up the Environment üõ†Ô∏è

Before diving into the code, you need to install the required packages and set up your environment variables.

### Installation

Run this command to install **autogen** and **langgraph**:
```bash
%pip install autogen langgraph
```

### Environment Setup Code

```python
import getpass
import os

# Helper function to securely set an environment variable if it's not already set
def _set_env(var: str):
    if not os.environ.get(var):
        # Prompts the user to enter the value securely (e.g., for your API key)
        os.environ[var] = getpass.getpass(f"{var}: ")

# Securely set the OPENAI_API_KEY
_set_env("OPENAI_API_KEY")
```

**Explanation:**
- **getpass & os:** Used for securely handling sensitive information.
- **_set_env:** Checks if an environment variable exists; if not, it prompts you to enter it. This is crucial for protecting your API keys.

---

## 3. Defining the AutoGen Agent üéØ

We now define our AutoGen agent which is responsible for generating responses. This step sets up both the assistant agent and a proxy agent that simulates user interaction.

### Code Example

```python
import autogen
import os

# Configure the model settings with your API key
config_list = [{"model": "gpt-4o", "api_key": os.environ["OPENAI_API_KEY"]}]

# Define language model parameters
llm_config = {
    "timeout": 600,            # Maximum time (in seconds) for the API call to complete
    "cache_seed": 42,          # A seed value to help with caching for consistency
    "config_list": config_list, # List of model configurations
    "temperature": 0,          # Temperature 0 for deterministic output
}

# Create the assistant agent that generates responses
autogen_agent = autogen.AssistantAgent(
    name="assistant",
    llm_config=llm_config,
)

# Create a user proxy agent that simulates user input and manages the interaction flow
user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",  # Disables direct human input during execution
    max_consecutive_auto_reply=10,
    # Function to check if the task is complete based on the message content
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    # Configuration for executing any generated code safely
    code_execution_config={
        "work_dir": "web",
        "use_docker": False,   # Set to True if Docker is available for a safer execution environment
    },
    llm_config=llm_config,
    system_message="Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.",
)
```

**Line-by-Line Explanation:**
- **config_list & llm_config:** Specify the model (GPT-4 in this case) and its parameters, including API key, timeout, and temperature.
- **autogen.AssistantAgent:** Creates an agent that uses the language model to generate responses.
- **autogen.UserProxyAgent:** Creates a proxy agent to manage the interaction between the user and the assistant, including checking for termination signals and handling code execution safely.

---

## 4. Creating the Workflow with LangGraph Functional API üìù

In this step, we create a workflow using LangGraph's functional API. We will define tasks and the entrypoint of our workflow.

### Code Example

```python
from typing import List
from langchain_core.messages import convert_to_openai_messages, BaseMessage
from langgraph.func import entrypoint, task
from langgraph.graph import add_messages
from langgraph.checkpoint.memory import MemorySaver

# Define a task that calls the AutoGen agent with the current messages
@task
def call_autogen_agent(messages: List[BaseMessage]):
    # Convert messages to OpenAI's format for compatibility
    messages = convert_to_openai_messages(messages)
    
    # Initiate a chat with the assistant agent using the latest message and previous messages as context
    response = user_proxy.initiate_chat(
        autogen_agent,
        message=messages[-1],   # Latest message sent by the user
        carryover=messages[:-1] # All previous messages for context
    )
    
    # Extract the final response message from the agent's chat history
    content = response.chat_history[-1]["content"]
    return {"role": "assistant", "content": content}

# Create a MemorySaver instance to store conversation history
checkpointer = MemorySaver()

# Define the main workflow entrypoint with persistence enabled via checkpointer
@entrypoint(checkpointer=checkpointer)
def workflow(messages: List[BaseMessage], previous: List[BaseMessage]):
    # Combine previous messages (if any) with the new messages
    messages = add_messages(previous or [], messages)
    
    # Call our task and wait for the result
    response = call_autogen_agent(messages).result()
    
    # Return the final response and update the conversation history
    return entrypoint.final(value=response, save=add_messages(messages, response))
```

**Line-by-Line Explanation:**
- **Imports:**  
  - `List`: For type annotations.
  - `convert_to_openai_messages` & `BaseMessage`: To work with message formats.
  - `entrypoint` & `task`: Decorators provided by LangGraph for defining workflow functions.
  - `add_messages`: Function to combine new and previous messages.
  - `MemorySaver`: Provides persistence to store conversation history.

- **@task decorator:**  
  - **`call_autogen_agent` function:**  
    - Converts messages into a format that the OpenAI API can understand.
    - Uses `user_proxy.initiate_chat` to send the latest message (`messages[-1]`) along with previous context (`messages[:-1]`).
    - Extracts the assistant‚Äôs final response from the chat history.
    - Returns the response in a structured format.

- **MemorySaver:**  
  - **`checkpointer`:** Stores the conversation history so that context is preserved across multiple interactions.

- **@entrypoint decorator:**  
  - **`workflow` function:**  
    - Combines previous messages (if available) with the current ones using `add_messages`.
    - Calls `call_autogen_agent` to process the conversation.
    - Returns the final response using `entrypoint.final`, which also saves the updated conversation history.

---

## 5. Running the Workflow üöÄ

Once the workflow is defined, you can invoke it and stream responses. This allows you to maintain a conversation over multiple interactions.

### Code Example: Initial Query

```python
# Configuration to assign a unique thread ID for the conversation
config = {"configurable": {"thread_id": "1"}}

# Start the workflow by sending the initial user message
for chunk in workflow.stream(
    [
        {
            "role": "user",
            "content": "Find numbers between 10 and 30 in fibonacci sequence",
        }
    ],
    config,
):
    print(chunk)
```

**Explanation:**
- **config:**  
  - Assigns a `thread_id` to persist the conversation history across multiple calls.
- **workflow.stream:**  
  - Starts the workflow with the initial user message.
  - Streams the response (which could be chunked) and prints each chunk.

### Code Example: Continuing the Conversation

After the initial interaction, you can continue the conversation. For example, to ask the agent to multiply the last Fibonacci number by 3:

```python
for chunk in workflow.stream(
    [
        {
            "role": "user",
            "content": "Multiply the last number by 3",
        }
    ],
    config,
):
    print(chunk)
```

**Explanation:**
- Uses the same `thread_id` in `config` so that the conversation history is preserved.
- Sends a follow-up message to further process the previous result.

---

## 6. Real-World Use Case: Workflow Automation in Business üè¢

Consider a business scenario where multiple agents are working together:
- **AutoGen Agent:**  
  Handles natural language interactions and provides detailed responses.
- **CrewAI (or similar agent):**  
  Manages scheduling, task allocation, or order tracking.
- **LangGraph:**  
  Orchestrates the conversation by maintaining context and routing messages to the appropriate agent.

**Example Use Case:**  
A company‚Äôs customer support system can integrate these agents to offer quick, context-aware responses while also managing backend operations such as order processing and technical troubleshooting.

---

## 7. Conclusion üéâ

By integrating **LangGraph** (using its functional API) with frameworks like **AutoGen** and potentially **CrewAI**, you can build sophisticated, multi-agent systems that:
- **Maintain Conversation Context:** Thanks to persistence and memory features.
- **Streamline Agent Communication:** Through a well-defined workflow.
- **Enhance Modularity and Scalability:** Each agent can be developed independently and then integrated seamlessly.

This detailed guide‚Äîwith explanations of every code line, examples, and real-world applications‚Äîshould help beginners get started with creating powerful agent-based applications. Happy coding!