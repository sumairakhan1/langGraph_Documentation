# 2. How to Force Tool-Calling Agent to Structure Output

# üìä How to Force Tool-Calling Agent to Structure Output

In many cases, it's necessary to ensure that the output from an agent (a system that interacts with tools) is structured in a consistent way. This structured output can be important when the agent's output needs to be consumed by other software systems, or when consistency is crucial for downstream processing. In this tutorial, we will explore how to force a tool-calling agent to produce structured outputs using two different methods.

### 1. üéØ **Overview of the Process**

The main goal here is to ensure that the output of the agent is consistently structured, which means that we want to control how the agent's responses are formatted before they're given to the user or another system.

In this notebook, we'll work with a **ReAct agent**. This type of agent consists of a **model node** and a **tool-calling node**, with a third node that formats the response for the user.

We will cover two options for enforcing structured output:
- **Option 1**: Binding the output to a specific tool in the agent.
- **Option 2**: Using an additional LLM (Large Language Model) to structure the output.

### 2. üõ†Ô∏è **Option 1: Bind the Output to a Tool**

#### Concept:
In Option 1, we can bind the output of the tool that we want to use, forcing the agent to call that tool. Instead of allowing the agent to choose between tools or ending the conversation, the agent will directly choose between specific tools it needs to call.

**How does this work?**
- The agent selects an **action tool**, and after receiving the result from this action tool, it calls the **response tool**.
- The response tool formats the output and then sends it back to the user.

#### Advantages:
- You only need a **single LLM** (saving time and cost).
- **Lower latency** since fewer calls are made.
  
#### Disadvantages:
- The tool might not always be called correctly by the LLM.
- Multiple tools might be selected at once, requiring extra checks in the routing function.

#### Example Code for Option 1:
```python
from langchain_core.tools import tool
from langchain_anthropic import ChatAnthropic

# Tool for fetching weather
@tool
def get_weather(city: str):
    """Fetches weather for a given city."""
    if city == "nyc":
        return {"temperature": 70, "wind_direction": "NE", "wind_speed": 5}
    elif city == "sf":
        return {"temperature": 75, "wind_direction": "SE", "wind_speed": 3}

# Define model and bind tool
model = ChatAnthropic(model="claude-3-opus-20240229")
model_with_tools = model.bind_tools([get_weather])

# Example of invoking the tool
response = model_with_tools.invoke({"city": "nyc"})
print(response)
```

**Explanation of Code**:
- `get_weather(city: str)`: This tool returns weather information for a city.
- `model.bind_tools([get_weather])`: This binds the `get_weather` tool to the agent model, ensuring that the agent uses it when needed.
- `response = model_with_tools.invoke({"city": "nyc"})`: The agent invokes the weather tool with `"nyc"` as input.

---

### 3. ‚öôÔ∏è **Option 2: Using a Second LLM for Structured Output**

#### Concept:
In **Option 2**, instead of directly using the first LLM, we introduce an additional LLM that structures the output. This ensures that the final output follows the desired format.

**How does this work?**
- The agent node still selects the tool, but after that, the response is sent to a **second LLM** which will structure the output according to the predefined schema.
  
#### Advantages:
- **Guaranteed structured output**, as the second LLM is specifically designed to structure the response.
  
#### Disadvantages:
- **Higher cost and latency** due to the additional LLM call.

#### Example Code for Option 2:
```python
from pydantic import BaseModel, Field
from langchain_anthropic import ChatAnthropic
from langgraph.graph import MessagesState
from langchain_core.tools import tool

# Weather response model
class WeatherResponse(BaseModel):
    temperature: float
    wind_direction: str
    wind_speed: float

# Tool for fetching weather
@tool
def get_weather(city: str):
    """Fetches weather for a given city."""
    if city == "nyc":
        return {"temperature": 70, "wind_direction": "NE", "wind_speed": 5}
    elif city == "sf":
        return {"temperature": 75, "wind_direction": "SE", "wind_speed": 3}

# Define the agent and second LLM for structured output
model = ChatAnthropic(model="claude-3-opus-20240229")
model_with_tools = model.bind_tools([get_weather])
model_with_structured_output = model.with_structured_output(WeatherResponse)

# Example invocation
response = model_with_structured_output.invoke({"city": "nyc"})
print(response)
```

**Explanation of Code**:
- `WeatherResponse(BaseModel)`: This class defines the structured format for the weather data (temperature, wind direction, wind speed).
- `model.with_structured_output(WeatherResponse)`: This ensures the agent outputs the response in the `WeatherResponse` format.
- The agent will first call the `get_weather` tool, and then the second LLM will structure the output based on the `WeatherResponse` schema.

---

### 4. üí° **Real-World Use Cases**

#### 1. **Weather Application**:
   - When building a weather app, it is essential to get weather data from various sources. The structured format ensures that the data (e.g., temperature, wind speed, humidity) is returned in a consistent format every time. The agent ensures this data is structured in a predictable manner, so the app can easily parse and display it.

#### 2. **E-commerce Product Listings**:
   - Imagine an agent fetching product details like price, stock, and description. Using structured output ensures that the e-commerce platform always receives data in the same structure, making integration simpler and more reliable.

#### 3. **Customer Support Automation**:
   - In automated customer support, structured data (e.g., customer queries, solutions, and status updates) ensures that responses are consistent and easily understood by both customers and downstream systems.

---

### 5. üîÑ **Comparison of Both Options**

| **Feature**            | **Option 1**                           | **Option 2**                              |
|------------------------|----------------------------------------|-------------------------------------------|
| **Ease of Setup**      | Easier, uses one LLM                   | Requires an additional LLM                |
| **Cost**               | Lower, since it uses one LLM           | Higher, due to the extra LLM invocation   |
| **Output Structure**   | Not guaranteed                         | Guaranteed structured output              |
| **Latency**            | Lower, fewer calls                    | Higher, as it involves an additional step|

---

### Conclusion

By forcing the tool-calling agent to return structured output, you can ensure that the data your system receives is in a predictable, usable format. Depending on your requirements for consistency and performance, you can choose between the two options presented above.

# Option 1: Bind Output as Tool

In this section, we will dive deep into how to structure your agent‚Äôs output by **binding output as a tool**. This technique forces the agent to return structured data in a way that is consistent, no matter the context.

## üß† **What is this concept?**

This concept is about **forcing a tool-calling agent** to produce output in a **structured format** (such as a WeatherResponse object). The purpose is to ensure that no matter the tool or action, the final result will always follow the expected structure.

By binding the output as a tool, the agent is given instructions to select the **weather response tool** after it calls the initial weather-fetching tool. This ensures consistency and a structured format for each response.

---

## üèóÔ∏è **Real World Use Case:**

Imagine you have a **weather app** that queries different data sources (APIs) for weather information. You want your app to always return the weather data in a **consistent format** (e.g., temperature, wind speed, and wind direction) regardless of the source or tool used. Binding the output ensures the data is returned in the desired structure for processing or display.

---

## üñ•Ô∏è **Step-by-Step Code Explanation**

Let‚Äôs go through the code to understand how binding works. In this example, the goal is to fetch weather information and return it in a structured format.

### 1. **Imports & Setup:**

We first import the necessary components to set up the graph and tools.

```python
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
```

Here, `StateGraph` allows us to create a workflow, and `ToolNode` lets us define which tools will be used within the graph.

---

### 2. **Defining the Tools:**

```python
tools = [get_weather, WeatherResponse]
```

In this line, we define two tools:
- **`get_weather`**: The tool that retrieves weather data (e.g., fetching weather for SF).
- **`WeatherResponse`**: The structured format for the weather response (temperature, wind speed, etc.).

---

### 3. **Binding Tools to the Model:**

```python
model_with_response_tool = model.bind_tools(tools, tool_choice="any")
```

Here, we're binding our tools (`get_weather` and `WeatherResponse`) to the model, and using `tool_choice="any"` forces the model to choose any of the provided tools. This means the model must invoke a tool for every step.

---

### 4. **Calling the Model:**

```python
def call_model(state: AgentState):
    response = model_with_response_tool.invoke(state["messages"])
    return {"messages": [response]}
```

In this function, we are calling the model with the given **state** and retrieving a response. This is where the tools are used. The response is added to the list of **messages** and returned.

---

### 5. **Responding to the User:**

```python
def respond(state: AgentState):
    weather_tool_call = state["messages"][-1].tool_calls[0]
    response = WeatherResponse(**weather_tool_call["args"])
    tool_message = {
        "type": "tool",
        "content": "Here is your structured response",
        "tool_call_id": weather_tool_call["id"],
    }
    return {"final_response": response, "messages": [tool_message]}
```

In this function, we **extract the structured weather data** (e.g., temperature, wind speed) from the last tool call and use it to create a **WeatherResponse object**. This ensures that the output is always structured.

We also create a **tool message** to follow the AI's tool call requirements.

---

### 6. **Determining If the Process Should Continue:**

```python
def should_continue(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]
    if (
        len(last_message.tool_calls) == 1
        and last_message.tool_calls[0]["name"] == "WeatherResponse"
    ):
        return "respond"
    else:
        return "continue"
```

This function checks whether the model has finished processing and is ready to **respond to the user**. If the weather tool has been called and we are ready to provide a response, it triggers the **"respond"** function. Otherwise, it tells the model to continue.

---

### 7. **Setting Up the Workflow:**

```python
workflow = StateGraph(AgentState)

workflow.add_node("agent", call_model)
workflow.add_node("respond", respond)
workflow.add_node("tools", ToolNode(tools))

workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "tools",
        "respond": "respond",
    },
)

workflow.add_edge("tools", "agent")
workflow.add_edge("respond", END)
graph = workflow.compile()
```

Here, we define the **workflow** using `StateGraph` and specify how the nodes interact:
- The **"agent"** node is the starting point.
- The **"respond"** node provides the structured response.
- The **"tools"** node calls the necessary tools to fetch the data.

We also define **conditional edges** to determine whether the agent should continue or respond to the user.

---

### 8. **Running the Workflow:**

```python
answer = graph.invoke(input={"messages": [("human", "what's the weather in SF?")]})["final_response"]
```

Finally, we invoke the graph with an input message asking about the weather in SF. The graph processes the message and returns the structured **WeatherResponse** object.

---

## üèÜ **What Did We Accomplish?**

Using **Option 1: Bind Output as Tool**, we ensured that the agent always returns structured output, even when calling different tools. This method guarantees consistency in the response format.

### **Pros:**
- Reduced complexity by requiring only a single LLM.
- It‚Äôs cost-effective and fast.
  
### **Cons:**
- Not foolproof, as the LLM may not always select the correct tools.
- If the agent calls multiple tools, additional checks are needed.

---

## üåç **Conclusion:**

Binding the output as a tool allows you to control the structure of the agent‚Äôs response, ensuring consistency no matter the context. This is especially useful in applications like weather apps, data-fetching tools, or any system that requires structured data for downstream processing.

By following this approach, you can make sure that your agent‚Äôs responses are **reliable** and **predictable**, which is critical for real-world applications where structured data is needed.

## üåü Understanding the Use of Multiple LLMs in a Workflow

In this explanation, we'll walk through the concept of using two Large Language Models (LLMs) in a workflow to generate structured output. The goal is to understand how to define a workflow using two models, and how you can integrate them effectively to handle tasks that require structured responses, like weather forecasting.

This workflow is useful in real-world applications such as **AI-driven customer support systems**, where structured data (like weather information) is needed in a consistent format, or **virtual assistants** that provide structured outputs to integrate with other tools.

---

### üßë‚Äçüíª Code Walkthrough: Using Two LLMs

#### **1. Defining the Graph:**

To define the workflow, we create a graph where different tasks are represented as nodes. Here, we're adding LLM tools and defining how the model should behave based on incoming messages.

```python
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage
```

- **`StateGraph`**: This is where we define the flow of tasks. Each task or action will be represented as a node in the graph.
- **`END`**: Represents the end of the workflow once the task is complete.
- **`ToolNode`**: This represents a tool or action within the graph.
- **`HumanMessage`**: Represents a message coming from the user.

---

#### **2. Call the First LLM (Initial Model)**

The first step in the workflow is calling a model with the provided inputs to start processing.

```python
def call_model(state: AgentState):
    response = model_with_tools.invoke(state["messages"])
    return {"messages": [response]}
```

- **`state`**: This contains the current state of the conversation, including messages and responses.
- **`model_with_tools.invoke()`**: This is where we invoke the LLM to process the message and get a response.
- **`response`**: The response from the model is returned and added to the `messages` list for further processing.

---

#### **3. Handling Responses Using Structured Output**

Next, we define how the model will respond to the user. If the tool returns structured data, we make sure the response is consistent.

```python
def respond(state: AgentState):
    response = model_with_structured_output.invoke(
        [HumanMessage(content=state["messages"][-2].content)]
    )
    return {"final_response": response}
```

- **`model_with_structured_output.invoke()`**: The model is called here to process the structured message format. We take the message content and pass it to the model, which ensures the output is structured in a consistent format.
- **`HumanMessage(content=...)`**: This is a human-readable message sent to the model.

---

#### **4. Determining Whether to Continue or Respond**

The decision of whether to continue processing with another tool or respond to the user depends on the presence of tool calls in the conversation history.

```python
def should_continue(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]
    
    if not last_message.tool_calls:
        return "respond"
    else:
        return "continue"
```

- **`last_message.tool_calls`**: Checks whether there was a previous tool call. If there is no tool call, it means we can respond with the final answer; otherwise, we continue to the next tool.

---

#### **5. Workflow Definition**

Here, we define the entire workflow, setting entry points, conditional edges, and actions.

```python
workflow = StateGraph(AgentState)

workflow.add_node("agent", call_model)
workflow.add_node("respond", respond)
workflow.add_node("tools", ToolNode(tools))

workflow.set_entry_point("agent")

workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "tools",
        "respond": "respond",
    },
)

workflow.add_edge("tools", "agent")
workflow.add_edge("respond", END)
graph = workflow.compile()
```

- **`add_node()`**: Adds nodes to the graph, representing different actions or stages in the workflow (calling the model, responding to the user, etc.).
- **`set_entry_point()`**: Specifies which node should be executed first in the workflow.
- **`add_conditional_edges()`**: Adds conditional logic to decide which path to follow (whether to continue or respond based on previous messages).
- **`compile()`**: Compiles the graph into a runnable object.

---

#### **6. Running the Graph**

Finally, the workflow is invoked, and the system processes the input to return a structured response.

```python
answer = graph.invoke(input={"messages": [("human", "what's the weather in SF?")]})["final_response"]
```

- **`graph.invoke()`**: This starts the workflow by providing the initial input (the user's message). The model processes this input, and the graph determines whether it should continue to another step or respond.
- **`final_response`**: The final output of the workflow, which would be the structured weather data in this case.

---

### üåç Real-World Example: Virtual Assistants

In real-world applications, this concept is highly useful for **virtual assistants** that need to perform multiple tasks, like gathering weather data, scheduling appointments, or fetching data from APIs.

For instance:
- **Weather forecasting**: The assistant asks for the weather, invokes a tool to fetch the data, and ensures the response is structured.
- **Customer support**: The assistant fetches structured data (order details, shipping status) and formats it appropriately before responding to the customer.

By using two LLMs (one for task execution and one for structured response), you ensure that your system can handle complex workflows and provide users with consistent and accurate information.

---

### üìä Summary

- **Two LLMs**: One LLM handles tasks (e.g., fetching weather data), and another ensures the output is structured for further use or user-friendly presentation.
- **State Graph**: Defines the workflow and how tasks should be executed based on conditions.
- **ToolNode**: Represents a specific action or tool in the workflow.
- **Real-World Use**: Ideal for virtual assistants, customer support bots, or any system that needs to process information and respond in a structured format.