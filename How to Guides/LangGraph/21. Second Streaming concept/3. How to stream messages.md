# 3. How to stream messages


# How to Stream Messages from Your Graph üì°

Streaming messages from your graph enables you to receive real-time updates from the nodes in your graph as they process data. When using `stream_mode="messages-tuple"`, individual tokens (messages) generated by the model are streamed back, helping you observe the progression of data in your system step-by-step.

## **What is Streaming in this Context?** üöÄ

In streaming, the system sends data continuously, as it is generated, instead of waiting until the entire process is complete. This helps in tracking ongoing activities in real-time. Specifically, `stream_mode="messages-tuple"` allows you to see individual tokens (like small chunks of messages) being generated, which is particularly useful when processing large or continuous conversations.

### **Key Concept:**

- **Messages:** These are individual pieces of information or tokens sent by the AI model in response to a query.
- **Tuple:** A tuple is a collection of data, typically structured as (message, metadata). Here, metadata can be additional information related to the message, like its origin or type.

---

## **Step-by-Step Example of Setting Up Your Client** üõ†Ô∏è

Before we start streaming messages, we need to set up a few things:

1. **Client Setup:**
   The client helps you connect to your deployed graph and lets you interact with it programmatically.

```python
from langgraph_sdk import get_client

# Connect to your graph using the deployment URL
client = get_client(url=<DEPLOYMENT_URL>)

# Reference to the graph deployed with the name "agent"
assistant_id = "agent"

# Create a new thread to manage the conversation
thread = await client.threads.create()
print(thread)
```

Here, the `get_client` function connects to the graph deployed on a server. A thread is then created, which is like a session where the conversation will happen.

**Output Example:**
```json
{
    'thread_id': 'e1431c95-e241-4d1d-a252-27eceb1e5c86',
    'created_at': '2024-06-21T15:48:59.808924+00:00',
    'updated_at': '2024-06-21T15:48:59.808924+00:00',
    'status': 'idle',
    'values': None
}
```

---

## **Streaming Messages in `messages-tuple` Mode** üìú

Now that the client and thread are set up, let's stream the messages from the graph in `messages-tuple` mode.

In this mode, messages (or tokens) are streamed back as they are generated by the model in response to the input.

### **Code Example for Streaming Messages:**

```python
input = {
    "messages": [
        {"role": "user", "content": "what's the weather in SF?"}
    ]
}
config = {"configurable": {"model_name": "openai"}}

async for chunk in client.runs.stream(
    thread["thread_id"],
    assistant_id=assistant_id,
    input=input,
    config=config,
    stream_mode="messages-tuple"
):
    print(f"Receiving new event of type: {chunk.event}...")
    print(chunk.data)
    print("\n\n")
```

### **What Happens Here?**

- The user asks about the weather in San Francisco.
- The graph sends the request to an AI model (OpenAI in this case).
- The model starts processing the input and returns messages back in chunks, which are streamed.

---

### **Sample Output:**

When the streaming happens, you will get real-time updates:

```json
Receiving new event of type: metadata...
{"run_id": "1ef971e0-9a84-6154-9047-247b4ce89c4d", "attempt": 1}

Receiving new event of type: messages...
[
  {
    "type": "AIMessageChunk",
    "tool_calls": [
      {
        "name": "tavily_search_results_json",
        "args": {
          "query": "weat"
        },
        "id": "toolu_0114XKXdNtHQEa3ozmY1uDdM"
      }
    ]
  }
]
```

Here, the message is processed in multiple chunks. Each chunk contains some text or tool calls related to the weather query.

---

## **Breaking Down the Key Parts of the Output:**

- **Event Type:**
   - `metadata`: This tells us information about the task, like the run ID.
   - `messages`: This contains the actual data being processed. It could be the response to the user‚Äôs query, or some intermediate processing steps.

- **Tool Calls:** 
   - These represent actions the AI model takes to answer the query. For example, it may call an external weather API to fetch data, as seen with the query `"weat"` in the example.

---

## **Real-World Example:**

Imagine you're building a customer support chatbot for an e-commerce website. This bot needs to interact with customers and fetch real-time product details or weather updates. By streaming messages, the bot can send the user immediate, partial responses while still processing complex requests in the background.

For example, a customer might ask, "What's the weather in my city?" The bot could send a quick response like "Fetching the weather..." while calling an API in the background. As the response is processed and streamed back in real-time, the bot can progressively display relevant weather data.

---

## **When Would You Use Streaming in Your Application?** üèôÔ∏è

### **Key Use Cases:**

- **Real-time chatbots:** Streaming allows users to see live responses, enhancing the experience.
- **Weather updates:** As shown in the example, the weather can be fetched in chunks, providing users with updates as they come in.
- **Interactive applications:** Any application that requires real-time updates, such as stock price tracking or live sports commentary, could benefit from streaming.

### **Why is this Useful?** üß†

- **Improved User Experience:** Users receive real-time responses, even for complex queries, rather than waiting for the entire process to complete.
- **Faster Feedback:** When the system is handling tasks in the background (e.g., API calls or computations), users still see incremental updates, making the interaction feel faster.

---

## **Conclusion**

Streaming messages from your graph with `messages-tuple` mode allows you to track every message generated by a model in real-time. This feature is especially beneficial for chatbots, real-time applications, and anything requiring immediate feedback to users. It helps make the system feel more interactive and dynamic, which is essential for modern-day applications.