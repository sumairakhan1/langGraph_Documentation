# 1. How to wait for user input using interrupt

# How to Wait for User Input Using Interrupt

Human-in-the-loop (HIL) interactions are crucial for making systems more interactive by waiting for feedback from users at specific points during execution. This concept is especially useful in scenarios where the system needs human input before it can proceed with further actions. For instance, if you're building an application that helps users make decisions or give feedback, waiting for user input allows the system to be more responsive and adaptive.

## What is `interrupt()` and Why Do We Use It? 

In agentic systems, you may need to pause the execution of a program to get input from the user. This can be done using the `interrupt()` function. The `interrupt()` function halts the execution of the graph (which is a sequence of operations or tasks), allows the system to ask the user for feedback, and then continues when the feedback is received.

In LangGraph, this can be done using the `interrupt()` function, which gives the flexibility to pause and resume execution with the user‚Äôs input.

## Real-World Example
Imagine you're building a customer support bot that helps users troubleshoot issues. The bot can ask clarifying questions (e.g., "What kind of device are you using?") and wait for the user to respond before continuing to the next step (e.g., providing relevant solutions based on the device type). This is a classic example of using `interrupt()` to collect input before proceeding.

---

## Setting Up the Environment

Before we dive into the code, we need to set up the required environment. We will use the following tools:
- **LangGraph**: A Python library used to create agentic systems where the flow can be interrupted for user input.
- **Langchain**: A library for interacting with language models (e.g., OpenAI's GPT, Anthropic's models).

### 1. Install the Necessary Packages

```bash
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_anthropic
```

### 2. Set Up API Keys

If you're using models like OpenAI or Anthropic, you need to set up API keys to authenticate your system.

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

# Setting the API key for Anthropic
_set_env("ANTHROPIC_API_KEY")
```

This will prompt you to input your API key securely.

---

## Implementing User Input Wait with `interrupt()`

Let's walk through the process of using the `interrupt()` function to wait for user input. Here's the basic flow:
1. **Create a Graph**: You define a series of steps that your system will perform.
2. **Pause at a Specific Step**: Use `interrupt()` to pause at the `human_feedback` node, where you will ask for user input.
3. **Resume Execution**: Once the user provides feedback, the execution of the graph continues.

### Code Example

```python
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command, interrupt
from langgraph.checkpoint.memory import MemorySaver
from IPython.display import Image, display

# Defining the state of our system
class State(TypedDict):
    input: str
    user_feedback: str

# Step 1: Initial step, no specific logic here
def step_1(state):
    print("---Step 1---")
    pass

# Human feedback step: Pauses the system for user input
def human_feedback(state):
    print("---human_feedback---")
    feedback = interrupt("Please provide feedback:")
    return {"user_feedback": feedback}

# Step 3: Final step after feedback is received
def step_3(state):
    print("---Step 3---")
    pass

# Building the graph with steps
builder = StateGraph(State)
builder.add_node("step_1", step_1)
builder.add_node("human_feedback", human_feedback)
builder.add_node("step_3", step_3)

# Connecting the steps in sequence
builder.add_edge(START, "step_1")
builder.add_edge("step_1", "human_feedback")
builder.add_edge("human_feedback", "step_3")
builder.add_edge("step_3", END)

# Set up memory to save the graph state
memory = MemorySaver()

# Compile the graph
graph = builder.compile(checkpointer=memory)

# Visualize the graph
display(Image(graph.get_graph().draw_mermaid_png()))
```

### Code Explanation:

1. **State**: We define the state as a `TypedDict` to hold the input and user feedback.
   ```python
   class State(TypedDict):
       input: str
       user_feedback: str
   ```

2. **Step Functions**: We define several steps (`step_1`, `human_feedback`, `step_3`) to represent stages in the system. The feedback step uses `interrupt()` to pause for user input.
   ```python
   def human_feedback(state):
       print("---human_feedback---")
       feedback = interrupt("Please provide feedback:")
       return {"user_feedback": feedback}
   ```

3. **Graph Building**: We create the state machine (graph) and define how nodes (steps) are connected. The flow starts at `step_1`, moves to `human_feedback` (where we wait for input), and then proceeds to `step_3` after the feedback is provided.

4. **Memory Setup**: The memory saver stores the state of the graph, so it can be resumed later after the user input is received.
   ```python
   memory = MemorySaver()
   ```

5. **Compile the Graph**: We compile the graph to prepare for execution.
   ```python
   graph = builder.compile(checkpointer=memory)
   ```

---

## Running the Graph with User Input

Now that we‚Äôve defined the graph, we can run it and see how the system waits for user input at the `human_feedback` step.

### First Run: Wait for Feedback

```python
# Initial input
initial_input = {"input": "hello world"}

# Start the thread with the initial configuration
thread = {"configurable": {"thread_id": "1"}}

# Run the graph until it reaches the interruption point
for event in graph.stream(initial_input, thread, stream_mode="updates"):
    print(event)
    print("\n")
```

### Output:
```
---Step 1---
{'step_1': None}

---human_feedback---
{'__interrupt__': (Interrupt(value='Please provide feedback:', resumable=True, ns=['human_feedback:e9a51d27-22ed-8c01-3f17-0ed33209b554'], when='during'),)}
```

At this point, the execution pauses and waits for the user to provide feedback.

### Second Run: Resume After Feedback

After receiving the feedback from the user (e.g., "go to step 3!"), we continue the graph execution.

```python
# Continue the graph execution with user input
for event in graph.stream(
    Command(resume="go to step 3!"), thread, stream_mode="updates"
):
    print(event)
    print("\n")
```

### Output:
```
---human_feedback---
{'human_feedback': {'user_feedback': 'go to step 3!'}}

---Step 3---
{'step_3': None}
```

---

## Conclusion

Using the `interrupt()` function in LangGraph allows you to pause execution at a specific node and wait for user input before resuming. This is useful in real-world scenarios like customer support bots, decision-making assistants, or any interactive system that requires human feedback before proceeding.

---
# üöÄ How to Wait for User Input Using Interrupt

In this guide, we'll learn how to pause a program to wait for user input using an **interrupt**. This concept is essential for creating interactive applications where human feedback is needed before proceeding. We'll explain everything step-by-step with detailed code examples, making it accessible for beginners. We'll also discuss a real-world use case to help you see how this concept applies beyond the code.

---

## ‚öôÔ∏è What Is an Interrupt?

An **interrupt** is a way to temporarily halt the execution of a program so that it can wait for something‚Äîusually input from a user. In our context, using LangGraph (a framework for building language-based graphs), the `interrupt()` function stops the graph's execution. This pause allows the system to ask the user for clarifying input and then resume processing once the input is provided.

---

## üõ†Ô∏è Setup and Dependencies

Before diving into the code, you need to install the required packages and set up your API keys. This step ensures that our LangGraph and language model (like Anthropic or OpenAI) work properly.

```python
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_anthropic
```

### Explanation:
- **`%%capture --no-stderr`**: Captures the output so that the installation messages don‚Äôt clutter your notebook.
- **`%pip install --quiet -U langgraph langchain_anthropic`**: Installs (or updates) the necessary packages quietly.

Next, we set up our API keys using a helper function:

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
```

### Explanation:
- **`import getpass, os`**: Imports modules to handle secure input and environment variables.
- **`_set_env` function**: Checks if the API key (e.g., `ANTHROPIC_API_KEY`) is set in the environment. If not, it prompts the user to enter it securely.
- **`_set_env("ANTHROPIC_API_KEY")`**: Calls the function to set the Anthropic API key.

---

## üìö Code Walkthrough

Below is a simplified version of the code that demonstrates how to wait for user input using an interrupt in a LangGraph. We'll break down each part.

### 1. Define the State and Nodes

```python
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command, interrupt
from langgraph.checkpoint.memory import MemorySaver
from IPython.display import Image, display

# Define a state with two keys: one for initial input and one for user feedback.
class State(TypedDict):
    input: str
    user_feedback: str
```

#### Explanation:
- **`TypedDict`**: Used to define a dictionary with specific keys and types.
- **`StateGraph, START, END`**: Core components from LangGraph to build and manage our graph.
- **`Command, interrupt`**: Types and functions that help us manage commands and interruptions.
- **`MemorySaver`**: A checkpoint mechanism to save the graph‚Äôs state.
- **`IPython.display`**: For displaying images of our graph.
- **`class State(TypedDict):`**: We define our state with an `input` (initial data) and `user_feedback` (to store user input).

---

### 2. Create Nodes (Steps in the Graph)

```python
def step_1(state):
    print("---Step 1---")
    # Placeholder for step 1 logic
    pass

def human_feedback(state):
    print("---human_feedback---")
    # Pause execution and request user feedback
    feedback = interrupt("Please provide feedback:")
    # Store the feedback in the state
    return {"user_feedback": feedback}

def step_3(state):
    print("---Step 3---")
    # Placeholder for step 3 logic
    pass
```

#### Explanation:
- **`step_1` function**:
  - Prints `---Step 1---` to indicate execution.
  - Contains a `pass` statement as a placeholder for actual logic.
  
- **`human_feedback` function**:
  - Prints `---human_feedback---`.
  - Calls `interrupt("Please provide feedback:")`:
    - **What It Does**: Halts the graph and waits for the user to enter feedback.
    - **Why It‚Äôs Important**: Allows human-in-the-loop interactions where the program pauses to receive input.
  - Returns a dictionary updating the state with the key `user_feedback`.

- **`step_3` function**:
  - Prints `---Step 3---` to indicate execution.
  - Contains a `pass` statement as a placeholder for further logic.

---

### 3. Build and Visualize the Graph

```python
# Create a graph with the defined state structure
builder = StateGraph(State)
builder.add_node("step_1", step_1)
builder.add_node("human_feedback", human_feedback)
builder.add_node("step_3", step_3)

# Define the flow of nodes using edges
builder.add_edge(START, "step_1")
builder.add_edge("step_1", "human_feedback")
builder.add_edge("human_feedback", "step_3")
builder.add_edge("step_3", END)

# Set up a memory checkpoint to save the state
memory = MemorySaver()

# Compile the graph with the memory checkpoint
graph = builder.compile(checkpointer=memory)

# Display a visual representation of the graph
display(Image(graph.get_graph().draw_mermaid_png()))
```

#### Explanation:
- **Graph Initialization**:
  - **`builder = StateGraph(State)`**: Creates a new state graph using our defined `State`.
- **Adding Nodes**:
  - **`builder.add_node("step_1", step_1)`**: Adds `step_1` node.
  - **`builder.add_node("human_feedback", human_feedback)`**: Adds node to capture human feedback.
  - **`builder.add_node("step_3", step_3)`**: Adds `step_3` node.
- **Connecting Nodes**:
  - **`builder.add_edge(START, "step_1")`**: Connects the start of the graph to `step_1`.
  - Subsequent edges define the flow from `step_1` to `human_feedback`, then to `step_3`, and finally to `END`.
- **MemorySaver**:
  - **`memory = MemorySaver()`**: Sets up a memory-based checkpoint to track the state.
- **Compiling the Graph**:
  - **`graph = builder.compile(checkpointer=memory)`**: Finalizes the graph, integrating the checkpoint system.
- **Visualization**:
  - **`display(Image(graph.get_graph().draw_mermaid_png()))`**: Displays a visual diagram of the graph using Mermaid syntax.

---

### 4. Running the Graph and Handling Interrupts

#### **Initial Run Until the Interruption:**

```python
# Define the initial state with some input
initial_input = {"input": "hello world"}

# Thread configuration for the execution context
thread = {"configurable": {"thread_id": "1"}}

# Execute the graph until it reaches the interruption
for event in graph.stream(initial_input, thread, stream_mode="updates"):
    print(event)
    print("\n")
```

#### Explanation:
- **`initial_input`**: Sets the starting input for the graph.
- **`thread`**: Contains configuration details (like a thread ID) to manage the execution context.
- **`graph.stream(...)`**: Runs the graph and streams updates until it reaches the `interrupt()` call in `human_feedback`.
  - The loop prints out each event, showing the execution progress:
    - **`---Step 1---`** is executed first.
    - Execution pauses at **`human_feedback`**, waiting for user input.

#### **Resuming Execution After User Input:**

```python
# Provide the user feedback to resume the graph execution
for event in graph.stream(
    Command(resume="go to step 3!"), thread, stream_mode="updates"
):
    print(event)
    print("\n")
```

#### Explanation:
- **`Command(resume="go to step 3!")`**: Creates a command to resume execution, providing the user feedback (`"go to step 3!"`).
- **`graph.stream(...)`**: Resumes the graph from where it was interrupted.
  - The graph continues by executing `step_3` after processing the feedback.
- Finally, you can check the state to confirm that the feedback was recorded:

```python
# Check the final state of the graph
print(graph.get_state(thread).values)
```

- **Output**:  
  ```python
  {'input': 'hello world', 'user_feedback': 'go to step 3!'}
  ```
  This confirms that the state now includes the user feedback.

---

## üåç Real-World Example: Interactive Customer Support

Imagine an interactive customer support chatbot. Here‚Äôs how the interrupt mechanism could be used in a real-world scenario:

1. **Initial Query**: A customer initiates a conversation with a chatbot.
2. **Clarification Request**: The chatbot might ask, "Could you please provide more details about your issue?"
3. **Waiting for Input**: Using an interrupt, the chatbot pauses its automated flow until the customer replies.
4. **Resume Process**: Once the customer provides additional details, the chatbot resumes processing the information and offers a tailored solution.

This human-in-the-loop design ensures that the system behaves intelligently by confirming the user's needs before proceeding.

---

## üéØ Summary

- **Interrupts** are used to pause execution and wait for user input.
- **LangGraph** enables you to build workflows (graphs) that can handle human feedback seamlessly.
- **Real-World Use Case**: Chatbots in customer support use interrupts to clarify user issues before proceeding.
- **Step-by-Step**:  
  1. **Setup**: Install dependencies and configure API keys.  
  2. **Define Nodes**: Create functions for each step in your workflow.  
  3. **Build the Graph**: Connect nodes to form the process flow.  
  4. **Run & Resume**: Execute until an interrupt is hit, then resume once input is provided.

By following this guide, you can integrate human-in-the-loop interactions into your applications, making them more interactive and responsive to real-time user feedback.

---

# ü§ñ Agent: Integrating Human Feedback into a ReAct-Style Agent

In this guide, we'll walk through building a **ReAct-style agent** that can ask clarifying questions from a human during its workflow. This agent uses Anthropic's chat model, a real tool (a web search), and a mock tool that triggers human input when needed.

We'll explain everything in detail so even a beginner can follow along. We'll cover what each code block does, line by line, and then look at a real-world example of how this concept can be useful.

---

## üìù What Is a ReAct-Style Agent?

A **ReAct-style agent** combines reasoning and acting. It:
- **Thinks** by using a language model to generate responses or decide what to do next.
- **Acts** by calling tools (like a search function) or asking the human for clarification when needed.

**Real-World Example:**  
Imagine a travel assistant chatbot. It might start by asking, "Where do you want to go?" If the user's request is unclear, the assistant could pause, ask a clarifying question (e.g., "What is your departure city?"), and then use a tool to search for travel options.

---

## üîß Step 1: Setting Up the Environment and Tools

### 1.1 Define the State

The **state** holds all the conversation messages. We use `MessagesState` to store our chat history.

```python
from langgraph.graph import MessagesState, START
```

- **`MessagesState`**: A data structure that holds a list of messages (conversation history).
- **`START`**: A constant that marks the beginning of the graph.

### 1.2 Define a Real Tool: The Search Function

We create a simple search tool using the `@tool` decorator. This tool simulates a web search.

```python
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode

@tool
def search(query: str):
    """Call to surf the web."""
    # Placeholder for an actual web search implementation.
    return f"I looked up: {query}. Result: It's sunny in San Francisco, but you better look out if you're a Gemini üòà."

tools = [search]
tool_node = ToolNode(tools)
```

- **`@tool`**: Decorator to mark a function as a tool callable by the agent.
- **`search(query: str)`**: Takes a search query and returns a simulated result.
- **`ToolNode`**: Wraps our tools so they can be used as a node in our workflow.

---

## üõ†Ô∏è Step 2: Setting Up the Model and Binding Tools

### 2.1 Instantiate the Chat Model

We use Anthropic's chat model to generate responses.

```python
from langchain_anthropic import ChatAnthropic
model = ChatAnthropic(model="claude-3-5-sonnet-latest")
```

- **`ChatAnthropic`**: The class that provides access to Anthropic‚Äôs chat model.
- **`model="claude-3-5-sonnet-latest"`**: Specifies which version of the model to use.

### 2.2 Define a Mock Tool for Human Interaction

We define a mock tool using Pydantic's `BaseModel` to represent a human query.

```python
from pydantic import BaseModel

class AskHuman(BaseModel):
    """Ask the human a question"""
    question: str
```

- **`AskHuman`**: A simple model representing a request for human input.
- **`question: str`**: The question that will be asked of the human.

### 2.3 Bind Tools to the Model

We bind both our real tool (`search`) and our mock tool (`AskHuman`) to the model.

```python
model = model.bind_tools(tools + [AskHuman])
```

- **`bind_tools`**: Integrates our tools with the language model so that the agent can decide when to call each tool.

---

## üîÑ Step 3: Defining Nodes and Workflow Logic

### 3.1 Determine the Next Step with `should_continue`

This function checks the latest message to decide what to do next.

```python
def should_continue(state):
    messages = state["messages"]
    last_message = messages[-1]
    # If there is no tool call, we finish.
    if not last_message.tool_calls:
        return END
    # If the tool call is asking the human, go to the "ask_human" node.
    elif last_message.tool_calls[0]["name"] == "AskHuman":
        return "ask_human"
    # Otherwise, go to the "action" node.
    else:
        return "action"
```

- **Check for tool calls:**  
  - **No tool call:** Ends the conversation.
  - **`AskHuman` tool call:** Redirects to a node that asks the human for input.
  - **Otherwise:** Proceeds to use the tool (i.e., the search action).

### 3.2 Node to Call the Model

This node sends the conversation history to the model and retrieves a response.

```python
def call_model(state):
    messages = state["messages"]
    response = model.invoke(messages)
    # Return a list so that the new response is added to existing messages.
    return {"messages": [response]}
```

- **`model.invoke(messages)`**: Sends messages to the model to generate a new response.
- **Return value:** Adds the new response to the list of messages in the state.

### 3.3 Node to Ask the Human

When the agent needs clarification, this node interrupts the process and asks the human for input.

```python
def ask_human(state):
    tool_call_id = state["messages"][-1].tool_calls[0]["id"]
    location = interrupt("Please provide your location:")
    tool_message = [{"tool_call_id": tool_call_id, "type": "tool", "content": location}]
    return {"messages": tool_message}
```

- **`tool_call_id`**: Retrieves the identifier of the current tool call that requires human input.
- **`interrupt(...)`**: Pauses the workflow and prompts the user with the message `"Please provide your location:"`.
- **`tool_message`**: Constructs a message that includes the human‚Äôs response.
- **Return value:** Updates the state with the human-provided response.

---

## üîó Step 4: Building the Graph Workflow

We now build the graph that defines how our agent flows between nodes.

```python
from langgraph.graph import END, StateGraph

# Create a new graph with a messages state.
workflow = StateGraph(MessagesState)

# Add nodes for each part of the workflow.
workflow.add_node("agent", call_model)   # The agent that calls the model.
workflow.add_node("action", tool_node)     # Node to handle tool calls (e.g., search).
workflow.add_node("ask_human", ask_human)   # Node to request human input.

# Set the entrypoint: the "agent" node starts the conversation.
workflow.add_edge(START, "agent")

# Add conditional edges after the "agent" node.
workflow.add_conditional_edges("agent", should_continue)

# After executing the tool, go back to the "agent".
workflow.add_edge("action", "agent")

# After receiving human input, also return to the "agent".
workflow.add_edge("ask_human", "agent")
```

- **`StateGraph(MessagesState)`**: Initializes our workflow graph with a state that holds messages.
- **Nodes:**
  - **`"agent"`**: Calls the model.
  - **`"action"`**: Handles tool calls (like our search tool).
  - **`"ask_human"`**: Pauses and collects human input.
- **Edges:**
  - **`START` to `"agent"`**: Begins the workflow.
  - **Conditional edges from `"agent"`**: Uses `should_continue` to decide which node to jump to next.
  - **Returning edges:** After tool execution or human input, the workflow goes back to the agent.

---

## üíæ Step 5: Setting Up Memory and Compiling the Workflow

### 5.1 Memory Checkpoint

We use a memory checkpoint to save the state between steps.

```python
from langgraph.checkpoint.memory import MemorySaver
memory = MemorySaver()
```

- **`MemorySaver`**: Saves and restores the state of our graph as it runs.

### 5.2 Compile the Graph

Finally, we compile our graph into a runnable application.

```python
app = workflow.compile(checkpointer=memory)
display(Image(app.get_graph().draw_mermaid_png()))
```

- **`workflow.compile(checkpointer=memory)`**: Finalizes the workflow with the memory checkpoint.
- **`draw_mermaid_png()`**: Visualizes the workflow as a diagram (using Mermaid) so you can see the structure.

---

## üåç Real-World Application: An Intelligent Travel Assistant

Imagine you're building a travel assistant chatbot. Here‚Äôs how the agent could work:

1. **Initial Interaction:**  
   The agent (powered by a language model) asks, "Where would you like to travel?"
2. **Tool Action:**  
   If the user provides a vague answer, the agent might call the **search tool** to look up popular travel destinations.
3. **Clarification:**  
   If the search tool indicates ambiguity (e.g., multiple cities with similar names), the agent uses the **ask_human** node to ask, "Please provide your location," ensuring accurate results.
4. **Final Response:**  
   The agent resumes with the clarified information and provides tailored travel options.

This design makes the system robust, interactive, and responsive to real-time user input.

---

## üéØ Summary

- **Agent & Tools Integration:**  
  - Use Anthropic‚Äôs chat model to generate responses.
  - Bind real tools (like a search function) and mock tools (for human input) to the model.
- **Workflow Logic:**  
  - **Nodes:** Define what happens at each step.
  - **Conditional Edges:** Decide the next step based on the conversation state.
  - **Interrupts:** Pause the agent to get human feedback when needed.
- **Real-World Use:**  
  A travel assistant or customer support chatbot that clarifies user queries before providing answers.

By following these steps, you can create an interactive agent that not only acts based on automated logic but also gracefully involves human feedback when necessary. This makes your application more reliable and user-friendly.

---

# ü§ñ Interacting with the Agent

In this section, we'll learn how to interact with our agent. The agent is designed to ask a user for their location and then look up the weather for that location. We'll walk through the code step by step, explain each line, and illustrate how the agent uses human feedback to progress through its workflow.

---

## üîë Key Concepts

- **Agent Interaction:** The agent receives instructions (messages) from the user, processes them using its built-in tools, and may interrupt execution to ask for clarification.
- **Interrupts:** When the agent needs additional information (like a location), it pauses execution. This is handled by the `ask_human` node in our workflow.
- **Resuming Execution:** Once the required human input is provided, the agent resumes its execution, incorporating the new information into its next steps.
- **Thread Configuration:** The configuration (using a thread ID) helps track separate interaction sessions.

---

## üöÄ Code Walkthrough

Below, you'll find the code that interacts with the agent. We will explain each part in detail.

### 1. Sending the Initial Request

```python
config = {"configurable": {"thread_id": "2"}}
for event in app.stream(
    {
        "messages": [
            (
                "user",
                "Use the search tool to ask the user where they are, then look up the weather there",
            )
        ]
    },
    config,
    stream_mode="values",
):
    event["messages"][-1].pretty_print()
```

#### Line-by-Line Explanation:

- **`config = {"configurable": {"thread_id": "2"}}`**  
  *Purpose:*  
  This line sets up a configuration dictionary for our interaction.  
  *Explanation:*  
  - The `"thread_id": "2"` uniquely identifies this interaction session.  
  - This is useful for managing multiple sessions concurrently.

- **`for event in app.stream(...):`**  
  *Purpose:*  
  This loop starts streaming the agent's response.  
  *Explanation:*  
  - `app.stream(...)` sends the initial message to the agent and listens for response events.
  - The `stream_mode="values"` parameter indicates that we want the actual values from each event (i.e., messages).

- **Inside `app.stream`:**  
  ```python
  {
      "messages": [
          (
              "user",
              "Use the search tool to ask the user where they are, then look up the weather there",
          )
      ]
  }
  ```  
  *Purpose:*  
  This dictionary represents the initial state, containing a single message from the user.  
  *Explanation:*  
  - The message tuple has two parts: the sender ("user") and the message text.
  - The message instructs the agent to first ask for the user's location using the search tool, then use that information to look up the weather.

- **`event["messages"][-1].pretty_print()`**  
  *Purpose:*  
  This line prints the latest message in a nicely formatted manner.  
  *Explanation:*  
  - `event["messages"]` retrieves the list of messages generated by the agent.
  - `[-1]` accesses the most recent message.
  - `.pretty_print()` formats and displays the message, making it easier to read.

#### What Happens Here:

- The agent processes the initial user message.
- It decides to use the `AskHuman` tool to request the user's location.
- The output shows a human message (your instruction) and then an agent message indicating that it will ask for location input.

---

### 2. Resuming the Interaction with User Input

After the agent pauses and waits for the location input, we provide that information to resume the process.

```python
for event in app.stream(Command(resume="san francisco"), config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

#### Line-by-Line Explanation:

- **`Command(resume="san francisco")`**  
  *Purpose:*  
  This creates a command to resume the agent's execution, providing the missing information.  
  *Explanation:*  
  - The `"resume"` parameter carries the value `"san francisco"`, which is the location input from the user.
  - This command signals the agent that the interruption can be resolved, and the workflow should now continue with this input.

- **`for event in app.stream(...):`**  
  *Purpose:*  
  This loop sends the resume command to the agent and listens for subsequent events.  
  *Explanation:*  
  - It works similarly to the previous loop but now with a command that includes the user's location.
  - The agent processes this command, replacing the interruption with the provided value.

- **`event["messages"][-1].pretty_print()`**  
  *Purpose:*  
  As before, this line prints the latest message from the agent.  
  *Explanation:*  
  - This allows us to see how the agent‚Äôs response evolves after receiving the location input.
  - The output now includes a tool call for the search tool to look up the weather in San Francisco, and later, the actual weather results.

#### What Happens Here:

- The agent resumes its workflow, now equipped with the user-provided location.
- It uses the `search` tool to fetch the weather information for San Francisco.
- The process results in multiple messages:
  - An agent message confirming the action.
  - A tool call message showing the use of the search tool.
  - A tool message displaying the simulated weather lookup result.
  - A final agent message summarizing the weather details.

---

## üåç Real-World Example: Weather Chatbot

Imagine a weather chatbot integrated into a smart assistant application. Here‚Äôs how this interaction can be useful in a real-world scenario:

1. **User Initiates Query:**  
   The user asks, "What's the weather like today?"  
   - The chatbot uses natural language processing to understand the query.

2. **Clarification Request:**  
   If the user's location is not provided, the agent interrupts its flow and asks, "Where are you located?"  
   - This ensures the response is personalized and accurate.

3. **Resuming Interaction:**  
   Once the user replies with their location (e.g., "San Francisco"), the agent resumes and fetches the current weather data for that location.  
   - The system might use a real weather API instead of a mock tool to provide live data.

4. **Providing the Answer:**  
   The chatbot then returns a detailed weather update, enhancing the user experience with timely and relevant information.

This process not only makes the chatbot interactive and user-friendly but also ensures the data it provides is tailored to the user‚Äôs context.

---

## üéØ Summary

- **Interactive Flow:**  
  The agent initiates an interaction, pauses for human input when needed, and resumes with that input.
  
- **Tool Integration:**  
  It integrates multiple tools (like `AskHuman` and `search`) to manage the workflow intelligently.

- **Real-World Application:**  
  This approach is perfect for chatbots, virtual assistants, or any application where user context is crucial‚Äîlike weather apps, travel assistants, or customer support systems.

- **Step-by-Step Process:**  
  1. **Send the Initial Message:**  
     The user instructs the agent on what to do.
  2. **Pause and Request Input:**  
     The agent uses an interrupt to ask for the missing information (e.g., location).
  3. **Resume with Input:**  
     The agent resumes using the provided input and completes the task.
  4. **Display Results:**  
     The final output is a combination of messages from the agent and tool responses.

By following these steps, you can create an interactive agent that smartly incorporates human feedback, making your applications more robust and user-centric.

---

# Interacting with the Agent ü§ñ

In this section, we'll walk through the process of interacting with the agent, asking it to ask the user where they are, and then looking up the weather there. This will demonstrate how the agent uses both a custom tool (AskHuman) and a standard tool (search).

## **1. Setting Up the Configuration üîß**

Before we start interacting with the agent, we need to set up the necessary configuration to manage the state and identify the thread for the interaction.

### Code Example:
```python
config = {"configurable": {"thread_id": "2"}}
```

**Explanation:**
- `config` is a dictionary where we define configurations for the agent‚Äôs behavior.
- Here, we specify the thread ID as `2`, which helps the agent know which interaction or conversation it is handling.
- This allows the agent to maintain the state and history of the conversation.

---

## **2. Initiating the Interaction üßë‚Äçüíª**

We want to ask the user where they are located and then use the agent‚Äôs tools to look up the weather for that location.

### Code Example:
```python
for event in app.stream(
    {
        "messages": [
            (
                "user",
                "Use the search tool to ask the user where they are, then look up the weather there",
            )
        ]
    },
    config,
    stream_mode="values",
):
    event["messages"][-1].pretty_print()
```

**Explanation:**
- `app.stream` starts the interaction by streaming messages to the agent.
- We provide the agent with a user message, asking it to ask the user‚Äôs location and then search for the weather.
- `stream_mode="values"` makes the agent continuously output values.
- We print out the last message from the stream using `.pretty_print()` to make it more readable.

**Real-world example:**  
Imagine a chatbot integrated into a weather app that helps users find the weather by first asking their location and then fetching real-time weather data.

---

## **3. Understanding the Tool Call üî®**

At this point, the agent will call the `AskHuman` tool to ask the user where they are located. This is a custom tool that collects input from the user.

### Code Example:
```python
[{'text': "I'll help you with that. Let me first ask the user about their location.", 'type': 'text'}, {'id': 'toolu_01KNvb7RCVu8yKYUuQQSKN1x', 'input': {'question': 'Where are you located?'}, 'name': 'AskHuman', 'type': 'tool_use'}]
```

**Explanation:**
- The agent sends a message that says, "I'll help you with that. Let me first ask the user about their location."
- The `toolu_01KNvb7RCVu8yKYUuQQSKN1x` ID represents the specific call to the `AskHuman` tool, where the agent asks, ‚ÄúWhere are you located?‚Äù

**Real-world example:**  
This step could be used in a customer service chatbot, where the bot needs to ask a user for their location before processing a request (e.g., for local promotions or recommendations).

---

## **4. Providing the User's Location üåç**

Once the agent asks for the user's location, the system needs to provide a response. In this example, the user‚Äôs location is "San Francisco."

### Code Example:
```python
for event in app.stream(Command(resume="san francisco"), config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

**Explanation:**
- `Command(resume="san francisco")` is used to simulate user input. Here, the user responds with "San Francisco."
- The agent resumes the conversation with the location provided by the user.

**Real-world example:**  
In a virtual assistant, you might interact with a smart speaker, and after asking for your location, it provides you with nearby events or services (e.g., restaurants or weather updates).

---

## **5. Using the Search Tool üîç**

After the agent has obtained the user's location, it uses the search tool to look up the weather in that area.

### Code Example:
```python
[{'text': "Now I'll search for the weather in San Francisco.", 'type': 'text'}, {'id': 'toolu_01Y5C4rU9WcxBqFLYSMGjV1F', 'input': {'query': 'current weather in san francisco'}, 'name': 'search', 'type': 'tool_use'}]
```

**Explanation:**
- The agent states that it will now search for the weather in San Francisco.
- The `search` tool is called with the query `'current weather in san francisco'`, asking the system to find the current weather information.

**Real-world example:**  
A real-time application such as a travel assistant app can use this logic to find local weather for a specific city after the user provides their location.

---

## **6. Getting the Search Result üåû**

After using the search tool, the agent receives a weather report. It could be a simple response like: "It‚Äôs sunny in San Francisco."

### Code Example:
```python
I looked up: current weather in san francisco. Result: It's sunny in San Francisco, but you better look out if you're a Gemini üòà.
```

**Explanation:**
- The search tool has provided the agent with a result about the current weather in San Francisco.
- The response contains the weather information along with some playful content like "look out if you're a Gemini."

**Real-world example:**  
This is similar to how weather apps give you updates on the weather and might add fun facts or details (e.g., horoscopes or special conditions).

---

## **7. Final Response to the User üé§**

Finally, the agent wraps up the conversation and provides a summary of the weather for San Francisco.

### Code Example:
```python
[{'text': "Based on the search results, it's currently sunny in San Francisco. Note that this is the current weather at the time of our conversation, and conditions can change throughout the day.", 'type': 'text'}]
```

**Explanation:**
- The agent sends a message summarizing the current weather based on the search result.
- It reminds the user that weather conditions can change throughout the day, ensuring the user is aware of the dynamic nature of weather.

**Real-world example:**  
In customer service bots, this behavior is common for providing users with real-time information, such as service availability or order status, along with a disclaimer that updates are based on real-time data.

---

### **Final Thoughts üí≠**
In this scenario, we demonstrated how an agent can interact with a user to collect information, use tools to fetch external data (such as weather), and continue the conversation based on the user‚Äôs responses. This type of agent can be used in various real-world applications like customer support, travel assistants, or virtual assistants.

The code provided is a simple example of a system that handles user inputs, uses multiple tools, and maintains conversation states. By utilizing the concept of tools and conditional logic, agents can provide powerful, dynamic interactions.