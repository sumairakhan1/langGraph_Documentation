# 7. How to review tool calls (Functional API)

# ğŸ› ï¸ How to Review Tool Calls in a ReAct Agent (Functional API)

In this guide, we will **implement human-in-the-loop workflows** in a **ReAct agent** using the **LangGraph Functional API**. The goal is to **review tool calls before execution** to ensure accuracy and correctness.

We will cover:

- **What a ReAct agent is** ğŸ§   
- **How tool calls work** ğŸ”§  
- **How to implement human review of tool calls** ğŸ‘¤  
- **Code examples with detailed explanations** ğŸ’»  

---

## ğŸ“Œ What is a ReAct Agent?

A **ReAct (Reasoning + Acting) Agent** is an AI agent that:

1. **Receives a message** (e.g., "Whatâ€™s the weather in San Francisco?").
2. **Decides if it needs external tools** (e.g., a weather API).
3. **Calls the necessary tools** (e.g., `get_weather(location)`).
4. **Processes the toolâ€™s response** and generates a final output.

ğŸ’¡ **Real-World Example:**  
Imagine a **customer support chatbot** that helps users **book flights** or **check weather updates**. The bot might need to **ask for human confirmation** before **booking a flight** to avoid mistakes.

---

## ğŸ” Why Review Tool Calls?

Before executing a tool call, we might want to **review it** to ensure:

âœ… The tool call is **correct**.  
âœ… No **wrong inputs** are passed.  
âœ… The **user is aware** before performing an action.  

We achieve this by **interrupting execution** to get human confirmation.  

---

## ğŸ—ï¸ Setting Up the Environment

Before we start coding, let's **install required packages**:

```python
!pip install -U langgraph langchain-openai
```

Then, set up your **API key**:

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")  # Set OpenAI API key
```

This ensures we have access to **GPT-based models** for processing.

---

## ğŸ—ï¸ Defining the AI Model and Tools

We use **OpenAI's GPT-based model** and define a **weather tool**.

```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# Define AI Model
model = ChatOpenAI(model="gpt-4o-mini")

# Define a tool that fetches weather details
@tool
def get_weather(location: str):
    """Fetches weather information for a given location."""
    if any(city in location.lower() for city in ["sf", "san francisco"]):
        return "It's sunny!"
    elif "boston" in location.lower():
        return "It's rainy!"
    else:
        return f"I am not sure what the weather is in {location}"

# List of tools available
tools = [get_weather]
```

### ğŸ” Explanation:

- **We define an AI model (`ChatOpenAI`)** to process text messages.
- **We define a `get_weather(location)` tool** that:
  - Returns `"It's sunny!"` for San Francisco.
  - Returns `"It's rainy!"` for Boston.
  - Returns `"I am not sure"` for unknown locations.
- **Tools are stored in a list** (`tools`), so the agent can access them.

---

## âš¡ Implementing Tool Call Review

We need to **review tool calls before execution** using the `interrupt` function.

```python
from typing import Union
from langchain_core.messages import ToolCall, ToolMessage

def review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:
    """Review a tool call before executing it."""
    
    # Ask for human review
    human_review = interrupt(
        {
            "question": "Is this correct?",
            "tool_call": tool_call,
        }
    )

    review_action = human_review["action"]
    review_data = human_review.get("data")

    if review_action == "continue":
        return tool_call  # Proceed with execution
    elif review_action == "update":
        updated_tool_call = {**tool_call, **{"args": review_data}}
        return updated_tool_call  # Execute with modified data
    elif review_action == "feedback":
        return ToolMessage(
            content=review_data, 
            name=tool_call["name"], 
            tool_call_id=tool_call["id"]
        )  # Send feedback instead of executing
```

### ğŸ” Explanation:

- **Interrupt execution** and ask a human reviewer if the tool call is correct.
- The reviewer can:
  1. **Continue (`"continue"`)** â†’ Execute as is.
  2. **Update (`"update"`)** â†’ Modify the toolâ€™s input before execution.
  3. **Give feedback (`"feedback"`)** â†’ Provide a message instead of executing.

ğŸ’¡ **Real-World Example:**  
If a **financial chatbot** tries to **transfer money**, it might **ask for human confirmation** before proceeding.

---

## ğŸ”„ Defining Tasks for the Agent

### **1ï¸âƒ£ Call the AI Model**
```python
from langgraph.func import task

@task
def call_model(messages):
    """Call the AI model with a sequence of messages."""
    response = model.bind_tools(tools).invoke(messages)
    return response
```
**ğŸ” Explanation:**
- The AI model is **queried with messages**.
- It **binds to tools** (like `get_weather`) and **decides if a tool is needed**.

---

### **2ï¸âƒ£ Call the Tool (After Review)**
```python
@task
def call_tool(tool_call):
    """Execute the tool call after reviewing it."""
    tool = tools_by_name[tool_call["name"]]
    observation = tool.invoke(tool_call["args"])
    return ToolMessage(content=observation, tool_call_id=tool_call["id"])
```
**ğŸ” Explanation:**
- The tool call **executes only if it is validated**.
- The **toolâ€™s output is returned** in a structured format (`ToolMessage`).

---

## ğŸ¯ Full Workflow Example

### **User Query â†’ AI Model â†’ Tool Review â†’ Execution**
```python
user_message = {
    "role": "user",
    "content": "What's the weather like in San Francisco?"
}

# Process the query
for step in call_model([user_message]):
    print(step)

# Example tool call generated
tool_call = {
    "name": "get_weather",
    "args": {"location": "San Francisco"},
    "id": "tool_12345"
}

# Review the tool call
reviewed_tool_call = review_tool_call(tool_call)

# Execute tool call if valid
if isinstance(reviewed_tool_call, ToolCall):
    response = call_tool(reviewed_tool_call)
    print(response)
```

---

## ğŸš€ Summary

âœ… **A ReAct agent** processes messages and calls external tools.  
âœ… **Tool calls are reviewed before execution** to prevent errors.  
âœ… **The review function allows humans to approve, modify, or reject tool calls.**  
âœ… **LangGraph Functional API** makes it easy to implement.  

### ğŸ”¥ **Real-World Use Cases**
1. **Medical Chatbots** ğŸ¥ â€“ Verify symptoms before suggesting a treatment.
2. **Financial Assistants** ğŸ’° â€“ Confirm transactions before transferring funds.
3. **Customer Support Bots** ğŸ“ â€“ Ensure correct actions before modifying accounts.

---

## ğŸ¯ Next Steps

- **Explore LangGraphâ€™s API** ğŸ“š  
- **Try implementing different tools** (e.g., booking appointments).  
- **Enhance the agent by integrating human reviews at multiple steps.**  

Would you like a **step-by-step video tutorial** on this? ğŸš€ Let me know! ğŸ˜Š

---

# ğŸ“Œ **Reviewing Tool Calls in LangGraph Functional API (Beginner-Friendly Guide)**  

When building AI-driven applications, we often rely on external tools to fetch data or perform certain actions. However, before executing a tool call, we might want to **review and validate it**â€”especially in **critical applications** like finance, healthcare, or customer service.  

This guide will help you understand **how to review tool calls before execution** using the LangGraph Functional API in a **step-by-step, beginner-friendly way**.  

---

## ğŸ¯ **What You Will Learn**
- âœ… **Why reviewing tool calls is important**
- âœ… **How to implement a review process using the `interrupt` function**
- âœ… **How to execute tool calls only after human approval**
- âœ… **Real-world use cases where this approach is beneficial**
- âœ… **Step-by-step code explanation with examples**

---

# ğŸš€ **1. Why Review Tool Calls?**
When working with AI agents, models may generate **incorrect or inappropriate tool calls**. To prevent issues, we introduce a **human review step** before execution. This allows us to:  

âœ” **Approve valid tool calls**  
âœ” **Modify tool calls if they contain errors**  
âœ” **Reject invalid tool calls and give feedback to the model**  

ğŸ’¡ **Example Scenario:**  
Imagine you have an AI-powered **customer support chatbot** that retrieves customer data from a CRM system. Before the bot fetches private user details, you might want a human agent to **approve** the request for security reasons.

---

# âš™ï¸ **2. Implementing the Review Process**
### **Step 1: Define the `review_tool_call` Function**
We create a function that interrupts execution to ask for human approval before running the tool call.  

### ğŸ“Œ **Code Explanation**
```python
from typing import Union
from langgraph.types import interrupt
from langchain_core.messages import ToolCall, ToolMessage

def review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:
    """Review a tool call, returning a validated version."""

    # ğŸ›‘ Interrupt execution and ask for human review
    human_review = interrupt(
        {
            "question": "Is this correct?",  # Display a question to the reviewer
            "tool_call": tool_call,         # Show the tool call details
        }
    )

    # âœ… Extract the human decision
    review_action = human_review["action"]
    review_data = human_review.get("data")

    # ğŸŸ¢ If the human reviewer approves, proceed with the original tool call
    if review_action == "continue":
        return tool_call

    # âœ If the human reviewer updates the tool call, return the modified version
    elif review_action == "update":
        updated_tool_call = {**tool_call, **{"args": review_data}}
        return updated_tool_call

    # ğŸ”„ If the human reviewer provides feedback, return a ToolMessage instead
    elif review_action == "feedback":
        return ToolMessage(
            content=review_data, name=tool_call["name"], tool_call_id=tool_call["id"]
        )
```

### ğŸ“Œ **How It Works**
1. **Interrupts execution** and asks a human reviewer: *"Is this correct?"*
2. Captures the **reviewer's action** (`continue`, `update`, or `feedback`).
3. If the reviewer **approves**, the tool call is executed.
4. If the reviewer **modifies** the tool call, we use the updated version.
5. If the reviewer **rejects** it and provides feedback, we send a message back to the AI model.

---

# ğŸš¦ **3. Implementing the Entry Point**
Now, we modify our **AI agentâ€™s entrypoint** to include the tool review process.

### ğŸ“Œ **Code Explanation**
```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.message import add_messages
from langgraph.types import Command, interrupt
from langgraph.func import entrypoint

checkpointer = MemorySaver()  # Stores previous messages

@entrypoint(checkpointer=checkpointer)
def agent(messages, previous):
    # ğŸ”„ Preserve previous messages in the conversation
    if previous is not None:
        messages = add_messages(previous, messages)

    # ğŸ“ Call the AI model to get a response
    llm_response = call_model(messages).result()

    while True:
        # âŒ If no tool calls were generated, exit loop
        if not llm_response.tool_calls:
            break

        # ğŸ” Review tool calls before execution
        tool_results = []
        tool_calls = []
        for i, tool_call in enumerate(llm_response.tool_calls):
            review = review_tool_call(tool_call)  # ğŸ›‘ Human validation step
            if isinstance(review, ToolMessage):
                tool_results.append(review)  # Feedback is treated as a message
            else:
                tool_calls.append(review)  # Store valid tool calls
                if review != tool_call:
                    llm_response.tool_calls[i] = review  # Update if modified

        # ğŸ”§ Execute validated tool calls
        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]
        remaining_tool_results = [fut.result() for fut in tool_result_futures]

        # ğŸ“ Add results to the message history
        messages = add_messages(
            messages,
            [llm_response, *tool_results, *remaining_tool_results],
        )

        # ğŸ“ Call the model again with updated messages
        llm_response = call_model(messages).result()

    # ğŸ¯ Return final response with all processed messages
    messages = add_messages(messages, llm_response)
    return entrypoint.final(value=llm_response, save=messages)
```

---

# ğŸ† **4. How Does This Work?**
1. **Keeps track of previous messages** to maintain conversation history.
2. **Calls the AI model** to generate a response.
3. **Checks if the response includes tool calls**:
   - If **no tool calls**, return the result.
   - If **tool calls exist**, send them for human review.
4. **Reviews each tool call** before execution:
   - If approved, execute it.
   - If modified, update it before execution.
   - If rejected, send feedback instead of running it.
5. **Executes validated tool calls** and appends results to the conversation.
6. **Calls the model again** if needed (e.g., if a revised tool call is needed).
7. **Returns the final response** after all reviews and tool executions.

---

# ğŸŒ **5. Real-World Use Cases**
### âœ… **1. AI-Powered Chatbots (Customer Support)**
- Before a chatbot fetches **sensitive user data**, a human reviewer can approve the request.

### âœ… **2. Financial Transactions (Banking AI)**
- Before processing **large transactions**, the AI asks a human for approval.

### âœ… **3. Medical AI Assistants**
- When an AI suggests a **prescription**, a doctor can review before confirming.

### âœ… **4. Legal Document Automation**
- Before an AI generates a **legal contract**, a lawyer can approve modifications.

---

# ğŸ **6. Summary**
âœ… **Reviewed tool calls** allow humans to validate AI decisions before execution.  
âœ… **Interrupt function** pauses execution until a reviewer approves or modifies the request.  
âœ… **AI agents can now interact with tools safely**, preventing incorrect actions.  
âœ… **Real-world applications include chatbots, finance, healthcare, and legal AI systems.**

By implementing a **human-in-the-loop workflow**, we can **increase reliability and trust** in AI-powered applications. ğŸš€

---

ğŸ”¹ **Whatâ€™s Next?** Want to extend this? You can:
- **Log reviewer decisions** for analytics.
- **Auto-approve low-risk actions** and only review high-risk ones.
- **Integrate with databases** to fetch real-time approval rules.

---

ğŸ”¥ **Did this guide help?** Let me know if you have any questions or need more examples! ğŸ˜ŠğŸš€

---
# ğŸŒŸ Understanding Entrypoints and Tool Call Review in AI Agents

In this guide, we'll dive deep into **entrypoints** and how we can **review tool calls** before executing them in an AI agent system. We'll break it down step by step in a way that is **easy for beginners to understand**, using **real-world examples, code snippets, and detailed explanations**.

---

## ğŸ“Œ What is an Entrypoint?

An **entrypoint** is the starting function of an AI agent where the **main logic of execution begins**. Think of it as the **main function** in a programâ€”it controls how tasks are processed and how the agent interacts with tools and human reviewers.

ğŸ’¡ **Real-world analogy**:  
Imagine a **customer service chatbot** that answers queries like *"Whatâ€™s the weather in San Francisco?"* The chatbot needs to call a **weather API (a tool)**. However, before sending the request, a human might **review** the tool call to ensure correctness. This process is managed using an **entrypoint**.

---

## ğŸ› ï¸ Why Do We Need Tool Call Review?

Tool call review is essential when an AI model interacts with **external tools** (like APIs or databases). Before executing a tool call, we can:

âœ” **Accept** the tool call if it's correct.  
âœ **Revise** the tool call if it needs modifications.  
ğŸ’¬ **Generate a custom response** (e.g., reformatting input before sending).  

This ensures accuracy and prevents **incorrect or harmful** tool executions.

---

## ğŸ” Code Implementation: Reviewing Tool Calls

We define a function to review the tool calls before execution.  

### âœ… Step 1: Define the Review Function

```python
from typing import Union

def review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:
    """Review a tool call, returning a validated version."""
    
    # Interrupt execution for human review
    human_review = interrupt(
        {
            "question": "Is this correct?",
            "tool_call": tool_call,
        }
    )

    # Extract the review action
    review_action = human_review["action"]
    review_data = human_review.get("data")

    # Accept the tool call if it's correct
    if review_action == "continue":
        return tool_call  

    # Modify tool call based on human input
    elif review_action == "update":
        updated_tool_call = {**tool_call, **{"args": review_data}}
        return updated_tool_call  

    # Generate a custom response
    elif review_action == "feedback":
        return ToolMessage(
            content=review_data, name=tool_call["name"], tool_call_id=tool_call["id"]
        )
```

### ğŸ” Code Explanation

| Line | Explanation |
|------|------------|
| `from typing import Union` | Imports the `Union` type, allowing the function to return either `ToolCall` or `ToolMessage`. |
| `def review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:` | Defines a function to **review a tool call**. |
| `human_review = interrupt(...)` | Pauses execution and asks for **human review** before proceeding. |
| `review_action = human_review["action"]` | Retrieves the **review decision** (continue, update, or feedback). |
| `if review_action == "continue": return tool_call` | If the human approves, we **proceed** with execution. |
| `elif review_action == "update": ...` | If changes are needed, we **update** the tool call. |
| `elif review_action == "feedback": ...` | If feedback is required, we **generate a message** for the tool. |

---

## ğŸš€ Step 2: Define the Entrypoint for the AI Agent

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.message import add_messages
from langgraph.types import Command, interrupt

# Save execution state
checkpointer = MemorySaver()

@entrypoint(checkpointer=checkpointer)
def agent(messages, previous):
    if previous is not None:
        messages = add_messages(previous, messages)

    llm_response = call_model(messages).result()

    while True:
        if not llm_response.tool_calls:
            break  # No tool calls, exit loop

        tool_results = []
        tool_calls = []

        for i, tool_call in enumerate(llm_response.tool_calls):
            review = review_tool_call(tool_call)
            if isinstance(review, ToolMessage):
                tool_results.append(review)  # Custom message
            else:
                tool_calls.append(review)  # Validated tool call
                if review != tool_call:
                    llm_response.tool_calls[i] = review  # Update message

        # Execute approved tool calls
        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]
        remaining_tool_results = [fut.result() for fut in tool_result_futures]

        # Append results to message history
        messages = add_messages(messages, [llm_response, *tool_results, *remaining_tool_results])

        # Call model again for next step
        llm_response = call_model(messages).result()

    return entrypoint.final(value=llm_response, save=messages)
```

### ğŸ“– Code Explanation

| Line | Explanation |
|------|------------|
| `checkpointer = MemorySaver()` | Saves execution history so that previous results are not lost. |
| `@entrypoint(checkpointer=checkpointer)` | Marks the function as the **entrypoint** of the AI agent. |
| `def agent(messages, previous):` | Defines the main function for handling agent logic. |
| `llm_response = call_model(messages).result()` | Calls the AI model to generate a response. |
| `for tool_call in llm_response.tool_calls:` | Loops through all tool calls in the response. |
| `review = review_tool_call(tool_call)` | Calls `review_tool_call()` to check if the tool call is valid. |
| `tool_results.append(review)` | Stores tool call results. |
| `call_tool(tool_call)` | Executes validated tool calls. |
| `messages = add_messages(messages, [...])` | Updates the conversation history. |
| `return entrypoint.final(...)` | Returns the **final AI response**. |

---

## ğŸ“Œ Usage Examples

### âœ… Accepting a Tool Call

```python
config = {"configurable": {"thread_id": "1"}}

user_message = {"role": "user", "content": "What's the weather in San Francisco?"}
print(user_message)

for step in agent.stream([user_message], config):
    _print_step(step)
```

âœ” **AI identifies a tool call** for `get_weather("San Francisco")`.  
âœ” **Human approves it** (`resume={"action": "continue"}`).  
âœ” **AI fetches weather info** and responds:  
   ğŸ—¨ï¸ *"The weather in San Francisco is sunny!"*

---

### âœ Revising a Tool Call

```python
human_input = Command(resume={"action": "update", "data": {"location": "SF, CA"}})

for step in agent.stream(human_input, config):
    _print_step(step)
```

âœ” **Human updates tool call** to use `"SF, CA"` instead of `"San Francisco"`.  
âœ” **AI executes updated request**.  

---

### ğŸ’¬ Generating a Custom Message

```python
human_input = Command(
    resume={
        "action": "feedback",
        "data": "Please format as <City>, <State>.",
    },
)
```

âœ” **Human asks AI to reformat** the tool call.  
âœ” AI changes `"San Francisco"` â†’ `"San Francisco, CA"`.  

---

## ğŸ¯ Key Takeaways

ğŸ”¹ **Entrypoints** define where execution begins in an AI agent.  
ğŸ”¹ **Tool calls** are reviewed before execution for correctness.  
ğŸ”¹ We can **accept, revise, or modify** tool calls before execution.  
ğŸ”¹ **Real-world use case:** Chatbots, AI assistants, and automated workflows.  

---

## ğŸ“ Final Thoughts

Understanding **entrypoints and tool call review** is crucial when working with **AI agents** that interact with external tools. By reviewing tool calls, we ensure **accuracy, security, and better user experience**. ğŸš€