# 6. How to wait for user input (Functional API)

# â³ How to Wait for User Input (Functional API)

## ğŸ“Œ Introduction
In **agentic systems**, **Human-in-the-loop (HIL) interactions** allow an AI agent to **pause execution**, ask a user for additional input, and then continue based on the user's response. This is useful when the AI needs **clarifications, approvals, or additional data** before proceeding.  

In LangGraph, we use the **`interrupt()`** function to **pause execution**, collect user input, and then resume execution with the input provided.  

## ğŸ¯ Real-World Example
Imagine you're developing a **customer support chatbot** that processes refund requests.  
- The bot collects initial information (order number, issue description).  
- Before proceeding with the refund, it **pauses** to ask a human agent for approval.  
- Once approved, it continues to process the refund.  

This kind of **interactive workflow** can be implemented using LangGraphâ€™s Functional API.  

---

## ğŸ› ï¸ **Step 1: Setup**
First, we need to install and import the necessary packages.  

### ğŸ“Œ Install Required Libraries
```python
%%capture --no-stderr
%pip install -U langgraph langchain-openai
```

### ğŸ“Œ Set API Keys for OpenAI
We use `getpass` to securely input API keys without exposing them in the code.  
```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):  # Only set if not already defined
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```
> ğŸ”¹ **Why do we use `os.environ`?**  
> This ensures the API key is set in the environment and can be accessed securely throughout our code.  

---

## ğŸ—ï¸ **Step 2: Creating Tasks with Interrupts**
We'll now define a **simple workflow** consisting of three steps:
1. **Append `"bar"`** to the input.
2. **Pause execution** for user feedback.  
3. **Append `"qux"`** after resuming.

### ğŸ“Œ Defining the Tasks
```python
from langgraph.func import entrypoint, task
from langgraph.types import Command, interrupt

@task
def step_1(input_query):
    """Append 'bar' to the input."""
    return f"{input_query} bar"

@task
def human_feedback(input_query):
    """Pause execution and wait for human input."""
    feedback = interrupt(f"Please provide feedback: {input_query}")
    return f"{input_query} {feedback}"

@task
def step_3(input_query):
    """Append 'qux' to the input."""
    return f"{input_query} qux"
```

### ğŸ” **Explanation of Code**
1. **`@task` decorator**  
   - Used to define a **step** in the LangGraph workflow.
2. **`step_1(input_query)`**  
   - Takes an input and appends `"bar"`.  
3. **`human_feedback(input_query)`**  
   - Calls `interrupt()`, which **pauses execution** and asks the user for input.  
   - Once input is received, it appends it to the query.  
4. **`step_3(input_query)`**  
   - Takes the modified input and appends `"qux"` at the end.  

---

## ğŸ¯ **Step 3: Composing the Graph**
Now, let's **connect these tasks** into a workflow.

### ğŸ“Œ Define a Memory Checkpoint
A **checkpoint** stores the progress of the workflow, so tasks do not need to restart after an interruption.
```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
```

### ğŸ“Œ Define the Workflow
```python
@entrypoint(checkpointer=checkpointer)
def graph(input_query):
    result_1 = step_1(input_query).result()
    result_2 = human_feedback(result_1).result()  # Waits for user input
    result_3 = step_3(result_2).result()

    return result_3
```
### ğŸ” **Explanation**
1. **`@entrypoint(checkpointer=checkpointer)`**  
   - This **defines the entry point** for executing the graph.
2. **Execution Steps**  
   - `step_1()` is called first.
   - `human_feedback()` is called **and waits for user input** (interrupt).  
   - Once the user provides input, `step_3()` runs.  

---

## ğŸš€ **Step 4: Running the Graph**
### ğŸ“Œ Provide an Initial Input
```python
config = {"configurable": {"thread_id": "1"}}

for event in graph.stream("foo", config):
    print(event)
    print("\n")
```

### ğŸ“Œ Expected Output (Before User Input)
```
{'step_1': 'foo bar'}

{'__interrupt__': (Interrupt(value='Please provide feedback: foo bar', resumable=True, ns=['graph:d66b2e35-0ee3-d8d6-1a22-aec9d58f13b9', 'human_feedback:e0cd4ee2-b874-e1d2-8bc4-3f7ddc06bcc2'], when='during'),)}
```
> ğŸ›‘ **Notice**: The execution **pauses** at `human_feedback` and waits for user input.  

---

## â© **Step 5: Resuming Execution**
Now, we **resume execution** by providing the expected input (`"baz"`).  

```python
for event in graph.stream(Command(resume="baz"), config):
    print(event)
    print("\n")
```
### ğŸ“Œ Expected Output (After Resuming)
```
{'human_feedback': 'foo bar baz'}

{'step_3': 'foo bar baz qux'}

{'graph': 'foo bar baz qux'}
```
> ğŸ”¹ The workflow resumes from where it paused, completes `step_3()`, and returns the final result.

---

## ğŸ¯ **Key Takeaways**
âœ… **`interrupt()` function** allows stopping execution and waiting for user input.  
âœ… The **Functional API** makes it easy to compose workflows using `@task`.  
âœ… **Checkpoints (`MemorySaver`)** ensure progress is saved, preventing redundant execution.  
âœ… Real-world use cases include **chatbots, approval workflows, and dynamic decision-making systems**.

---

## ğŸ¬ **Final Thoughts**
Using **LangGraph's `interrupt()`**, you can build **interactive AI workflows** where human feedback is required. This is essential for applications like **customer support bots, legal document processing, and AI-assisted decision-making**.

ğŸš€ **Next Steps**: Try modifying the example to handle **multiple interruptions** dynamically! ğŸ’¡

---

# ğŸŒŸ **Waiting for User Input in LangGraph (Functional API)**
Human-in-the-loop (HIL) interactions are crucial for intelligent systems, especially when AI requires user clarification or assistance before proceeding. In LangGraph, we can **pause execution and wait for user input** using the `interrupt()` function.

This guide explains:
- **What "waiting for user input" means in AI workflows**
- **How to implement it using LangGraph**
- **Real-world applications**
- **Code breakdown with detailed explanations**
- **Three alternative implementations for better understanding**

---

## ğŸ§  **What is "Waiting for User Input" in AI?**
When AI agents process a task, they may **need human feedback** before continuing. For example:
- **Chatbots** ask users to confirm their requests.
- **Data annotation systems** pause until users label the data.
- **AI assistants** wait for missing information before executing actions.

### ğŸ”¥ **Real-World Example**
Imagine you have a **weather bot** that tells users the weather in any city. If the bot is unsure about a city name (e.g., "Springfield"â€”which exists in many states), it **pauses execution and asks the user for clarification** before continuing.

---

## ğŸ› ï¸ **Setting Up LangGraph for Human-in-the-Loop**
To implement this in **LangGraph**, install the necessary dependencies:

```python
!pip install -U langgraph langchain-openai
```

Then, set up your **OpenAI API key**:

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

Now, weâ€™re ready to implement user input waiting!

---

## ğŸ—ï¸ **Simple Example: Waiting for User Input**
Let's create a simple **three-step process** where:
1. The AI appends `"bar"` to the input.
2. It pauses and **waits for human feedback**.
3. After receiving input, it appends `"qux"` and completes execution.

### ğŸ“Œ **Code Implementation**
```python
from langgraph.func import entrypoint, task
from langgraph.types import Command, interrupt

# Step 1: Append "bar"
@task
def step_1(input_query):
    """Append bar to input"""
    return f"{input_query} bar"

# Step 2: Wait for human feedback
@task
def human_feedback(input_query):
    """Wait for human input and append it to the text"""
    feedback = interrupt(f"Please provide feedback: {input_query}")
    return f"{input_query} {feedback}"

# Step 3: Append "qux"
@task
def step_3(input_query):
    """Append qux to input"""
    return f"{input_query} qux"
```

### ğŸ“ **Code Breakdown**
- `step_1()` appends `"bar"` to the input.
- `human_feedback()` **pauses execution** and waits for the user to provide additional input.
- `step_3()` appends `"qux"` to the updated text.

---

## ğŸ”— **Connecting the Tasks**
Now, we connect these tasks using LangGraphâ€™s **functional API**:

```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()

@entrypoint(checkpointer=checkpointer)
def graph(input_query):
    result_1 = step_1(input_query).result()
    result_2 = human_feedback(result_1).result()
    result_3 = step_3(result_2).result()

    return result_3
```

### ğŸ” **How This Works**
- The `entrypoint()` function **executes tasks in sequence**.
- **Execution stops** at `human_feedback()` and **waits for user input**.
- After receiving input, execution **resumes** and completes.

---

## ğŸš€ **Running the Example**
We now **run the graph** and see how execution **pauses for input**:

```python
config = {"configurable": {"thread_id": "1"}}

for event in graph.stream("foo", config):
    print(event)
    print("\n")
```

### ğŸ”¹ **Expected Output**
```
{'step_1': 'foo bar'}

{'__interrupt__': (Interrupt(value='Please provide feedback: foo bar', resumable=True))}
```
ğŸ‘‰ The execution **stops** at `human_feedback()` and waits for input.

Now, we **resume execution** by providing `"baz"` as feedback:

```python
for event in graph.stream(Command(resume="baz"), config):
    print(event)
    print("\n")
```

### ğŸ”¹ **Final Output**
```
{'human_feedback': 'foo bar baz'}

{'step_3': 'foo bar baz qux'}

{'graph': 'foo bar baz qux'}
```
ğŸ‘‰ The AI **resumes execution** and appends `"qux"` to the text.

---

## ğŸ”¥ **Extending with an AI Agent**
Now, letâ€™s **extend this concept** to a **ReAct agent** that:
1. Calls an **AI model** to answer questions.
2. Uses a **weather tool** to fetch weather data.
3. **Asks a human for assistance** if unsure.

### ğŸ“Œ **Step 1: Define AI Model and Tools**
```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# AI Model
model = ChatOpenAI(model="gpt-4o-mini")

# Weather Tool
@tool
def get_weather(location: str):
    """Fetch weather for a location"""
    if any(city in location.lower() for city in ["sf", "san francisco"]):
        return "It's sunny!"
    elif "boston" in location.lower():
        return "It's rainy!"
    else:
        return f"I am not sure about the weather in {location}"
```

---

### ğŸ“Œ **Step 2: Add a Human Assistance Tool**
```python
from langgraph.types import Command, interrupt

@tool
def human_assistance(query: str) -> str:
    """Ask a human for help"""
    human_response = interrupt({"query": query})
    return human_response["data"]

tools = [get_weather, human_assistance]
```

### ğŸ” **How This Works**
- If the AI **doesnâ€™t know the answer**, it calls `human_assistance()`.
- Execution **pauses** and waits for user input.

---

### ğŸ“Œ **Step 3: Define Tasks**
```python
from langchain_core.messages import ToolMessage
from langgraph.func import entrypoint, task

tools_by_name = {tool.name: tool for tool in tools}

@task
def call_model(messages):
    """Query AI model with a list of messages"""
    response = model.bind_tools(tools).invoke(messages)
    return response

@task
def call_tool(tool_call):
    """Call a tool if the AI requests it"""
    tool = tools_by_name[tool_call["name"]]
    observation = tool.invoke(tool_call)
    return ToolMessage(content=observation, tool_call_id=tool_call["id"])
```

### ğŸ” **How This Works**
- `call_model()` **queries AI** with messages.
- `call_tool()` **executes tools** when needed.

---

## ğŸ¯ **Alternative Implementations**
### ğŸ”„ **1. Human Input with a Time Limit**
If you want the AI to **wait for a limited time** before proceeding:
```python
import time

@task
def human_feedback_with_timeout(input_query):
    """Wait for human input, but proceed after 10 seconds"""
    start_time = time.time()
    while time.time() - start_time < 10:
        feedback = interrupt(f"Please provide feedback within 10 seconds: {input_query}")
        if feedback:
            return f"{input_query} {feedback}"
    return f"{input_query} (No feedback received)"
```

---

### ğŸ’¡ **2. Automatic Default Response**
If no human input is received, use a **default response**:
```python
@task
def human_feedback_with_default(input_query):
    """Wait for input, but use default if none is given"""
    feedback = interrupt(f"Provide feedback or type 'skip': {input_query}")
    return f"{input_query} {feedback if feedback != 'skip' else 'default response'}"
```

---

### ğŸ“Œ **3. Multi-Step Human Verification**
Require **multiple approvals** before continuing:
```python
@task
def multi_step_verification(input_query):
    """Ask for multiple approvals before proceeding"""
    first_approval = interrupt(f"First approval: {input_query}")
    second_approval = interrupt(f"Second approval: {input_query} {first_approval}")
    return f"{input_query} {first_approval} {second_approval}"
```

---

## ğŸ‰ **Conclusion**
We learned how to **pause AI execution for human input** in LangGraph using `interrupt()`. This is useful for chatbots, AI assistants, and interactive workflows!

Would you like more real-world examples? ğŸš€

---

# ğŸ§  Building an AI Agent with Human Assistance in LangChain  

AI agents can perform many tasks, but sometimes they need human input. In this guide, weâ€™ll extend a **ReAct (Reasoning + Acting) AI agent** so that it can reach out to a human when needed. Weâ€™ll go step by step, explaining **each line of code in detail**, so even beginners can understand.  

---

## ğŸ“Œ **Real-World Example**  
Imagine you have an **AI customer support chatbot**. It can answer common questions like refund policies, but sometimes a customer's request is too complex. In that case, the bot should automatically **escalate the query to a human** while continuing to provide useful automated responses.  

---

# ğŸ“Œ **1. Setting Up the AI Model and Tools**  
### ğŸš€ **What Are We Doing Here?**  
We will:  
âœ… Use OpenAI's GPT-4 model as our AI agent.  
âœ… Define a **weather-checking tool** that can fetch the weather for a location.  
âœ… Define a **human assistance tool** to request help when needed.  

### ğŸ“ **Code Implementation**  

```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# Define the AI model
model = ChatOpenAI(model="gpt-4o-mini")

# Define a tool to get weather updates
@tool
def get_weather(location: str):
    """Call to get the weather from a specific location."""
    # This is a placeholder for an actual weather API call
    if any([city in location.lower() for city in ["sf", "san francisco"]]):
        return "It's sunny!"
    elif "boston" in location.lower():
        return "It's rainy!"
    else:
        return f"I am not sure what the weather is in {location}"
```

### ğŸ” **Code Explanation**  

| Line of Code | Purpose |
|-------------|---------|
| `from langchain_openai import ChatOpenAI` | Imports the OpenAI model. |
| `model = ChatOpenAI(model="gpt-4o-mini")` | Initializes GPT-4o-mini for conversation. |
| `@tool` | Declares a tool (function) that the AI can use. |
| `def get_weather(location: str):` | Defines a function that accepts a location as input. |
| `if any([city in location.lower() for city in ["sf", "san francisco"]]):` | Checks if the input contains "San Francisco". |
| `return "It's sunny!"` | Returns the weather condition if the location is recognized. |
| `else: return f"I am not sure what the weather is in {location}"` | Handles unknown locations. |

---

# ğŸ“Œ **2. Adding Human Assistance**  
### ğŸ¤” **Why Do We Need This?**  
Sometimes the AI **doesnâ€™t know the answer** or needs **human input**. We create a function that allows the AI to **pause execution** and request assistance.  

### ğŸ“ **Code Implementation**  

```python
from langgraph.types import Command, interrupt

@tool
def human_assistance(query: str) -> str:
    """Request assistance from a human."""
    human_response = interrupt({"query": query})
    return human_response["data"]

# Define the list of tools available
tools = [get_weather, human_assistance]
```

### ğŸ” **Code Explanation**  

| Line of Code | Purpose |
|-------------|---------|
| `from langgraph.types import Command, interrupt` | Imports tools for pausing execution. |
| `@tool` | Declares a tool function. |
| `def human_assistance(query: str):` | Defines a function to request human help. |
| `human_response = interrupt({"query": query})` | Interrupts execution and asks a human for input. |
| `return human_response["data"]` | Returns the human's response back to the AI. |
| `tools = [get_weather, human_assistance]` | Makes these tools available to the AI. |

---

# ğŸ“Œ **3. Defining AI Tasks**  
### ğŸ”¥ **What Are Tasks?**  
Tasks are **actions** that the AI can take:  
âœ… **Calling the model** to generate a response.  
âœ… **Executing tools** (weather check, human assistance).  

### ğŸ“ **Code Implementation**  

```python
from langchain_core.messages import ToolMessage
from langgraph.func import entrypoint, task

tools_by_name = {tool.name: tool for tool in tools}

@task
def call_model(messages):
    """Call model with a sequence of messages."""
    response = model.bind_tools(tools).invoke(messages)
    return response

@task
def call_tool(tool_call):
    tool = tools_by_name[tool_call["name"]]
    observation = tool.invoke(tool_call)
    return ToolMessage(content=observation, tool_call_id=tool_call["id"])
```

### ğŸ” **Code Explanation**  

| Line of Code | Purpose |
|-------------|---------|
| `tools_by_name = {tool.name: tool for tool in tools}` | Stores tools in a dictionary. |
| `@task` | Declares a function as a background task. |
| `def call_model(messages):` | Calls the AI model with a conversation history. |
| `response = model.bind_tools(tools).invoke(messages)` | Binds the tools and asks the AI to respond. |
| `def call_tool(tool_call):` | Defines a function to call a tool. |
| `tool = tools_by_name[tool_call["name"]]` | Retrieves the correct tool from the dictionary. |
| `observation = tool.invoke(tool_call)` | Calls the tool and gets the result. |

---

# ğŸ“Œ **4. Creating the Agentâ€™s Logic**  
### ğŸ”„ **How It Works?**  
1ï¸âƒ£ The agent receives a userâ€™s message.  
2ï¸âƒ£ It asks the AI to generate a response.  
3ï¸âƒ£ If the AI calls a tool, the agent executes it.  
4ï¸âƒ£ If human assistance is needed, it pauses execution.  
5ï¸âƒ£ The AI continues after human input is received.  

### ğŸ“ **Code Implementation**  

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.message import add_messages

checkpointer = MemorySaver()

@entrypoint(checkpointer=checkpointer)
def agent(messages, previous):
    if previous is not None:
        messages = add_messages(previous, messages)

    llm_response = call_model(messages).result()
    while True:
        if not llm_response.tool_calls:
            break

        # Execute tools
        tool_result_futures = [call_tool(tool_call) for tool_call in llm_response.tool_calls]
        tool_results = [fut.result() for fut in tool_result_futures]

        # Append tool results to messages
        messages = add_messages(messages, [llm_response, *tool_results])

        # Call model again
        llm_response = call_model(messages).result()

    messages = add_messages(messages, llm_response)
    return entrypoint.final(value=llm_response, save=messages)
```

---

# ğŸ“Œ **5. Running the AI Agent**
### ğŸ“ **Code Implementation**  

```python
def _print_step(step: dict) -> None:
    for task_name, result in step.items():
        if task_name == "agent":
            continue  # just stream from tasks
        print(f"\n{task_name}:")
        if task_name == "__interrupt__":
            print(result)
        else:
            result.pretty_print()

config = {"configurable": {"thread_id": "1"}}

user_message = {
    "role": "user",
    "content": (
        "Can you reach out for human assistance: what should I feed my cat? "
        "Separately, can you check the weather in San Francisco?"
    ),
}

for step in agent.stream([user_message], config):
    _print_step(step)
```

---

# ğŸ¯ **Final Thoughts**  
âœ… We built an AI agent that can **call external tools**.  
âœ… The AI can **pause execution and request human assistance**.  
âœ… The AI **resumes execution** after human input.  

### ğŸ’¡ **Alternative Use Cases**  
1ï¸âƒ£ **Healthcare AI Assistant**: Can ask a doctor for help when needed.  
2ï¸âƒ£ **Finance Chatbot**: Can reach a human for complex investment advice.  
3ï¸âƒ£ **E-commerce AI**: Can involve a human for order cancellations.  

Now you have a **fully functional AI agent** that integrates **human intervention** when necessary. ğŸš€