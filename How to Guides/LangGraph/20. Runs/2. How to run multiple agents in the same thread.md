

# 2. How to Run Multiple Agents on the Same Thread 🤖

In the LangGraph Cloud platform, the concept of threads plays a crucial role in managing interactions between agents. A thread is not explicitly tied to a specific agent, which means multiple agents can operate within the same thread. This allows agents to pick up where another left off, using shared context to respond more intelligently.

### What is a Thread? 🧵

A thread is a pathway where multiple agents can communicate and continue their work. The real benefit of using threads is that it allows you to run different agents and share information between them seamlessly. In LangGraph, a thread isn't bound to one agent, making it possible for several agents to work together using the same data and context.

### Example Scenario: Running Two Agents on the Same Thread ⚙️

Imagine you have two AI assistants—one created by OpenAI and another with a default configuration. You want to use the first assistant to answer a question, then let the second assistant build on the information from the first.

#### 1. **Create Two Agents** 🛠️

First, you'll set up the agents. One of them will be configured with OpenAI's model, and the other will have the default settings.

```python
from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)

# Create OpenAI Assistant
openai_assistant = await client.assistants.create(
    graph_id="agent", config={"configurable": {"model_name": "openai"}}
)

# Retrieve all available assistants
assistants = await client.assistants.search()
default_assistant = [a for a in assistants if not a["config"]][0]

# Print the assistant details to confirm
print(openai_assistant)
print(default_assistant)
```

#### 2. **Run the First Agent** 🎤

Next, you run the first assistant on the thread. In this case, you ask it, "Who made you?"

```python
thread = await client.threads.create()
input = {"messages": [{"role": "user", "content": "who made you?"}]}
async for event in client.runs.stream(
    thread["thread_id"],
    openai_assistant["assistant_id"],
    input=input,
    stream_mode="updates",
):
    print(f"Receiving event of type: {event.event}")
    print(event.data)
```

##### Output:

This interaction will produce a response from the OpenAI assistant, which provides an answer like:  
"I was created by OpenAI, a research organization focused on developing and advancing artificial intelligence technology."

#### 3. **Run the Second Agent Using Context** 🔄

Now that the first agent has responded, you run the second agent (the default assistant). The second assistant is aware of the prior context from the thread and responds to the follow-up question, "And you?"

```python
input = {"messages": [{"role": "user", "content": "and you?"}]}
async for event in client.runs.stream(
    thread["thread_id"],
    default_assistant["assistant_id"],
    input=input,
    stream_mode="updates",
):
    print(f"Receiving event of type: {event.event}")
    print(event.data)
```

##### Output:

The second assistant responds with:  
"I am an artificial intelligence created by Anthropic, not by OpenAI. I should not have stated that OpenAI created me, as that is incorrect. Anthropic is the company that developed and trained me using advanced language models and AI technology."

This shows how the second assistant used the context from the first to give a relevant answer.

### Key Benefits of Running Multiple Agents on One Thread 💡

- **Shared Context:** Multiple agents can build on each other's knowledge. This allows for more coherent and nuanced responses, especially in complex scenarios where different agents might specialize in different tasks.
- **Efficiency:** By using the same thread, you reduce the overhead of starting new threads for each interaction, making it more resource-efficient.
- **Flexibility:** Different agents with different configurations can run in parallel on the same thread, providing diverse perspectives and solutions.

### Real-World Example 🌍

Consider a customer support chatbot for an e-commerce platform. One agent could handle product inquiries, while another handles shipping questions. By running both on the same thread, the platform can provide more personalized responses:

- **Customer asks about a product:** The product inquiry agent responds.
- **Customer asks about delivery time:** The shipping agent uses the product details (context) to provide accurate delivery time based on the customer's location.

### Conclusion 🔑

Running multiple agents on the same thread in LangGraph Cloud allows for more dynamic, efficient, and context-aware interactions. This approach is ideal for building systems that require collaboration between different agents, each with its own specialization.