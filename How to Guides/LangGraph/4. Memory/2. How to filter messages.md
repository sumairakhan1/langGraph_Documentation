# ğŸ“ Managing Conversation History in LangGraph

## ğŸ” Why Manage Conversation History?
Managing conversation history is crucial when working with Large Language Models (LLMs). If the conversation history is not managed properly, it can:
- **Increase memory usage** ğŸ’¾
- **Make API calls more expensive** ğŸ’°
- **Slow down processing time** â³
- **Cause errors due to exceeding token limits** âŒ

To handle this, we need an efficient way to **filter, trim, and manage** conversation history.

---

## ğŸ› ï¸ Setting Up the Environment
Before we start implementing conversation history management, we need to install and configure the required packages.

### ğŸ“Œ Install Required Packages
Run the following command to install the necessary Python libraries:

```python
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_anthropic
```
ğŸ”¹ **What does this do?**
- Installs **LangGraph**, a library for structuring conversation workflows.
- Installs **LangChain Anthropic**, which helps interact with the **Claude** model.

---

### ğŸ”‘ Set Up API Keys
We need an API key to use **Anthropic's Claude model**.

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
```

ğŸ”¹ **Explanation**:
- We use `os.environ` to check if the API key is already set.
- If it's not, we use `getpass.getpass()` to securely input the API key.

---

## ğŸ¤– Building a Conversation Agent
We will now build an **AI agent** that can remember conversation history.

### ğŸ“Œ Import Required Libraries
```python
from typing import Literal
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import MessagesState, StateGraph, START, END
from langgraph.prebuilt import ToolNode
```

ğŸ”¹ **Explanation**:
- `ChatAnthropic`: Used to interact with **Claude**.
- `MemorySaver`: Stores conversation history.
- `StateGraph`: Manages the flow of conversation.

---

### ğŸ” Create a Search Tool
Let's create a simple **search tool** that an AI agent can use.

```python
@tool
def search(query: str):
    """Call to surf the web."""
    return "It's sunny in San Francisco, but you better look out if you're a Gemini ğŸ˜ˆ."
```

ğŸ”¹ **Explanation**:
- This is a **dummy function** representing a search tool.
- The AI can call this function to **fetch** information.

---

### ğŸ”— Bind the Model and Tools
Now, let's bind the **Claude model** to our search tool.

```python
tools = [search]
tool_node = ToolNode(tools)
model = ChatAnthropic(model_name="claude-3-haiku-20240307")
bound_model = model.bind_tools(tools)
```

ğŸ”¹ **Explanation**:
- `ToolNode(tools)`: Wraps our tools so the AI can use them.
- `ChatAnthropic(model_name="claude-3-haiku-20240307")`: Initializes the Claude model.
- `bind_tools(tools)`: Connects our search tool to the model.

---

## ğŸ”„ Controlling the Conversation Flow
### ğŸ“Œ Define a Function to Decide When to Stop
```python
def should_continue(state: MessagesState):
    """Return the next node to execute."""
    last_message = state["messages"][-1]
    
    if not last_message.tool_calls:
        return END  # Stop if there is no function call
    return "action"  # Continue if there's a function call
```

ğŸ”¹ **Explanation**:
- If the AI's last message does **not** contain a function call, **the conversation stops**.
- Otherwise, it continues executing actions.

---

### ğŸ“Œ Define the Function to Call the Model
```python
def call_model(state: MessagesState):
    response = bound_model.invoke(state["messages"])
    return {"messages": response}  # Return the AI's response
```

ğŸ”¹ **Explanation**:
- This function **calls** the AI model.
- It returns a **list of messages** that will be added to the conversation history.

---

### ğŸ”— Create the Conversation Workflow
```python
workflow = StateGraph(MessagesState)

workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)

workflow.add_edge(START, "agent")
workflow.add_conditional_edges("agent", should_continue, ["action", END])
workflow.add_edge("action", "agent")

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

ğŸ”¹ **Explanation**:
- Creates a **StateGraph** to manage conversation flow.
- Defines **nodes**:
  - `"agent"` calls the AI model.
  - `"action"` executes tools.
- Uses `should_continue()` to decide when to stop.
- Saves conversation history using `MemorySaver()`.

---

## ğŸ“ Testing the Agent
Let's test our agent by sending messages.

```python
from langchain_core.messages import HumanMessage

config = {"configurable": {"thread_id": "2"}}
input_message = HumanMessage(content="hi! I'm Bob")

for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

ğŸ”¹ **Explanation**:
- **Sends a message** `"hi! I'm Bob"` to the AI.
- AI **remembers Bobâ€™s name** and responds accordingly.

### ğŸ“Œ Check if AI Remembers
```python
input_message = HumanMessage(content="what's my name?")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```
âœ… **Output**:
```
You said your name is Bob, so that is the name I have for you.
```
---

## ğŸš€ Optimizing Conversation History
### ğŸ” Filtering Messages to Reduce Memory Usage
Instead of storing **all messages**, we can **filter** and keep only the last message.

```python
def filter_messages(messages: list):
    return messages[-1:]  # Keep only the last message
```

### ğŸ“Œ Modify `call_model` to Use Filtering
```python
def call_model(state: MessagesState):
    messages = filter_messages(state["messages"])
    response = bound_model.invoke(messages)
    return {"messages": response}
```

ğŸ”¹ **Effect**:
- AI **forgets** past messages, making conversation **lighter** and **cheaper**.

### ğŸ“Œ Test the Filtered Conversation
```python
input_message = HumanMessage(content="what's my name?")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```
âœ… **Output**:
```
I'm afraid I don't actually know your name.
```
(ğŸš¨ AI **forgot** because we only keep the last message!)

---

## ğŸŒ Real-World Use Cases
Managing conversation history is essential in:
1. **Chatbots** ğŸ¤– - To keep conversations **relevant** while saving memory.
2. **Customer Support** ğŸ’¬ - To store recent interactions and resolve issues **faster**.
3. **Virtual Assistants** ğŸ¡ - To remember **short-term** context for efficient responses.
4. **AI-Powered

----
# ğŸ“© How to Filter Messages in LangChain

Filtering messages is an important feature when working with **LangChain**, especially in complex conversational AI applications. When dealing with a long list of messages from multiple sources (e.g., AI responses, human input, system messages), filtering helps in **managing state**, controlling **what data is passed** to different models, and structuring responses efficiently.

This guide will cover:

- What is **message filtering** and why it is useful ğŸ”
- How to **filter messages by type, ID, or name** ğŸ—‚ï¸
- **Real-world applications** of message filtering ğŸŒ
- **Code examples** with **step-by-step explanations** ğŸ“

---

## ğŸ“Œ **What is Message Filtering?**

In **LangChain**, messages are stored as a **list of message objects**, which may come from:
- **System messages** (instructions for the AI)
- **Human messages** (user inputs)
- **AI messages** (AI-generated responses)

When interacting with AI models, we may want to **selectively filter** messages based on:
1. **Message Type** â€“ Filter only `HumanMessage` or `AIMessage`
2. **Message ID** â€“ Include or exclude messages based on their unique ID
3. **Message Name** â€“ Include/exclude messages from a particular speaker

---

## ğŸš€ **Real-World Use Cases**
Filtering messages is useful in **AI chatbots, virtual assistants, and customer service automation**. Here are some real-world applications:

âœ… **Selective AI Memory:**  
- A chatbot may need to **remember only user inputs** while ignoring system prompts.

âœ… **Filtering for Summarization:**  
- If we want to **summarize only human questions**, we can filter out AI responses.

âœ… **Custom AI Training:**  
- When fine-tuning models, we might want to **exclude certain types of messages**, such as greetings.

---

## ğŸ—ï¸ **Basic Implementation of Message Filtering**
To use message filtering in **LangChain**, we first import the necessary modules:

```python
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    filter_messages,
)
```

Now, letâ€™s create a list of messages:

```python
messages = [
    SystemMessage("you are a good assistant", id="1"),
    HumanMessage("example input", id="2", name="example_user"),
    AIMessage("example output", id="3", name="example_assistant"),
    HumanMessage("real input", id="4", name="bob"),
    AIMessage("real output", id="5", name="alice"),
]
```

---

## ğŸ¯ **Filtering Messages by Type**
If we want to filter **only human messages**:

```python
filtered = filter_messages(messages, include_types="human")
print(filtered)
```

ğŸ”¹ **Output:**
```python
[
    HumanMessage(content='example input', name='example_user', id='2'),
    HumanMessage(content='real input', name='bob', id='4')
]
```

### ğŸ“ **Explanation:**
- `include_types="human"` tells `filter_messages` to return **only HumanMessage objects**.
- This removes `AIMessage` and `SystemMessage`.

---

## ğŸš« **Excluding Messages by Name**
We can **exclude** messages from specific names:

```python
filtered = filter_messages(messages, exclude_names=["example_user", "example_assistant"])
print(filtered)
```

ğŸ”¹ **Output:**
```python
[
    SystemMessage(content='you are a good assistant', id='1'),
    HumanMessage(content='real input', name='bob', id='4'),
    AIMessage(content='real output', name='alice', id='5')
]
```

### ğŸ“ **Explanation:**
- `exclude_names=["example_user", "example_assistant"]` removes messages from users **example_user** and **example_assistant**.
- The system message and the remaining human/AI messages are included.

---

## ğŸ†” **Filtering Messages by Type and Excluding Specific IDs**
We can **filter messages by type while excluding specific IDs**:

```python
filtered = filter_messages(messages, include_types=[HumanMessage, AIMessage], exclude_ids=["3"])
print(filtered)
```

ğŸ”¹ **Output:**
```python
[
    HumanMessage(content='example input', name='example_user', id='2'),
    HumanMessage(content='real input', name='bob', id='4'),
    AIMessage(content='real output', name='alice', id='5')
]
```

### ğŸ“ **Explanation:**
- `include_types=[HumanMessage, AIMessage]` includes only **human and AI messages**.
- `exclude_ids=["3"]` removes the **AIMessage with ID 3**.

---

## ğŸ”— **Using Message Filtering in Chains**
Filtering messages is useful in **AI workflows**, such as **processing only relevant messages** before passing them to an AI model.

### **Example: Filtering Messages Before Sending to AI**
We can integrate filtering into a **LangChain AI model pipeline**:

```python
%pip install -qU langchain-anthropic

from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=0)

# Define a filter to exclude messages from certain users
filter_ = filter_messages(exclude_names=["example_user", "example_assistant"])

# Chain the filter with the AI model
chain = filter_ | llm
response = chain.invoke(messages)

print(response)
```

### ğŸ“ **Explanation:**
- We **install** `langchain-anthropic` (required for `ChatAnthropic` model).
- We **define a filter** to exclude messages from `example_user` and `example_assistant`.
- We **create a processing chain** (`filter_ | llm`) where the filter **removes unnecessary messages** before passing the remaining ones to `ChatAnthropic`.

---

## ğŸ¯ **Using `invoke()` to Run Filters Separately**
Instead of using the filter in a chain, we can **invoke it separately**:

```python
filtered_messages = filter_.invoke(messages)
print(filtered_messages)
```

ğŸ”¹ **Output:**
```python
[
    HumanMessage(content='real input', name='bob', id='4'),
    AIMessage(content='real output', name='alice', id='5')
]
```

### ğŸ“ **Explanation:**
- `invoke(messages)` applies the **filter** to our list.
- This removes messages from `example_user` and `example_assistant`.

---

## ğŸ“Œ **Key Takeaways**
âœ… **Message filtering** is crucial for managing AI conversations efficiently.  
âœ… We can filter messages by **type, ID, and name** to customize AI interactions.  
âœ… **Real-world applications** include **chatbots, AI memory management, and summarization**.  
âœ… Filtering can be used **independently** or within **AI processing chains** for better performance.  
âœ… The `filter_messages()` utility provides a **clean and simple way** to work with message lists.  

---

## ğŸ¯ **Next Steps**
ğŸ”¹ Try implementing message filtering in your own **LangChain chatbot**.  
ğŸ”¹ Experiment with **different filtering conditions** to optimize responses.  
ğŸ”¹ Use **filtered messages as context** for AI models to improve accuracy.  

---

Thatâ€™s everything you need to **filter messages like a pro** in **LangChain**! ğŸš€  
Let me know if you need further **clarification or more examples**! ğŸ˜Š