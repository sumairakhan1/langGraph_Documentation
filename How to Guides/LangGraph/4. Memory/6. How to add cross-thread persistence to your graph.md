# 6. How to add cross-thread persistence to your graph

# ğŸ§  **Adding Cross-Thread Persistence to Your Graph in LangGraph**  

When working with conversational AI, it's essential to maintain **user-specific context** across multiple threads. This ensures that users donâ€™t have to repeat their details every time they start a new conversation.  

LangGraph provides a way to persist **graph state** across multiple interactions and even across multiple conversation threads. This is useful for applications like **personalized AI assistants, customer support bots, or any AI-driven system that needs memory retention across sessions**.  

In this guide, we will explore:  
âœ… What **cross-thread persistence** means  
âœ… How to implement **shared memory** using LangGraph  
âœ… Real-world **examples** of this feature  
âœ… Step-by-step **code walkthrough** with explanations  

---

## ğŸ” **What is Cross-Thread Persistence?**  

Normally, AI chatbots process conversations **independently**â€”each thread starts fresh without knowledge of previous interactions.  

**Cross-thread persistence** allows you to:  
- Store **user preferences** across multiple conversations  
- Retrieve past information **without user repetition**  
- Maintain a **seamless experience** across sessions  

ğŸ’¡ **Example Use Case:**  
Imagine a **personalized AI shopping assistant**. If a user tells the assistant their **clothing size and color preference**, it should remember these preferences across multiple chats without asking again.  

---

## âš™ **Setting Up the Environment**  

Before implementing cross-thread persistence, install the required dependencies:  

```bash
pip install -U langchain_openai langgraph
```

Then, set up API keys:  

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")
_set_env("OPENAI_API_KEY")
```

ğŸ”¹ **Explanation:**  
- We use `os.environ` to check if an API key is already set.  
- If not, we prompt the user using `getpass.getpass()` (hides input for security).  
- This ensures the API keys are securely stored in the environment.  

---

## ğŸ“‚ **Defining Shared Memory (In-Memory Store)**  

To enable memory persistence, we define an **InMemoryStore** to save and retrieve user information.  

```python
from langgraph.store.memory import InMemoryStore
from langchain_openai import OpenAIEmbeddings

in_memory_store = InMemoryStore(
    index={
        "embed": OpenAIEmbeddings(model="text-embedding-3-small"),
        "dims": 1536,
    }
)
```

ğŸ”¹ **Explanation:**  
- `InMemoryStore` acts as a **temporary database** for storing user details.  
- `OpenAIEmbeddings` enables **semantic search** on stored information.  
- `"dims": 1536` sets the **vector dimensions** for embeddings.  

ğŸ’¡ **Why use embeddings?**  
This allows the system to **search and retrieve relevant user memories** based on the context of the conversation.  

---

## ğŸ— **Building the Graph with Cross-Thread Memory**  

Now, letâ€™s create a **conversation graph** that can store and recall user details.  

### 1ï¸âƒ£ **Import Required Modules**  

```python
import uuid
from typing import Annotated
from typing_extensions import TypedDict

from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.base import BaseStore
```

ğŸ”¹ **Explanation:**  
- `uuid` is used to generate unique memory IDs.  
- `ChatAnthropic` provides access to the **Claude AI model** for responses.  
- `StateGraph` helps **define a conversational graph structure**.  
- `BaseStore` is the parent class for all **memory storage** systems.  

---

### 2ï¸âƒ£ **Defining the AI Chat Model**  

```python
model = ChatAnthropic(model="claude-3-5-sonnet-20240620")
```

- We initialize **Claude 3.5 Sonnet** as the AI model.  
- This model will process **user messages and generate responses**.  

---

### 3ï¸âƒ£ **Defining the Memory Retrieval Function**  

```python
def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):
    user_id = config["configurable"]["user_id"]
    namespace = ("memories", user_id)
    memories = store.search(namespace, query=str(state["messages"][-1].content))
    info = "\n".join([d.value["data"] for d in memories])
    system_msg = f"You are a helpful assistant talking to the user. User info: {info}"

    # Store new memories if the user asks the model to remember
    last_message = state["messages"][-1]
    if "remember" in last_message.content.lower():
        memory = "User name is Bob"
        store.put(namespace, str(uuid.uuid4()), {"data": memory})

    response = model.invoke(
        [{"role": "system", "content": system_msg}] + state["messages"]
    )
    return {"messages": response}
```

ğŸ”¹ **Explanation:**  
1ï¸âƒ£ Retrieves the `user_id` from the **config settings**.  
2ï¸âƒ£ Uses `store.search(namespace, query=...)` to **fetch previous memories**.  
3ï¸âƒ£ If the user says **"remember"**, it saves the data in memory (`store.put(...)`).  
4ï¸âƒ£ Constructs a **system message** that includes retrieved user information.  
5ï¸âƒ£ Passes the full conversation to **Claude AI** for response generation.  

ğŸ’¡ **Why store in `namespace`?**  
Each **user's memory** is stored separately (`("memories", user_id)`). This prevents one userâ€™s data from affecting anotherâ€™s.  

---

### 4ï¸âƒ£ **Building the Conversation Graph**  

```python
builder = StateGraph(MessagesState)
builder.add_node("call_model", call_model)
builder.add_edge(START, "call_model")

graph = builder.compile(checkpointer=MemorySaver(), store=in_memory_store)
```

ğŸ”¹ **Explanation:**  
- `StateGraph(MessagesState)`: Creates a **stateful conversation graph**.  
- `add_node("call_model", call_model)`: Adds our **memory-aware AI function** to the graph.  
- `add_edge(START, "call_model")`: Sets the **starting point** for conversations.  
- `compile(checkpointer=MemorySaver(), store=in_memory_store)`:  
  - Uses `MemorySaver()` to **persist data**.  
  - Passes `in_memory_store` for **retrieving user memories**.  

---

## ğŸš€ **Running the Graph**  

### âœ… **User Saves Their Name**  

```python
config = {"configurable": {"thread_id": "1", "user_id": "1"}}
input_message = {"role": "user", "content": "Hi! Remember: my name is Bob"}

for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

ğŸ’¡ **Expected Output:**  
```
Hello Bob! It's nice to meet you. I'll remember that your name is Bob.
```

---

### âœ… **Checking If the AI Remembers**  

```python
config = {"configurable": {"thread_id": "2", "user_id": "1"}}
input_message = {"role": "user", "content": "what is my name?"}

for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

ğŸ’¡ **Expected Output:**  
```
Your name is Bob.
```

---

### âœ… **Testing with a New User (No Memory)**  

```python
config = {"configurable": {"thread_id": "3", "user_id": "2"}}
input_message = {"role": "user", "content": "what is my name?"}

for chunk in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```

ğŸ’¡ **Expected Output:**  
```
I apologize, but I don't have any information about your name.
```

---

## ğŸ¯ **Conclusion**  

âœ… **Cross-thread persistence** ensures users donâ€™t have to repeat themselves.  
âœ… **Stored memories** allow for more **personalized interactions**.  
âœ… **LangGraph** makes it easy to implement **stateful AI chatbots**.  

ğŸ’¡ **Real-World Use Cases:**  
- **AI-powered customer service bots**  
- **Personalized learning assistants**  
- **Virtual shopping assistants**  

ğŸ”¹ **Whatâ€™s next?** Try expanding this with **long-term memory storage (e.g., databases)**! ğŸš€