# 7. How to Add Semantic Search to Your Agent's Memory

# Adding Semantic Search to Your Agent's Memory üß†üîç

In this guide, we'll explore how to add **semantic search** to your agent's memory, which allows your agent to retrieve information based on semantic similarity. This means it can search for content that is *meaningfully similar* to a query rather than just searching for exact keywords. For example, if a user asks about their favorite food preferences, the agent can find memories related to food, even if the query is worded differently.

## What is Semantic Search? üß†üí°

Semantic search improves the accuracy and relevance of search results by understanding the *meaning* of the query, not just the words. In traditional search, if you search for "pizza," it might return memories with exact matches for that word. But with semantic search, it can find memories related to food preferences even if the user asks differently (e.g., "What do I like to eat?").

### Real-world Example: üçïüçù
Imagine a virtual assistant that helps a user remember their preferences for food. The assistant can store memories like "I love pizza" or "I prefer Italian food." When the user asks something like "What do I like to eat?", the assistant should return relevant memories, even if they don‚Äôt contain the exact words from the query.

## Step-by-Step Guide üìù

### Prerequisites üß∞

Before diving in, you need to install the necessary dependencies:

```bash
pip install -U langgraph langchain-openai langchain
```

This installs the tools required for semantic search in your agent‚Äôs memory.

### Step 1: Set Up Environment Variables üåçüîë

First, you need to set up your API key for OpenAI (or whichever model you are using). This is necessary for embedding text and performing semantic search.

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

### Explanation:
- The `_set_env()` function ensures your environment variables are set (such as the API key).
- It uses `getpass.getpass()` to securely ask for your OpenAI API key.

### Step 2: Initialize the Memory Store üóÉÔ∏è

We‚Äôll create a memory store with semantic search capabilities. This is where the memories (information) will be stored and searched later.

```python
from langchain.embeddings import init_embeddings
from langgraph.store.memory import InMemoryStore

# Create store with semantic search enabled
embeddings = init_embeddings("openai:text-embedding-3-small")
store = InMemoryStore(
    index={
        "embed": embeddings,
        "dims": 1536,
    }
)
```

### Explanation:
- **`init_embeddings("openai:text-embedding-3-small")`**: This initializes an embedding model, specifically the "text-embedding-3-small" model from OpenAI. Embeddings convert text into a numerical vector that can be compared for similarity.
- **`InMemoryStore`**: This is where we store our memories. The `index` configuration enables semantic search using embeddings.

### Step 3: Store Some Memories üìú

Now, let‚Äôs store some memories related to the user‚Äôs preferences.

```python
# Store some memories
store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
store.put(("user_123", "memories"), "2", {"text": "I prefer Italian food"})
store.put(("user_123", "memories"), "3", {"text": "I don't like spicy food"})
store.put(("user_123", "memories"), "4", {"text": "I am studying econometrics"})
store.put(("user_123", "memories"), "5", {"text": "I am a plumber"})
```

### Explanation:
- **`store.put()`**: This stores a memory in the agent‚Äôs memory store. Each memory has a unique ID (e.g., `"1"`, `"2"`).
- We are storing text-based memories like "I love pizza" and "I prefer Italian food," which will later be used for semantic search.

### Step 4: Perform Semantic Search üîç

Now, we‚Äôll use semantic search to find memories based on a query. For example, the user might ask, "I like food?"

```python
# Find memories about food preferences
memories = store.search(("user_123", "memories"), query="I like food?", limit=5)

for memory in memories:
    print(f'Memory: {memory.value["text"]} (similarity: {memory.score})')
```

### Explanation:
- **`store.search()`**: This searches the memories for those most similar to the query. The `query` here is "I like food?", and it will return memories about food preferences.
- **`memory.value["text"]`**: The actual memory text.
- **`memory.score`**: A similarity score showing how close the memory is to the query.

### Output:

```plaintext
Memory: I prefer Italian food (similarity: 0.46482669521168163)
Memory: I love pizza (similarity: 0.35514845174380766)
Memory: I am a plumber (similarity: 0.155698702336571)
```

As you can see, the search results return memories related to food even though the user didn‚Äôt specifically mention "pizza" or "Italian food" in their query.

### Step 5: Use Semantic Search in Your Agent üßë‚Äçüíªü§ñ

Let‚Äôs integrate this memory search into a chatbot-like agent that can use the semantic search results in real-time. This will make the assistant more context-aware.

```python
from langchain.chat_models import init_chat_model
from langgraph.store.base import BaseStore
from langgraph.graph import START, MessagesState, StateGraph

llm = init_chat_model("openai:gpt-4o-mini")

def chat(state, *, store: BaseStore):
    # Search based on user's last message
    items = store.search(
        ("user_123", "memories"), query=state["messages"][-1].content, limit=2
    )
    memories = "\n".join(item.value["text"] for item in items)
    memories = f"## Memories of user\n{memories}" if memories else ""
    response = llm.invoke(
        [
            {"role": "system", "content": f"You are a helpful assistant.\n{memories}"},
            *state["messages"],
        ]
    )
    return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(chat)
builder.add_edge(START, "chat")
graph = builder.compile(store=store)

for message, metadata in graph.stream(
    input={"messages": [{"role": "user", "content": "I'm hungry"}]},
    stream_mode="messages",
):
    print(message.content, end="")
```

### Explanation:
- **`chat()`**: This function handles the chat interactions. It performs a semantic search on the user‚Äôs last message to gather relevant memories.
- **`llm.invoke()`**: This calls the language model (in this case, GPT-4) to generate a response, incorporating the memories retrieved by semantic search.

### Output:
```plaintext
What are you in the mood for? Since you love Italian food and pizza, would you like to order a pizza or try making one at home?
```

### Real-World Use Case üè†üí°

**Virtual Personal Assistant for Food Preferences**: 
You can build an assistant that remembers the user's food preferences and provides suggestions based on past interactions. For example, if the user says, "I'm hungry," the assistant will recall that they love Italian food and offer to order pizza or make a pasta dish. 

This kind of semantic memory allows for a more natural and intuitive interaction.

## Conclusion üèÅ

Adding semantic search to your agent‚Äôs memory allows it to retrieve contextually relevant information, making interactions feel more intelligent and responsive. This setup can be applied to virtual assistants, recommendation systems, or any scenario where understanding user intent beyond exact keyword matches is crucial.

By following this guide, you now have the ability to integrate semantic search into your agent‚Äôs memory system! üéâ

---


# üöÄ How to Add Semantic Search to Your Agent's Memory

In this guide, we'll walk through how to integrate **semantic search** into your agent's memory store. This will enable your agent to find information based on the meaning of the text‚Äînot just keywords. We'll cover each step in detail with examples, real-world applications, and thorough code explanations.

---

## üåü What Is Semantic Search?

Semantic search uses vector embeddings to capture the underlying meaning of text. Instead of matching exact words, it compares the context and concepts behind the text. This is especially useful in applications like chatbots, recommendation systems, and intelligent assistants.

**Real-World Example:**  
Imagine a customer support bot that recalls past conversations. When a customer says, "I'm having trouble with my order," the bot can semantically retrieve previous interactions about order issues‚Äîeven if the exact words differ.

---

## üõ† Step 1: Prerequisites & Environment Setup

Before diving in, you need to install the required libraries and set up your environment. We use `langgraph`, `langchain-openai`, and `langchain` for this project.

### Code Example

```python
%%capture --no-stderr
%pip install -U langgraph langchain-openai langchain

import getpass
import os

# Function to securely set an environment variable if not already set
def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

# Set your OpenAI API key securely
_set_env("OPENAI_API_KEY")
```

### Explanation
- **`%%capture --no-stderr`**: This Jupyter magic command hides installation logs and errors for a cleaner output.
- **`%pip install -U ...`**: Installs or updates the required packages.
- **`import getpass, os`**: Imports modules to handle secure input and environment variables.
- **`_set_env(var: str)`**: Checks if an environment variable exists; if not, prompts the user to input it securely.
- **`_set_env("OPENAI_API_KEY")`**: Sets the API key necessary for accessing OpenAI's services.

---

## üîç Step 2: Creating a Semantic Memory Store

We now create a memory store that supports semantic search by configuring it with an embedding model.

### Code Example

```python
from langchain.embeddings import init_embeddings
from langgraph.store.memory import InMemoryStore

# Initialize the embeddings model using OpenAI's text embedding model.
embeddings = init_embeddings("openai:text-embedding-3-small")

# Create an in-memory store with semantic search enabled
store = InMemoryStore(
    index={
        "embed": embeddings,  # Embedding function to convert text into vectors
        "dims": 1536,         # Dimensionality of the embedding vectors
    }
)
```

### Explanation
- **`from langchain.embeddings import init_embeddings`**: Imports the function to initialize an embedding model.
- **`from langgraph.store.memory import InMemoryStore`**: Imports the in-memory store class.
- **`init_embeddings("openai:text-embedding-3-small")`**: Sets up the embedding model that converts text into numerical vectors.
- **`store = InMemoryStore(index={...})`**: Creates the store with semantic search enabled:
  - **`"embed": embeddings`**: Provides the embedding function.
  - **`"dims": 1536`**: Specifies the size (number of dimensions) of the embedding vectors.

---

## üíæ Step 3: Storing Memories

Next, we store user "memories" in our store. Each memory is identified by a key, which helps in grouping and retrieving data.

### Code Example

```python
# Store various user memories in the memory store
store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
store.put(("user_123", "memories"), "2", {"text": "I prefer Italian food"})
store.put(("user_123", "memories"), "3", {"text": "I don't like spicy food"})
store.put(("user_123", "memories"), "4", {"text": "I am studying econometrics"})
store.put(("user_123", "memories"), "5", {"text": "I am a plumber"})
```

### Explanation
- **`store.put(key, id, data)`**: Stores a memory in the store.
  - **`("user_123", "memories")`**: A namespace grouping related to a particular user.
  - **`"1", "2", ...`**: Unique identifiers for each memory.
  - **`{"text": ...}`**: The data being stored. The text is converted into a vector via the embedding function.

---

## üîé Step 4: Searching Memories

Now that we have stored memories, we can search them using natural language queries. The semantic search retrieves the most relevant memories based on the query's meaning.

### Code Example

```python
# Perform a semantic search for memories related to food preferences
memories = store.search(("user_123", "memories"), query="I like food?", limit=5)

# Print out the found memories and their similarity scores
for memory in memories:
    print(f'Memory: {memory.value["text"]} (similarity: {memory.score})')
```

### Explanation
- **`store.search(namespace, query, limit)`**: Searches within a specific namespace using a natural language query.
  - **`("user_123", "memories")`**: Specifies the memory group to search within.
  - **`query="I like food?"`**: The natural language query.
  - **`limit=5`**: Restricts the number of search results.
- **`memory.value["text"]`**: Retrieves the stored text.
- **`memory.score`**: Indicates the similarity between the query and the memory. A higher score means greater relevance.

---

## ü§ñ Step 5: Integrating Semantic Search into an Agent

Finally, we'll integrate semantic search into a chat agent. This allows the agent to provide more context-aware responses by including relevant memories in its prompts.

### Code Example

```python
from typing import Optional
from langchain.chat_models import init_chat_model
from langgraph.store.base import BaseStore
from langgraph.graph import START, MessagesState, StateGraph

# Initialize the chat model for the agent.
llm = init_chat_model("openai:gpt-4o-mini")

def chat(state, *, store: BaseStore):
    # Retrieve the user's last message to search for related memories
    items = store.search(
        ("user_123", "memories"), query=state["messages"][-1].content, limit=2
    )
    
    # Concatenate the memory texts if any memories are found
    memories = "\n".join(item.value["text"] for item in items)
    memories = f"## Memories of user\n{memories}" if memories else ""
    
    # Build the prompt for the language model, including the memories
    response = llm.invoke(
        [
            {"role": "system", "content": f"You are a helpful assistant.\n{memories}"},
            *state["messages"],
        ]
    )
    return {"messages": [response]}

# Build a state graph to manage conversation flows
builder = StateGraph(MessagesState)
builder.add_node(chat)
builder.add_edge(START, "chat")
graph = builder.compile(store=store)

# Simulate a conversation by streaming messages from the agent
for message, metadata in graph.stream(
    input={"messages": [{"role": "user", "content": "I'm hungry"}]},
    stream_mode="messages",
):
    print(message.content, end="")
```

### Explanation
- **`init_chat_model("openai:gpt-4o-mini")`**: Initializes the language model used for chatting.
- **`chat(state, *, store: BaseStore)`**: Defines a function that handles conversation flow:
  - **`state["messages"][-1].content`**: Retrieves the most recent user message.
  - **`store.search(...)`**: Searches for memories related to the user's last message.
  - **`"\n".join(...)`**: Joins the found memories into a single string.
  - **`llm.invoke(...)`**: Calls the language model with a system prompt that includes the user's memories, enhancing context awareness.
- **`StateGraph`**: Manages conversation states and transitions.
- **`graph.stream(...)`**: Streams the conversation responses, simulating a live chat.

---

## üéØ Summary & Takeaways

- **Semantic Search**: Uses embeddings to understand the meaning behind text, not just keywords.
- **Memory Store Setup**: Configure an in-memory store with semantic indexing to store and retrieve memories.
- **Storing Data**: Use the `.put()` method to add memories with contextual text.
- **Searching**: Use natural language queries to retrieve semantically similar memories.
- **Agent Integration**: Enhance your chat agent by incorporating relevant past memories into the conversation, leading to more personalized and context-aware responses.

By following these steps, you add a powerful layer of semantic search to your agent's memory, making it more intelligent and responsive. Happy coding! üöÄ

---

# üåü **Adding Semantic Search to Your Agent's Memory** üåü

In this guide, we'll explore how to integrate **semantic search** into your agent's memory system. This allows your agent to **search for information based on the meaning** of the query, rather than just exact matches. This approach is particularly useful for **contextual understanding** and **memory-based interactions**, making your agent more intelligent and conversational.

## üìö **What is Semantic Search?**

Semantic search improves the relevance of search results by focusing on the **meaning** of the content, rather than just keyword matching. For example, if your agent remembers that a user loves "Italian food" and the user asks for "food preferences," the semantic search will retrieve memories related to Italian food, even if the exact words aren't used.

This is useful in **real-world applications** like:

- **Customer support agents** that recall past interactions.
- **Personal assistants** remembering preferences and recommending activities.
- **Recommendation systems** that suggest content based on users‚Äô past behaviors.

Now let's dive into the code examples and how we can implement this in our agent's memory.

## üí° **Prerequisites: Installation**

Before we begin, you need to install the following libraries:

```bash
%pip install -U langgraph langchain-openai langchain
```

These libraries allow us to interact with language models and store memories for the agent.

## üß† **Step 1: Setting Up the Memory Store with Semantic Search**

To enable semantic search in the memory store, we need to **configure the memory store with an index**. This index helps organize how the memories are stored and searched.

Here's how you set up the store with semantic search:

```python
from langchain.embeddings import init_embeddings
from langgraph.store.memory import InMemoryStore

# Initialize the embeddings (OpenAI's model)
embeddings = init_embeddings("openai:text-embedding-3-small")

# Create a store with semantic search enabled
store = InMemoryStore(
    index={
        "embed": embeddings,  # Use embeddings for vector search
        "dims": 1536,         # Dimensionality of the embeddings
    }
)
```

### üßë‚Äçüíª **Explanation:**
1. **`init_embeddings("openai:text-embedding-3-small")`**: This function loads an embedding model from OpenAI. Embeddings are numerical representations of text that capture semantic meaning.
2. **`InMemoryStore`**: This is where we store our memories. By configuring it with **semantic search** enabled, it will search based on meaning, not just text matching.

## üì¶ **Step 2: Storing Memories**

Now, let's store some memories in our agent's memory store. These memories are text-based entries, like preferences or past experiences.

```python
store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
store.put(("user_123", "memories"), "2", {"text": "I prefer Italian food"})
store.put(("user_123", "memories"), "3", {"text": "I don't like spicy food"})
store.put(("user_123", "memories"), "4", {"text": "I am studying econometrics"})
store.put(("user_123", "memories"), "5", {"text": "I am a plumber"})
```

### üßë‚Äçüíª **Explanation:**
- **`store.put()`**: This method is used to add memories into the store. Each memory is stored with a unique key (e.g., "1", "2") and its corresponding text.
- **`("user_123", "memories")`**: The first argument identifies the user and the category ("memories"). The second argument is a unique identifier for the memory.
- **`{"text": "..."}`**: The value of each memory is a dictionary with a "text" field containing the user's memory.

## üîç **Step 3: Searching Memories Using Natural Language**

Once we have some memories stored, we can search them using natural language queries. This is where **semantic search** comes into play.

```python
memories = store.search(("user_123", "memories"), query="I like food?", limit=5)

for memory in memories:
    print(f'Memory: {memory.value["text"]} (similarity: {memory.score})')
```

### üßë‚Äçüíª **Explanation:**
1. **`store.search()`**: This method searches for memories that match the query, "I like food?". The `limit=5` argument restricts the number of results to 5.
2. **`memory.score`**: Each result comes with a similarity score that indicates how closely the memory matches the query.

#### Output:
```
Memory: I prefer Italian food (similarity: 0.46482669521168163)
Memory: I love pizza (similarity: 0.35514845174380766)
Memory: I am a plumber (similarity: 0.155698702336571)
```

In the output, the agent retrieves memories that are **semantically similar** to the query, even if the exact words aren‚Äôt used.

## üßë‚Äçüíº **Step 4: Using Semantic Search in Your Agent**

Now, let‚Äôs see how to use semantic search inside an agent. We'll add a **`prompt` function** to prepare the conversation for the agent and inject memories into the conversation flow.

```python
from langchain.chat_models import init_chat_model
from langgraph.store.base import BaseStore
from langgraph.graph import START, StateGraph

# Initialize a language model (OpenAI GPT-4)
llm = init_chat_model("openai:gpt-4o-mini")

def chat(state, *, store: BaseStore):
    # Search based on the user's last message
    items = store.search(
        ("user_123", "memories"), query=state["messages"][-1].content, limit=2
    )
    memories = "\n".join(item.value["text"] for item in items)
    memories = f"## Memories of user\n{memories}" if memories else ""
    
    response = llm.invoke(
        [
            {"role": "system", "content": f"You are a helpful assistant.\n{memories}"},
            *state["messages"],
        ]
    )
    return {"messages": [response]}

# Build the agent with semantic memory
builder = StateGraph(MessagesState)
builder.add_node(chat)
builder.add_edge(START, "chat")
graph = builder.compile(store=store)

for message, metadata in graph.stream(
    input={"messages": [{"role": "user", "content": "I'm hungry"}]},
    stream_mode="messages",
):
    print(message.content, end="")
```

### üßë‚Äçüíª **Explanation:**
1. **`chat(state, *, store: BaseStore)`**: This function gets the latest message from the user and uses semantic search to retrieve relevant memories.
2. **`llm.invoke()`**: This sends the messages (including memories) to the language model to generate a response.
3. **`StateGraph`**: This class handles the flow of the conversation, integrating the memories into the agent's responses.

#### Output:
```
What are you in the mood for? Since you love Italian food and pizza, maybe something in that realm would be great! Would you like suggestions for a specific dish or restaurant?
```

## üß© **Advanced Usage: Multi-Vector Indexing**

In some cases, you might want to **index different aspects of a memory** separately, like its content and emotional context. This helps improve **search precision**.

```python
store = InMemoryStore(
    index={"embed": embeddings, "dims": 1536, "fields": ["memory", "emotional_context"]}
)
store.put(
    ("user_123", "memories"),
    "mem1",
    {"memory": "Had pizza with friends at Mario's", "emotional_context": "felt happy and connected"}
)
```

### üßë‚Äçüíª **Explanation:**
- **`fields: ["memory", "emotional_context"]`**: This allows us to index both the content of the memory (e.g., "Had pizza with friends") and the emotional context (e.g., "felt happy").
- You can then search based on these fields separately to get more relevant results.

## üéØ **Conclusion**

Integrating **semantic search** into your agent‚Äôs memory system helps it **recall and respond** based on the meaning of previous interactions. This approach can significantly improve **user experience** in applications like personal assistants, customer support bots, and recommendation engines. 

With this guide, you now know how to enable semantic search, store memories, and create smarter agents that can **remember and understand** users' preferences in a conversational manner.

---

# Understanding Memory Storage and Search with LangGraph: A Beginner‚Äôs Guide

In this guide, we will walk through the concept of memory storage, overriding fields, and disabling indexing in LangGraph. This concept is useful when creating applications that remember information (memories) from interactions, such as chatbots, personal assistants, or recommendation systems. We will use examples, explain the code step by step, and also provide real-world scenarios where these concepts can be applied.

## üß† **What is Memory Storage in LangGraph?**

Memory storage in LangGraph allows us to store, search, and manage the "memories" of a user or system. This is done using a store, which can hold multiple memories, each containing key-value pairs. The store can be configured to index specific fields in a memory for efficient search.

### Example: Chatbot with Memory

Imagine a chatbot that remembers your food preferences. The chatbot can store your memory, like "I love spicy food" and use it later when making food recommendations.

### Code Example: Basic Memory Storage

```python
# Setting up an in-memory store with embeddings
store = InMemoryStore(index={"embed": embeddings, "dims": 1536, "fields": ["memory"]})

# Storing a memory
store.put(
    ("user_123", "memories"),  # User identifier and memory type
    "mem1",  # Memory identifier
    {"memory": "I love spicy food", "context": "At a Thai restaurant"}  # Memory content
)

# Searching memories with a query
results = store.search(
    ("user_123", "memories"), query="what food do they like", limit=1
)

# Displaying the results
for r in results:
    print(f"Item: {r.key}; Score ({r.score})")
    print(f"Memory: {r.value['memory']}")
    print(f"Context: {r.value['context']}\n")
```

### Code Breakdown:

- **InMemoryStore**: Initializes a store that will keep memories in memory.
- **store.put**: Adds a new memory with a user identifier, memory ID, and content.
- **store.search**: Allows you to search through stored memories based on a query.
  
In this example, the chatbot remembers "I love spicy food" and uses it for later queries, such as when asked, "What food do they like?"

---

## üîÑ **Overriding Fields During Storage**

Sometimes, you might want to override which fields of a memory are indexed when storing it. By default, LangGraph indexes the "memory" field, but you can choose to index other fields (e.g., "context") or exclude specific fields entirely.

### Real-World Example:

You might want to store memories related to food preferences but search specifically for the context of the restaurant experience.

### Code Example: Overriding Indexed Fields

```python
# Store one memory with default indexing
store.put(
    ("user_123", "memories"),
    "mem1",
    {"memory": "I love spicy food", "context": "At a Thai restaurant"},
)

# Store another overriding which fields to embed
store.put(
    ("user_123", "memories"),
    "mem2",
    {"memory": "The restaurant was too loud", "context": "Dinner at an Italian place"},
    index=["context"],  # Only index the 'context' field, not 'memory'
)

# Search about food preferences - finds mem1
results = store.search(
    ("user_123", "memories"), query="what food do they like", limit=1
)
for r in results:
    print(f"Item: {r.key}; Score ({r.score})")
    print(f"Memory: {r.value['memory']}")
    print(f"Context: {r.value['context']}\n")

# Search about restaurant atmosphere - finds mem2
results = store.search(
    ("user_123", "memories"), query="restaurant environment", limit=1
)
for r in results:
    print(f"Item: {r.key}; Score ({r.score})")
    print(f"Memory: {r.value['memory']}")
    print(f"Context: {r.value['context']}\n")
```

### Code Breakdown:

- **index=["context"]**: This tells the system to only index the "context" field for the second memory (`mem2`).
- **store.search**: We query the memories based on different criteria‚Äîfood preferences or restaurant environment.

In this case, searching for "what food do they like" will match `mem1`, and searching for "restaurant environment" will match `mem2`.

---

## üö´ **Disabling Indexing for Specific Memories**

There may be situations where certain memories should not be indexed for search, such as system-level memories or temporary data that shouldn't influence search results.

### Real-World Example:

In a personal assistant, you may want to store a "system" memory like "User completed onboarding" but not index it for searches, as it is not relevant to most queries.

### Code Example: Disabling Indexing

```python
# Store a normal indexed memory
store.put(
    ("user_123", "memories"),
    "mem1",
    {"memory": "I love chocolate ice cream", "type": "preference"},
)

# Store a system memory without indexing
store.put(
    ("user_123", "memories"),
    "mem2",
    {"memory": "User completed onboarding", "type": "system"},
    index=False,  # Disable indexing entirely
)

# Search about food preferences - finds mem1
results = store.search(("user_123", "memories"), query="what food preferences", limit=1)
for r in results:
    print(f"Item: {r.key}; Score ({r.score})")
    print(f"Memory: {r.value['memory']}")
    print(f"Type: {r.value['type']}\n")

# Search about onboarding - won't find mem2 (not indexed)
results = store.search(("user_123", "memories"), query="onboarding status", limit=1)
for r in results:
    print(f"Item: {r.key}; Score ({r.score})")
    print(f"Memory: {r.value['memory']}")
    print(f"Type: {r.value['type']}\n")
```

### Code Breakdown:

- **index=False**: This disables indexing for the `mem2` memory, meaning it won‚Äôt show up in search results.
- **store.search**: When searching for "onboarding status", it doesn't return `mem2` because it was not indexed.

By disabling indexing for specific memories like system information, you can ensure that they do not interfere with searches or recommendations.

---

## üìà **When to Use These Features in Real-World Applications**

These concepts of memory storage and search are powerful in many real-world applications. Here are some practical uses:

### 1. **Personal Assistants**
   - A personal assistant can remember your preferences (e.g., favorite food) and store them as memories.
   - It can override certain fields like emotional context (happy or sad) for better response tailoring.
   - System-level data like onboarding steps should not be searchable.

### 2. **E-commerce Recommendations**
   - E-commerce platforms can store customer preferences and purchase history.
   - The system can be optimized by overriding fields to focus on product categories while ignoring irrelevant fields like transaction IDs.

### 3. **Healthcare Applications**
   - In a healthcare chatbot, memories of patient symptoms, treatment plans, and emotional states can be stored and searched for personalized advice.

---

## üöÄ **Conclusion**

By leveraging LangGraph‚Äôs memory storage features, you can build smarter applications that understand and adapt to user preferences over time. Whether you're building a chatbot, a recommendation system, or any personalized service, these tools can help you efficiently manage and search through memories. You can also fine-tune memory storage by overriding fields or disabling indexing for certain memories, making your application more efficient and relevant to user needs.