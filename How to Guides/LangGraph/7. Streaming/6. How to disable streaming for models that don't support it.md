# 6. How to disable streaming for models that don't support it


# 🛑 How to Disable Streaming for Models That Don't Support It  

Some AI models, like OpenAI’s **O1 models**, do not support **streaming**. If you try to use them in a **streaming mode**, it will cause errors. This guide will show you how to **disable streaming** to avoid issues and ensure smooth execution.

---

## 🎯 Why Do We Need to Disable Streaming?  

### 🚨 The Problem  
- Some models do **not** support streaming.  
- If you try to stream from these models using `astream_events`, it **throws an error**.  
- **Example:** OpenAI's `o1-preview` model does **not** support streaming.  

### ✅ The Solution  
We need to **disable streaming** for such models by setting the `disable_streaming=True` parameter.

---

## 🚀 Code Example: Without Disabling Streaming  

Let’s define a simple **LangGraph** workflow using OpenAI’s **O1 model**.  

### 🔹 Step 1: Import Required Libraries  
```python
from langchain_openai import ChatOpenAI
from langgraph.graph import MessagesState, StateGraph, START, END
```
- `ChatOpenAI`: Interface for OpenAI's chat models.  
- `MessagesState`: Keeps track of chat messages.  
- `StateGraph`: Builds a graph for our chatbot.  
- `START`, `END`: Defines the start and end points of our graph.  

---

### 🔹 Step 2: Define Chatbot and Graph  
```python
llm = ChatOpenAI(model="o1-preview", temperature=1)

graph_builder = StateGraph(MessagesState)

def chatbot(state: MessagesState):
    return {"messages": [llm.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

graph = graph_builder.compile()
```
📌 **Explanation:**  
- We **initialize** the `ChatOpenAI` model (`o1-preview`).  
- **Define a chatbot function** that processes messages using `llm.invoke()`.  
- **Create a graph**:
  - Add a chatbot **node**.  
  - Connect `START` → `chatbot` → `END`.  
  - Compile the graph.  

---

### 🔹 Step 3: Try Running the Chatbot with Streaming  
```python
input = {"messages": {"role": "user", "content": "how many r's are in strawberry?"}}

try:
    async for event in graph.astream_events(input, version="v2"):
        if event["event"] == "on_chat_model_end":
            print(event["data"]["output"].content, end="", flush=True)
except:
    print("Streaming not supported!")
```
📌 **Explanation:**  
- We send a **user message** as input.  
- We attempt to **stream events** using `astream_events()`.  
- Since `o1-preview` does not support streaming, it will **throw an error**.  
- The error is **caught**, and we print `"Streaming not supported!"`.  

🔴 **Output:**  
```
Streaming not supported!
```

---

## ✅ Fixing the Issue: Disabling Streaming  

### 🔹 Step 4: Disable Streaming in the Model  
```python
llm = ChatOpenAI(model="o1-preview", temperature=1, disable_streaming=True)

graph_builder = StateGraph(MessagesState)

def chatbot(state: MessagesState):
    return {"messages": [llm.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

graph = graph_builder.compile()
```
📌 **What Changed?**  
We added `disable_streaming=True` while defining the `ChatOpenAI` model.  
This ensures that the model **never** tries to stream.  

---

### 🔹 Step 5: Run the Chatbot Again  
```python
input = {"messages": {"role": "user", "content": "how many r's are in strawberry?"}}

async for event in graph.astream_events(input, version="v2"):
    if event["event"] == "on_chat_model_end":
        print(event["data"]["output"].content, end="", flush=True)
```
📌 **Now, it works fine!**  
✅ **Output:**  
```
There are three "r"s in the word "strawberry".
```

---

## 🌍 Real-World Use Case  
### 🔹 AI Chatbots & Virtual Assistants  
- Many chatbots use **streaming responses** for a better user experience.  
- If a model **doesn’t support streaming**, users might **face errors** or slow responses.  
- **Solution:** Disabling streaming ensures that the chatbot works **without issues**.  

### 🔹 Live Transcription Services  
- Some **AI models stream transcripts** for real-time speech-to-text applications.  
- If the model doesn’t support streaming, **disabling streaming** prevents system crashes.

---

## 🏆 Key Takeaways  
✅ Some models **don’t support streaming**, causing errors.  
✅ We can **disable streaming** by setting `disable_streaming=True`.  
✅ This helps in **avoiding errors** and ensures smooth chatbot execution.  
✅ Useful for **chatbots, transcription services, and AI assistants**.  

---

🎯 **Now, your chatbot will work seamlessly, even if the model doesn’t support streaming!** 🚀