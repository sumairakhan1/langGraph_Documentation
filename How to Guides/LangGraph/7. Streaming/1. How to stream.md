# 1. How to stream

# 📡 How to Stream: A Beginner's Guide with Code Examples  

Streaming is an essential technique in modern applications, particularly when working with **Large Language Models (LLMs)**. Instead of waiting for the entire response to be generated, **streaming allows us to display data progressively**, improving **user experience (UX)** and reducing latency.  

LangGraph, a framework for **graph-based workflows with LLMs**, has built-in support for streaming. This guide will explain **what streaming is, different ways to stream in LangGraph, and provide code examples with detailed explanations**.  

---

## 📌 **1. What is Streaming?**  
Streaming refers to **processing and sending data in chunks** rather than waiting for all computations to finish. In LLM-based applications, this is particularly useful to **show responses in real-time** instead of making users wait for the entire output.  

### ✅ **Real-World Example:**  
Imagine you're using **ChatGPT** to generate a long paragraph. Instead of waiting for the entire response to be ready, **words start appearing progressively**. This is **streaming in action**, making your experience faster and smoother.  

---

## ⚡ **2. Streaming Modes in LangGraph**  
LangGraph offers several ways to stream data from a graph-based LLM workflow:  

### 🔹 **1. values** – Streams **all values in the state** after each step.  
### 🔹 **2. updates** – Streams **only the changes** made by nodes after each step.  
### 🔹 **3. custom** – Streams **custom data** from inside nodes using `StreamWriter`.  
### 🔹 **4. messages** – Streams **LLM-generated messages token-by-token**.  
### 🔹 **5. debug** – Streams **debug events**, providing detailed execution information.  

Let's dive into **code examples** to understand how these work!  

---

## 🛠 **3. Setting Up LangGraph**  

Before we start coding, install the required dependencies:  

```python
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_openai
```

Then, set up an **API key** for OpenAI models:  

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

### 🔍 **What does this code do?**
- It **checks** if the `OPENAI_API_KEY` is already set.  
- If **not set**, it prompts the user to enter the API key securely.  
- The entered key is stored as an environment variable.  

---

## 🔄 **4. Defining a Simple Graph**  

Let's define a **basic LangGraph workflow** with two nodes:  

1️⃣ **`refine_topic`** – Adds "and cats" to the topic.  
2️⃣ **`generate_joke`** – Creates a joke based on the refined topic.  

### 📝 **Code Example:**
```python
from typing import TypedDict
from langgraph.graph import StateGraph, START

# Define the state structure
class State(TypedDict):
    topic: str
    joke: str

# Function to refine the topic
def refine_topic(state: State):
    return {"topic": state["topic"] + " and cats"}

# Function to generate a joke
def generate_joke(state: State):
    return {"joke": f"This is a joke about {state['topic']}"}

# Creating the graph
graph = (
    StateGraph(State)
    .add_node(refine_topic)
    .add_node(generate_joke)
    .add_edge(START, "refine_topic")
    .add_edge("refine_topic", "generate_joke")
    .compile()
)
```

### 🔍 **Explanation:**
- `State(TypedDict)`: Defines the **structure of our state** (with `topic` and `joke`).  
- `refine_topic(state)`: Modifies the topic by **adding "and cats"**.  
- `generate_joke(state)`: Generates a **joke using the topic**.  
- **Graph Construction**:
  - Adds the two nodes.
  - Connects **`START → refine_topic → generate_joke`**.
  - Finally, **compiles the graph**.

---

## 📢 **5. Streaming Examples**  

### 🎯 **1. Streaming All State Values (`values`)**  
This mode **streams the entire state** after each step.

```python
for chunk in graph.stream({"topic": "ice cream"}, stream_mode="values"):
    print(chunk)
```

### 📝 **Output:**
```
{'topic': 'ice cream'}
{'topic': 'ice cream and cats'}
{'topic': 'ice cream and cats', 'joke': 'This is a joke about ice cream and cats'}
```

### 🔍 **Explanation:**
- Each step **updates the state**, and the full state is printed **after every step**.  

---

### 🛠 **2. Streaming Only Updates (`updates`)**  
Instead of streaming the full state, **only changes are streamed**.

```python
for chunk in graph.stream({"topic": "ice cream"}, stream_mode="updates"):
    print(chunk)
```

### 📝 **Output:**
```
{'refine_topic': {'topic': 'ice cream and cats'}}
{'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}
```

### 🔍 **Explanation:**
- Only **updates made by nodes** (`refine_topic` and `generate_joke`) are streamed.  

---

### 🐞 **3. Debugging the Process (`debug`)**  
This mode **provides detailed logs** for each step.

```python
for chunk in graph.stream({"topic": "ice cream"}, stream_mode="debug"):
    print(chunk)
```

### 📝 **Sample Output:**
```
{'type': 'task', 'step': 1, 'payload': {'name': 'refine_topic', 'input': {'topic': 'ice cream'}}}
{'type': 'task_result', 'step': 1, 'payload': {'name': 'refine_topic', 'result': [('topic', 'ice cream and cats')]}}
...
```

### 🔍 **Explanation:**
- It logs **each task execution** (input, output, timestamps).  
- Helps **debug complex workflows**.  

---

### 🔤 **4. Streaming LLM Messages (`messages`)**  
This mode streams **LLM-generated text token-by-token**.

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

def generate_joke(state: State):
    llm_response = llm.invoke(
        [{"role": "user", "content": f"Generate a joke about {state['topic']}"}]
    )
    return {"joke": llm_response.content}

graph = (
    StateGraph(State)
    .add_node(refine_topic)
    .add_node(generate_joke)
    .add_edge(START, "refine_topic")
    .add_edge("refine_topic", "generate_joke")
    .compile()
)

for message_chunk, metadata in graph.stream(
    {"topic": "ice cream"},
    stream_mode="messages",
):
    if message_chunk.content:
        print(message_chunk.content, end="|", flush=True)
```

### 📝 **Sample Output:**
```
Why| did| the| cat| sit| on| the| ice| cream| cone|?|
Because| it| wanted| to| be| a| "|purr-fect|"| scoop!|
```

### 🔍 **Explanation:**
- **LLM responses are streamed token-by-token** for a real-time experience.  
- `flush=True` ensures smooth **continuous output**.  

---

### 🔧 **5. Combining Multiple Streaming Modes**  
We can **combine multiple streaming modes** for more flexibility.

```python
for stream_mode, chunk in graph.stream(
    {"topic": "ice cream"},
    stream_mode=["updates", "custom"],
):
    print(f"Stream mode: {stream_mode}")
    print(chunk)
    print("\n")
```

### 🔍 **Explanation:**
- Combines **updates** (state changes) and **custom logs**.  

---

## 🎯 **Conclusion**  
✅ Streaming improves **user experience** by reducing latency.  
✅ LangGraph provides multiple **streaming modes** for different needs.  
✅ Code examples show how to **stream efficiently in LangGraph**.  

---

🔥 **What’s Next?**  
- Try modifying the code to add **more nodes**!  
- Experiment with **different LLM models**.  
- Use **LangSmith** for advanced debugging.  

Happy coding! 🚀


---
# o3 mini:

# 🌟 Streaming in LangGraph: A Beginner's Guide

Streaming is a technique that allows your application to send or display data **progressively** rather than waiting for the entire output to be ready. This is especially important when working with large language models (LLMs) or any process that takes time to compute a complete response. Imagine watching a movie that loads a few seconds ahead rather than waiting for the whole film to download—that's the essence of streaming!

---

## 🚀 What is Streaming?

When you stream output, your application sends chunks of data as soon as they are available. This provides a **smoother and more responsive user experience (UX)** because users start seeing results immediately instead of waiting for everything to finish.

### Real-World Example

- **Chat Applications:** Think of messaging apps where messages appear one after another. When you type in a question, you might see the response gradually appear token by token, making the conversation feel more interactive.
- **Live Video/Audio:** Just as live video or radio streams send data continuously, streaming text responses makes the interaction feel "live" even if the data is generated on the fly.

---

## 🔍 Streaming Modes in LangGraph

LangGraph offers various streaming modes that you can choose based on your needs:

1. **Values Mode ("values")**  
   - **What it does:** Emits the entire state after each processing step.
   - **When to use:** When you want a complete picture of the state at every stage.

2. **Updates Mode ("updates")**  
   - **What it does:** Emits only the updates or changes made by each node.
   - **When to use:** To track what each node changes without showing the entire state.

3. **Custom Mode ("custom")**  
   - **What it does:** Allows nodes to send custom data using a `StreamWriter`.
   - **When to use:** When you need to send specialized or extra data from inside nodes.

4. **Messages Mode ("messages")**  
   - **What it does:** Streams LLM messages token-by-token, including metadata.
   - **When to use:** When you want to see the fine-grained process of text generation.

5. **Debug Mode ("debug")**  
   - **What it does:** Streams detailed debug events, including tasks and their results.
   - **When to use:** For in-depth troubleshooting and performance analysis.

6. **Combining Modes**  
   - **What it does:** You can combine multiple streaming modes to get several types of outputs at once.
   - **When to use:** When you need multiple perspectives (e.g., both updates and custom data).

---

## 🛠️ Code Examples Explained

Let's break down some code examples that illustrate how to set up and use streaming in LangGraph.

### 1. **Setting Up the Environment**

```python
# Import necessary modules
import getpass
import os

# Function to set environment variables securely
def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

# Set the OpenAI API key securely
_set_env("OPENAI_API_KEY")
```

**Explanation:**
- **Imports:**  
  - `getpass` is used to securely input passwords or API keys.
  - `os` is used to interact with environment variables.
- **Function `_set_env`:**  
  - Checks if an environment variable (like `OPENAI_API_KEY`) is already set.
  - If not, it prompts the user to input it securely.
- **Usage:**  
  - The API key is necessary for making calls to the OpenAI service.

---

### 2. **Defining a Simple Graph**

```python
from typing import TypedDict
from langgraph.graph import StateGraph, START

# Define a type for our state dictionary
class State(TypedDict):
    topic: str
    joke: str

# Node function to refine the topic
def refine_topic(state: State):
    return {"topic": state["topic"] + " and cats"}

# Node function to generate a joke based on the refined topic
def generate_joke(state: State):
    return {"joke": f"This is a joke about {state['topic']}"}

# Build the graph: set up nodes and connect them with edges
graph = (
    StateGraph(State)
    .add_node(refine_topic)           # Add first node
    .add_node(generate_joke)          # Add second node
    .add_edge(START, "refine_topic")    # Start -> refine_topic
    .add_edge("refine_topic", "generate_joke")  # refine_topic -> generate_joke
    .compile()                        # Compile the graph
)
```

**Explanation:**
- **Defining `State`:**  
  - We use a `TypedDict` to specify that our state will have a `topic` and a `joke`.
- **Node Functions:**
  - `refine_topic`: Adds extra detail ("and cats") to the topic.
  - `generate_joke`: Creates a joke based on the refined topic.
- **Building the Graph:**
  - `StateGraph(State)`: Initializes a graph with the given state type.
  - `.add_node(...)`: Adds our processing functions as nodes.
  - `.add_edge(...)`: Connects nodes in the order they should execute.
  - `.compile()`: Finalizes the graph structure for execution.

---

### 3. **Streaming with Different Modes**

#### a. **Stream All Values (Mode: "values")**

```python
for chunk in graph.stream(
    {"topic": "ice cream"},
    stream_mode="values",
):
    print(chunk)
```

**Explanation:**
- **Inputs:**  
  - The initial state is set with `{"topic": "ice cream"}`.
- **stream_mode="values":**  
  - This mode outputs the entire state after each node processes the data.
- **Output:**  
  - You will see progressive updates:
    1. Initial state: `{'topic': 'ice cream'}`
    2. After refining: `{'topic': 'ice cream and cats'}`
    3. Final state: `{'topic': 'ice cream and cats', 'joke': 'This is a joke about ice cream and cats'}`

---

#### b. **Stream Updates Only (Mode: "updates")**

```python
for chunk in graph.stream(
    {"topic": "ice cream"},
    stream_mode="updates",
):
    print(chunk)
```

**Explanation:**
- **stream_mode="updates":**  
  - This mode only emits the changes made by each node.
- **Output:**  
  - You will see only the parts of the state that are updated:
    1. `{'refine_topic': {'topic': 'ice cream and cats'}}`
    2. `{'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}`

---

#### c. **Stream Debug Events (Mode: "debug")**

```python
for chunk in graph.stream(
    {"topic": "ice cream"},
    stream_mode="debug",
):
    print(chunk)
```

**Explanation:**
- **stream_mode="debug":**  
  - Provides detailed internal information, such as timestamps, task IDs, and results.
- **Use Case:**  
  - Very useful during development to understand the sequence of events and debug issues.

---

#### d. **Stream LLM Tokens (Mode: "messages")**

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

def generate_joke(state: State):
    llm_response = llm.invoke(
        [{"role": "user", "content": f"Generate a joke about {state['topic']}"}]
    )
    return {"joke": llm_response.content}

graph = (
    StateGraph(State)
    .add_node(refine_topic)
    .add_node(generate_joke)
    .add_edge(START, "refine_topic")
    .add_edge("refine_topic", "generate_joke")
    .compile()
)

for message_chunk, metadata in graph.stream(
    {"topic": "ice cream"},
    stream_mode="messages",
):
    if message_chunk.content:
        print(message_chunk.content, end="|", flush=True)
```

**Explanation:**
- **LLM Setup:**  
  - Import and initialize a ChatOpenAI model to generate jokes.
- **Modified `generate_joke`:**  
  - Uses the LLM to generate a joke based on the input topic.
- **stream_mode="messages":**  
  - Streams the response token-by-token along with metadata (e.g., node name, model details).
- **Output:**  
  - You see the joke being built gradually, e.g., `Why| did| the| cat| sit| on| the| ice| cream| cone|?...`

---

#### e. **Stream Custom Data (Mode: "custom")**

```python
from langgraph.types import StreamWriter

def generate_joke(state: State, writer: StreamWriter):
    # Send custom data during processing
    writer({"custom_key": "Writing custom data while generating a joke"})
    return {"joke": f"This is a joke about {state['topic']}"}

graph = (
    StateGraph(State)
    .add_node(refine_topic)
    .add_node(generate_joke)
    .add_edge(START, "refine_topic")
    .add_edge("refine_topic", "generate_joke")
    .compile()
)

for chunk in graph.stream(
    {"topic": "ice cream"},
    stream_mode="custom",
):
    print(chunk)
```

**Explanation:**
- **StreamWriter:**  
  - A helper that lets you send extra, custom data from within your node function.
- **Custom Data Emission:**  
  - Before generating the joke, the node writes a custom message.
- **stream_mode="custom":**  
  - Outputs this custom data, allowing you to monitor or log specific events.

---

#### f. **Combining Multiple Streaming Modes**

```python
for stream_mode, chunk in graph.stream(
    {"topic": "ice cream"},
    stream_mode=["updates", "custom"],
):
    print(f"Stream mode: {stream_mode}")
    print(chunk)
    print("\n")
```

**Explanation:**
- **Multiple Modes:**  
  - The graph streams outputs for both `updates` and `custom` modes.
- **Tuple Output:**  
  - Each output is a tuple where the first element indicates the mode and the second is the actual data.
- **Output Example:**  
  - You might see:
    ```
    Stream mode: updates
    {'refine_topic': {'topic': 'ice cream and cats'}}

    Stream mode: custom
    {'custom_key': 'Writing custom data while generating a joke'}

    Stream mode: updates
    {'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}
    ```

---

## 🔗 Bringing It All Together

Streaming in LangGraph allows you to:
- **Improve UX:** By sending immediate feedback to the user.
- **Debug Efficiently:** With modes like `debug` that reveal inner workings.
- **Tailor Output:** With custom streaming modes that let you emit exactly the data you need.

By understanding and using these streaming modes, you can create responsive, interactive applications that make working with LLMs much more engaging and efficient.

Happy coding! 🚀