# 4. How to stream data from within a tool


# ğŸ“¡ How to Stream Data from Within a Tool  

Streaming data from within a tool is useful when you want to provide real-time updates while executing a long-running task. This is especially important when calling tools that interact with Large Language Models (LLMs) or external APIs.  

In this guide, we'll cover:  
âœ… **What streaming from within a tool means**  
âœ… **How to implement it using `stream_mode="custom"`**  
âœ… **Real-world applications of this concept**  
âœ… **Step-by-step code explanation**  

---

## ğŸ§ What Does Streaming from Within a Tool Mean?  

When calling a tool that performs a complex or time-consuming task (like fetching data from an API or generating LLM responses), you may not want to wait for the entire process to complete before getting output. Instead, you can **stream partial results** as soon as they become available.  

**Example Scenario:**  
Imagine you're building an AI assistant that retrieves information from a database. Instead of waiting for the entire query to complete, the assistant can **stream partial results** as they are fetched.  

---
  
## ğŸ› ï¸ How to Implement Streaming from Within a Tool  

LangGraph provides a way to stream data from inside a tool using `stream_mode="custom"` and `get_stream_writer()`.  

### ğŸ”¹ Key Components  
1. **`get_stream_writer()`** â†’ Used to send data chunks as they are generated.  
2. **`stream_mode="custom"`** â†’ Enables streaming for any kind of data.  
3. **`async for chunk in agent.astream()`** â†’ Streams the data asynchronously.  

---

## ğŸ”¥ Real-World Use Cases  

ğŸ“Œ **Chatbots** â€“ Stream responses as they are generated instead of waiting for full completion.  
ğŸ“Œ **Data Fetching** â€“ Stream real-time data from APIs or databases.  
ğŸ“Œ **Live Monitoring** â€“ Stream logs from a background process.  

---

## ğŸš€ Code Implementation with Explanation  

### ğŸ“Œ Step 1: Install Dependencies & Set Up API Key  

We need to install LangGraph and LangChain-OpenAI.  

```python
# Install required packages
!pip install -U langgraph langchain-openai

import getpass
import os

# Function to set environment variable
def _set_env(var: str):
    if not os.environ.get(var):  
        os.environ[var] = getpass.getpass(f"{var}: ")

# Set OpenAI API key
_set_env("OPENAI_API_KEY")
```

ğŸ” **Explanation:**  
- `!pip install -U langgraph langchain-openai` â†’ Installs required libraries.  
- `_set_env("OPENAI_API_KEY")` â†’ Sets the OpenAI API key securely.  

---

### ğŸ“Œ Step 2: Define the Streaming Tool  

Now, let's create a tool that streams a list of office items using `get_stream_writer()`.  

```python
from langchain_core.tools import tool
from langgraph.config import get_stream_writer
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

# Define a tool with streaming
@tool
async def get_items(place: str) -> str:
    """Use this tool to list items in a specified place."""
    
    writer = get_stream_writer()  # Initialize stream writer
    
    # Sample data to stream
    items = ["books", "pencils", "pictures"]
    
    # Stream data in chunks
    for chunk in items:
        writer({"custom_tool_data": chunk})  # Send each item as it is processed
    
    return ", ".join(items)  # Return full result at the end
```

ğŸ” **Explanation:**  
- `@tool` â†’ Declares this function as a LangChain tool.  
- `writer = get_stream_writer()` â†’ Initializes a stream writer.  
- `for chunk in items: writer({"custom_tool_data": chunk})` â†’ Streams data one item at a time.  
- `return ", ".join(items)` â†’ Returns the final concatenated result.  

---

### ğŸ“Œ Step 3: Create the AI Agent  

We now create an agent that will use this streaming tool.  

```python
# Initialize the LLM
llm = ChatOpenAI(model_name="gpt-4o-mini")

# List of tools (we only have one for now)
tools = [get_items]

# Create an agent with tool support
agent = create_react_agent(llm, tools=tools)
```

ğŸ” **Explanation:**  
- `ChatOpenAI(model_name="gpt-4o-mini")` â†’ Loads an OpenAI model.  
- `tools = [get_items]` â†’ Registers the streaming tool.  
- `create_react_agent(llm, tools=tools)` â†’ Creates an agent that can call tools.  

---

### ğŸ“Œ Step 4: Stream Data Using the Agent  

Now, we can send a query to the agent and stream the results.  

```python
# Define input message
inputs = {
    "messages": [  
        {"role": "user", "content": "what items are in the office?"}
    ]
}

# Stream the response
async for chunk in agent.astream(inputs, stream_mode="custom"):
    print(chunk)
```

ğŸ” **Explanation:**  
- `inputs = {...}` â†’ Defines the user's query.  
- `async for chunk in agent.astream(inputs, stream_mode="custom"):` â†’ Asynchronously streams the response.  
- `print(chunk)` â†’ Prints each streamed chunk.  

---

### ğŸ“Œ Expected Output  

```
{'custom_tool_data': 'books'}
{'custom_tool_data': 'pencils'}
{'custom_tool_data': 'pictures'}
```

âœ… **Instead of waiting for the full response, we get each item as soon as itâ€™s processed!**  

---

## ğŸ¯ Summary  

ğŸ”¹ **Streaming from within a tool** allows real-time updates while executing a task.  
ğŸ”¹ **`get_stream_writer()`** helps send partial results during execution.  
ğŸ”¹ **Useful in chatbots, live monitoring, and real-time data fetching.**  
ğŸ”¹ **Step-by-step implementation provided with code and explanations.**  

Would you like to see more examples or modifications for your use case? ğŸ˜Š

---


# ğŸ“Œ **Streaming Data from a Tool in LangGraph: A Beginner-Friendly Guide**  

Streaming data from a tool is an essential feature when working with LLM-based applications, especially when tools take time to generate results. By streaming data, users get real-time updates instead of waiting for the tool to finish execution.

This guide will cover:  
âœ… **What streaming means in this context**  
âœ… **How to implement streaming with LangGraph**  
âœ… **Real-world applications**  
âœ… **Step-by-step code walkthrough with explanations**  

---  
## ğŸ¯ **What is Streaming in LLM-Based Tools?**  
Streaming in LLM-based tools means sending data **incrementally** instead of waiting for the entire response to be ready. This is useful when calling APIs like OpenAIâ€™s chat models or running tools that process data over time.  

**Example:**  
- Imagine you are building a chatbot that fetches product details from a slow database.  
- Instead of making the user wait for 10 seconds, you **stream results one by one**, displaying partial information.  

---

## ğŸ”¥ **Real-World Example: Where is Streaming Used?**  
âœ… **Chatbots & Virtual Assistants:** ChatGPT and Google Assistant use streaming to show partial responses in real-time.  
âœ… **Stock Market Apps:** Live stock price updates are streamed instead of waiting for a complete report.  
âœ… **Real-Time Translation:** Live subtitles for meetings or videos use streaming for immediate translations.  
âœ… **AI-Powered Search Engines:** Google Search suggests results as you type, using streaming.  

---

# ğŸš€ **Implementing Streaming in LangGraph**  
Weâ€™ll explore two ways to stream data from a tool:  
1ï¸âƒ£ **Streaming Arbitrary Data** (Custom Data Streaming)  
2ï¸âƒ£ **Streaming LLM Tokens** (LLM Response Streaming)  

---

## ğŸ›  **1ï¸âƒ£ Streaming Arbitrary Data from a Tool**  
This method streams **custom tool data** instead of waiting for the full response.

### **ğŸ“Œ Code Example: Streaming Custom Data**
```python
from langgraph.config import get_stream_writer
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

@tool
async def get_items(place: str) -> str:
    """Use this tool to list items one might find in a place you're asked about."""
    writer = get_stream_writer()  # Get the streaming writer

    # Example: Streaming a list of items found in a given place
    items = ["books", "pencils", "pictures"]
    for chunk in items:
        writer({"custom_tool_data": chunk})  # Send each item as a stream

    return ", ".join(items)  # Return full result after streaming is complete

# Initialize the LLM model
llm = ChatOpenAI(model_name="gpt-4o-mini")

# Setup tool execution
tools = [get_items]
agent = create_react_agent(llm, tools=tools)

# Input request
inputs = {
    "messages": [  
        {"role": "user", "content": "what items are in the office?"}
    ]
}

# Stream output
async for chunk in agent.astream(inputs, stream_mode="custom"):
    print(chunk)  
```

---

### ğŸ§ **Code Explanation**
1. **`get_stream_writer()`**: Initializes a writer that allows us to send partial data.  
2. **Loop through `items`**: Instead of returning all at once, we stream items one by one.  
3. **`writer({"custom_tool_data": chunk})`**: Sends each item as a separate message.  
4. **Final return statement**: Once streaming is complete, we return the full list as a final response.  
5. **`agent.astream(..., stream_mode="custom")`**: Calls the tool and starts streaming data.

âœ… **Expected Output:**  
```
{'custom_tool_data': 'books'}
{'custom_tool_data': 'pencils'}
{'custom_tool_data': 'pictures'}
```

---

## ğŸ¤– **2ï¸âƒ£ Streaming LLM Tokens from a Tool**  
Instead of streaming tool data, we can stream **tokens** generated by the LLM itself.

### **ğŸ“Œ Code Example: Streaming LLM Responses**
```python
from langchain_core.messages import AIMessageChunk
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI

@tool
async def get_items(place: str, config: RunnableConfig) -> str:
    """Generate a list of items found in a given place using an LLM."""
    response = await llm.ainvoke(
        [
            {
                "role": "user",
                "content": (
                    f"Can you tell me what kind of items i might find in '{place}'?"
                ),
            }
        ],
        config,  # Ensures proper context propagation
    )
    return response.content  # Return the response from the LLM

# Initialize LLM
llm = ChatOpenAI(model_name="gpt-4o-mini")

tools = [get_items]
agent = create_react_agent(llm, tools=tools)

# Input request
inputs = {
    "messages": [  
        {"role": "user", "content": "what items are in the bedroom?"}
    ]
}

# Stream LLM responses
async for msg, metadata in agent.astream(inputs, stream_mode="messages"):
    if isinstance(msg, AIMessageChunk) and msg.content and metadata["langgraph_node"] == "tools":
        print(msg.content, end="|", flush=True)
```

---

### ğŸ§ **Code Explanation**
1. **Streaming AI Tokens**  
   - Unlike the previous example, here we **stream AI-generated text** instead of pre-defined items.  
   - `llm.ainvoke(...)` calls the LLM and gets a response asynchronously.  

2. **Handling Messages Efficiently**  
   - We use `agent.astream(..., stream_mode="messages")` to stream LLM responses.  
   - `AIMessageChunk` helps break responses into smaller parts, so we can process them one by one.  
   - **Final result is displayed word by word instead of waiting for the full response.**

âœ… **Expected Output:**  
```
Sure|!| Here| are| three| items| you| might| find| in| a| bedroom|:
1.| **Bed**| -| Used| for| sleeping| and| relaxing|.
2.| **Dresser**| -| Stores| clothes| and| personal| belongings|.
3.| **Nightstand**| -| Holds| a| lamp|,| books|,| and| an| alarm| clock|.
```

---

# âš¡ **Example Without LangChain (For Full Control)**  
If you donâ€™t want to use LangChain, you can directly use OpenAIâ€™s API.

### **ğŸ“Œ Code Example: Direct OpenAI Streaming**
```python
import json
import operator
from typing import TypedDict
from typing_extensions import Annotated
from langgraph.graph import StateGraph, START
from openai import AsyncOpenAI

openai_client = AsyncOpenAI()

async def stream_tokens(model_name: str, messages: list[dict]):
    """Streams LLM tokens directly from OpenAI."""
    response = await openai_client.chat.completions.create(
        messages=messages, model=model_name, stream=True
    )
    async for chunk in response:
        yield {"role": "assistant", "content": chunk.choices[0].delta.content}

async def get_items(place: str):
    """Generates a list of items using OpenAI API."""
    async for msg_chunk in stream_tokens(
        "gpt-4o-mini",
        [{"role": "user", "content": f"What items are in '{place}'?"}]
    ):
        print(msg_chunk["content"], end="|", flush=True)

await get_items("bedroom")
```

âœ… **This works without LangChain but still streams tokens efficiently.**  

---

# ğŸ¯ **Final Thoughts**
âœ… **Streaming improves user experience by displaying real-time updates.**  
âœ… **Custom Data Streaming** is useful for sending partial results from a tool.  
âœ… **LLM Token Streaming** helps when calling OpenAI or similar models.  
âœ… **LangChain makes it easier, but direct API calls also work.**  

---

# ğŸ¯ **Which Method Should You Use?**
| **Use Case** | **Best Approach** |
|-------------|----------------|
| Streaming items from a database or API | **Custom Data Streaming** |
| Streaming responses from an LLM (like ChatGPT) | **LLM Token Streaming** |
| Full control without LangChain | **Direct OpenAI API** |

ğŸš€ **Which method do you need help with? Let me know!** ğŸ˜Š