# 3. How to stream LLM tokens from specific nodes

# üöÄ Streaming LLM Tokens from Specific Nodes

When your LangGraph workflow contains multiple nodes that each make LLM calls, you might only want to stream tokens from **specific nodes**. For example, if one node writes a joke and another writes a poem, you may choose to display tokens only from the poem node. This is possible by filtering the streamed tokens using the `langgraph_node` field found in the metadata.

---

## üîç Why Filter by Node?

**Real-World Example:**  
Imagine you are building a multi-functional chatbot that both tells jokes and composes poems simultaneously. In your UI, you might want to update the poem section in real time without mixing in tokens from the joke generation. Filtering by node allows you to extract just the tokens you need from a particular part of your graph.

---

## üõ†Ô∏è Code Example Using LangGraph & LangChain

Below is an example showing how to stream tokens only from specific nodes by checking the `langgraph_node` field in the metadata.

### Example 1: Basic Node Filtering

```python
from langgraph.graph import StateGraph
from langchain_openai import ChatOpenAI

# Initialize the ChatOpenAI model instance
model = ChatOpenAI()

# Define two example nodes that each invoke the LLM
def node_a(state):
    # Node A might perform some LLM call here
    model.invoke([{"role": "user", "content": "Example prompt for node_a"}])
    # Other logic...
    
def node_b(state):
    # Node B might perform another LLM call here
    model.invoke([{"role": "user", "content": "Example prompt for node_b"}])
    # Other logic...

# Create the graph with both nodes (assume State is defined elsewhere)
graph = (
    StateGraph(dict)  # Using dict as a generic state placeholder
    .add_node(node_a)
    .add_node(node_b)
    .compile()
)

# Stream tokens from the graph using the "messages" mode
for msg, metadata in graph.stream(
    {"topic": "cats"},         # Example input state
    stream_mode="messages"     # Stream LLM tokens token-by-token
):
    # Filter tokens to only stream those coming from 'node_a'
    if metadata["langgraph_node"] == "node_a":
        print(msg, end=" | ", flush=True)
```

### Line-by-Line Explanation

- **Imports and Model Initialization:**
  ```python
  from langgraph.graph import StateGraph
  from langchain_openai import ChatOpenAI

  model = ChatOpenAI()
  ```
  - **Purpose:** Import the necessary classes and initialize the LLM model.

- **Defining Nodes:**
  ```python
  def node_a(state):
      model.invoke([{"role": "user", "content": "Example prompt for node_a"}])
      # Additional processing can be added here
  ```
  - **Purpose:** `node_a` calls the LLM with a specific prompt. In practice, this function would return or update state based on the response.

  ```python
  def node_b(state):
      model.invoke([{"role": "user", "content": "Example prompt for node_b"}])
      # Additional processing can be added here
  ```
  - **Purpose:** Similarly, `node_b` is another node that makes its own LLM call.

- **Building the Graph:**
  ```python
  graph = (
      StateGraph(dict)
      .add_node(node_a)
      .add_node(node_b)
      .compile()
  )
  ```
  - **Purpose:** Construct a LangGraph graph with both nodes. Here, `dict` is used as a generic state type.

- **Streaming and Filtering Tokens:**
  ```python
  for msg, metadata in graph.stream(
      {"topic": "cats"},
      stream_mode="messages"
  ):
      if metadata["langgraph_node"] == "node_a":
          print(msg, end=" | ", flush=True)
  ```
  - **`graph.stream(...)`:** Streams tokens in "messages" mode.
  - **Filtering:** Checks if the metadata field `langgraph_node` equals `"node_a"`. Only then will it print the token.
  - **Output:** Only tokens from `node_a` are printed, separated by `" | "`.

---

## üõ†Ô∏è Extended Example: Filtering Specific LLM Invocation

In this example, we have two nodes: one writes a joke and the other writes a poem. We will stream tokens only from the poem-writing node.

### Example 2: Filtering by Node Name ("write_poem")

```python
from typing import TypedDict
from langgraph.graph import START, StateGraph, MessagesState
from langchain_openai import ChatOpenAI

# Initialize our LLM model with a specific variant
model = ChatOpenAI(model="gpt-4o-mini")

# Define the state structure using TypedDict for clarity
class State(TypedDict):
    topic: str
    joke: str
    poem: str

# Define a node that writes a joke using the LLM
def write_joke(state: State):
    topic = state["topic"]
    joke_response = model.invoke(
        [{"role": "user", "content": f"Write a joke about {topic}"}]
    )
    return {"joke": joke_response.content}

# Define a node that writes a poem using the LLM
def write_poem(state: State):
    topic = state["topic"]
    poem_response = model.invoke(
        [{"role": "user", "content": f"Write a short poem about {topic}"}]
    )
    return {"poem": poem_response.content}

# Build the graph with both nodes running concurrently
graph = (
    StateGraph(State)
    .add_node(write_joke)
    .add_node(write_poem)
    .add_edge(START, "write_joke")
    .add_edge(START, "write_poem")
    .compile()
)

# Stream tokens from the graph, filtering only for the "write_poem" node
for msg, metadata in graph.stream(
    {"topic": "cats"},
    stream_mode="messages",
):
    if msg.content and metadata["langgraph_node"] == "write_poem":
        print(msg.content, end="|", flush=True)
```

### Detailed Explanation

- **Imports and Model Setup:**
  ```python
  from typing import TypedDict
  from langgraph.graph import START, StateGraph, MessagesState
  from langchain_openai import ChatOpenAI

  model = ChatOpenAI(model="gpt-4o-mini")
  ```
  - **Purpose:** Imports necessary modules, defines our LLM model, and sets up the environment.

- **Defining the State:**
  ```python
  class State(TypedDict):
      topic: str
      joke: str
      poem: str
  ```
  - **Purpose:** Specifies that our state will have three keys: `topic`, `joke`, and `poem`.

- **Node Function `write_joke`:**
  ```python
  def write_joke(state: State):
      topic = state["topic"]
      joke_response = model.invoke(
          [{"role": "user", "content": f"Write a joke about {topic}"}]
      )
      return {"joke": joke_response.content}
  ```
  - **Purpose:** This node uses the LLM to generate a joke based on the topic. It returns the joke in the state.

- **Node Function `write_poem`:**
  ```python
  def write_poem(state: State):
      topic = state["topic"]
      poem_response = model.invoke(
          [{"role": "user", "content": f"Write a short poem about {topic}"}]
      )
      return {"poem": poem_response.content}
  ```
  - **Purpose:** This node uses the LLM to generate a poem. The response is added to the state.

- **Graph Construction:**
  ```python
  graph = (
      StateGraph(State)
      .add_node(write_joke)
      .add_node(write_poem)
      .add_edge(START, "write_joke")
      .add_edge(START, "write_poem")
      .compile()
  )
  ```
  - **Purpose:** Builds the graph so that both `write_joke` and `write_poem` are executed concurrently when the graph is run.

- **Streaming and Filtering Tokens:**
  ```python
  for msg, metadata in graph.stream(
      {"topic": "cats"},
      stream_mode="messages",
  ):
      if msg.content and metadata["langgraph_node"] == "write_poem":
          print(msg.content, end="|", flush=True)
  ```
  - **Purpose:** Streams tokens in "messages" mode from the graph execution.
  - **Filtering Condition:** Checks if the metadata's `langgraph_node` equals `"write_poem"`, ensuring only poem tokens are printed.
  - **Output:** Tokens from the poem node are printed in real time, separated by `"|"`.

---

## üåü Recap

- **Filtering by Node:**  
  Use the `langgraph_node` field in the streamed metadata to isolate tokens from specific nodes.

- **Why It's Useful:**  
  Allows you to control which part of your application receives which streamed tokens, ideal for multi-functional or modular applications.

- **Real-World Use Case:**  
  In a multi-task chatbot, you might want separate sections updating with jokes and poems. Filtering ensures that each section only displays the relevant content.

By applying these filtering techniques, you can build highly interactive and modular applications where each component receives exactly the data it needs. Happy coding! üöÄ