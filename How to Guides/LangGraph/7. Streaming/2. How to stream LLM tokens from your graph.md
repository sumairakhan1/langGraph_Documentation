# 2. How to stream LLM tokens from your graph

# üöÄ Streaming LLM Tokens from Your Graph

When building LLM-powered applications, sometimes you want to see the output **as it's being generated** instead of waiting for the full response. This is especially useful for creating interactive experiences like live chat apps or real-time content generators. In LangGraph, you can stream individual tokens from LLM calls using the `stream_mode="messages"` option. Let's break this down step by step!

---

## üìù What Is Token Streaming?

**Token streaming** means that as the language model generates text, each piece (or token) is sent immediately to the application. Imagine watching a sentence being typed out character by character or word by word in a chat window rather than waiting for the full sentence to appear at once.

### Real-World Example

- **Live Chat Interfaces:** In chat applications, seeing the message being typed out gives a more dynamic and responsive feel.
- **Content Generation Tools:** While generating a story or article, you can show users the content in real-time, making the experience interactive.

---

## üîß Code Walkthrough: Streaming LLM Tokens

Below is an example that demonstrates how to stream tokens from two LLM calls (one for a joke and one for a poem) within a single node in LangGraph.

### üìå Code Example

```python
# Import necessary modules and classes
from typing import TypedDict
from langgraph.graph import START, StateGraph
from langchain_openai import ChatOpenAI

# Create two instances of ChatOpenAI with tags for filtering outputs
joke_model = ChatOpenAI(model="gpt-4o-mini", tags=["joke"])
poem_model = ChatOpenAI(model="gpt-4o-mini", tags=["poem"])

# Define the state using TypedDict for type safety
class State(TypedDict):
    topic: str
    joke: str
    poem: str

# Async function that makes two LLM calls: one for a joke and one for a poem
async def call_model(state, config):
    # Extract the topic from the state
    topic = state["topic"]
    print("Writing joke...")
    
    # Call the joke model asynchronously.
    # Note: For Python versions < 3.11, we need to explicitly pass the config.
    joke_response = await joke_model.ainvoke(
        [{"role": "user", "content": f"Write a joke about {topic}"}],
        config,  # Passing the configuration for async context propagation
    )
    
    print("\n\nWriting poem...")
    
    # Call the poem model asynchronously
    poem_response = await poem_model.ainvoke(
        [{"role": "user", "content": f"Write a short poem about {topic}"}],
        config,
    )
    
    # Return the combined results as a dictionary
    return {"joke": joke_response.content, "poem": poem_response.content}

# Build the execution graph by creating a state graph and adding the node
graph = StateGraph(State)\
    .add_node(call_model)\
    .add_edge(START, "call_model")\
    .compile()

# Asynchronously stream messages from the graph.
# Each iteration provides a tuple of (message chunk, metadata)
async for msg, metadata in graph.astream(
    {"topic": "cats"},  # Initial state with the topic "cats"
    stream_mode="messages",  # Use "messages" mode to stream token-by-token
):
    if msg.content:
        # Print each token with a separator '|' for clarity
        print(msg.content, end="|", flush=True)
```

---

## üßê Detailed Explanation

### 1. **Importing Modules and Setting Up Models**

- **Imports:**
  - `from typing import TypedDict`: Provides type hints for our state.
  - `from langgraph.graph import START, StateGraph`: Imports the essential components to build our processing graph.
  - `from langchain_openai import ChatOpenAI`: Imports the ChatOpenAI class to interact with the OpenAI API.

- **Creating LLM Instances:**
  ```python
  joke_model = ChatOpenAI(model="gpt-4o-mini", tags=["joke"])
  poem_model = ChatOpenAI(model="gpt-4o-mini", tags=["poem"])
  ```
  - We create two separate instances of the ChatOpenAI model. Each is tagged so that later, when streaming, you can filter or recognize which token came from which model.

### 2. **Defining the State**

- **TypedDict for State:**
  ```python
  class State(TypedDict):
      topic: str
      joke: str
      poem: str
  ```
  - This defines the structure of our state. It must contain a `topic` as input, and will eventually include a `joke` and a `poem`.

### 3. **Creating the Node Function (`call_model`)**

- **Asynchronous Node Function:**
  ```python
  async def call_model(state, config):
  ```
  - This function is asynchronous (using `async def`), meaning it can perform non-blocking operations like awaiting LLM responses.

- **Extracting the Topic:**
  ```python
  topic = state["topic"]
  print("Writing joke...")
  ```
  - We retrieve the topic from the state and print a message to indicate that the joke generation is starting.

- **Invoking the Joke Model:**
  ```python
  joke_response = await joke_model.ainvoke(
      [{"role": "user", "content": f"Write a joke about {topic}"}],
      config,
  )
  ```
  - **`await`** is used to pause the function until the asynchronous call completes.
  - **`ainvoke`** sends a prompt to the LLM and streams the response tokens.
  - The `config` parameter is passed to ensure proper context management, especially in Python versions before 3.11.

- **Invoking the Poem Model:**
  ```python
  print("\n\nWriting poem...")
  poem_response = await poem_model.ainvoke(
      [{"role": "user", "content": f"Write a short poem about {topic}"}],
      config,
  )
  ```
  - Similarly, the poem model is invoked after printing a status message.

- **Returning the Combined Results:**
  ```python
  return {"joke": joke_response.content, "poem": poem_response.content}
  ```
  - The function returns a dictionary containing the full content of the joke and poem.

### 4. **Building the Graph**

- **Graph Setup:**
  ```python
  graph = StateGraph(State)\
      .add_node(call_model)\
      .add_edge(START, "call_model")\
      .compile()
  ```
  - A `StateGraph` is created with our defined state.
  - The node (`call_model`) is added to the graph.
  - An edge is established from the start node (`START`) to our node (`call_model`), ensuring the correct execution order.
  - Finally, the graph is compiled to be ready for execution.

### 5. **Streaming the LLM Tokens**

- **Asynchronous Streaming:**
  ```python
  async for msg, metadata in graph.astream(
      {"topic": "cats"},
      stream_mode="messages",
  ):
      if msg.content:
          print(msg.content, end="|", flush=True)
  ```
  - **`graph.astream(...)`:** This function starts the asynchronous streaming process.
  - **Input State:** The initial state is set with the topic `"cats"`.
  - **`stream_mode="messages"`:** This mode ensures that tokens are streamed as they are generated.
  - **Looping through the Stream:**
    - Each iteration returns a tuple:
      - **`msg`:** A chunk (or token) from the LLM output.
      - **`metadata`:** A dictionary containing details like which node generated the token, model details, and more.
  - **Printing the Tokens:**  
    - `print(msg.content, end="|", flush=True)` prints each token followed by a separator (`|`) for visual clarity.

---

## üîó Putting It All Together

By streaming tokens from your graph, you create applications that:
- **Enhance Interactivity:** Users see content as it's generated.
- **Improve User Experience:** Especially in scenarios where waiting for a full response might feel slow.
- **Debug More Easily:** Metadata allows you to trace where each token comes from.

Whether you're building live chat interfaces or dynamic content generators, streaming LLM tokens can bring your application closer to real-time interactivity.

Happy coding! üöÄ

---

# üöÄ Filtering Specific LLM Invocation in LangGraph

When building LLM applications, you might have multiple LLM calls happening in your graph simultaneously. However, there are times when you want to **focus on one specific invocation** (for example, just the joke generation) and filter out the others (like poem generation). This improves clarity and lets you process or display only the relevant token stream.

> **Real-World Example:**  
> Imagine a live chat interface where a user asks a question. Behind the scenes, the system generates both a detailed explanation and a brief summary. If you only want to show the summary in one part of the UI, you can filter the tokens by their tags (e.g., `"summary"`) while ignoring the detailed explanation tokens.

---

## üõ†Ô∏è How Does Filtering Work?

LangGraph streams LLM tokens along with **metadata** that contains information about each token's origin. By adding **tags** when setting up your LLM models (for example, `"joke"` or `"poem"`), you can later filter the streamed tokens based on these tags.

There are two common scenarios:

1. **Using LangChain:**  
   You can stream messages using `stream_mode="messages"` and filter tokens based on metadata tags.

2. **Without LangChain:**  
   You stream tokens using `stream_mode="custom"`, manually manage token streaming, and include your own metadata.

---

## üîß Code Example 1: Filtering with LangChain

Below is an example that demonstrates how to filter tokens for a specific LLM invocation (in this case, filtering for tokens tagged as `"joke"`).

```python
# Iterate over the streamed tokens from the graph asynchronously.
async for msg, metadata in graph.astream(
    {"topic": "cats"},         # Initial state with the topic "cats"
    stream_mode="messages",    # Use "messages" mode to stream token-by-token
):
    # Check if the message has content and if its metadata contains the "joke" tag.
    if msg.content and "joke" in metadata.get("tags", []):
        # Print the token content followed by a separator for clarity.
        print(msg.content, end="|", flush=True)
```

### Line-by-Line Explanation

- **Line 1:**  
  ```python
  async for msg, metadata in graph.astream(
  ```  
  This starts an asynchronous loop to iterate over each token (and its metadata) as it streams from the graph.

- **Line 2:**  
  ```python
      {"topic": "cats"},         # Initial state with the topic "cats"
  ```  
  We provide the initial state for our graph, here setting the topic to `"cats"`.

- **Line 3:**  
  ```python
      stream_mode="messages",    # Use "messages" mode to stream token-by-token
  ```  
  We specify `stream_mode="messages"` so that tokens are streamed one by one along with their metadata.

- **Line 5:**  
  ```python
      if msg.content and "joke" in metadata.get("tags", []):
  ```  
  This line checks two conditions:  
  1. **`msg.content`:** Ensures there is actual text content in the token.  
  2. **`"joke" in metadata.get("tags", [])`:** Checks if the `"joke"` tag is present in the metadata tags (if no tags exist, it defaults to an empty list).

- **Line 7:**  
  ```python
          print(msg.content, end="|", flush=True)
  ```  
  If both conditions are met, the token content is printed, with a `"|"` separator for readability.  
  - **`end="|"`**: Prevents newlines between tokens, making it appear as a continuous stream.  
  - **`flush=True`**: Ensures the output is immediately written to the terminal.

---

## üîß Code Example 2: Filtering Without LangChain

If you prefer to work directly with the LLM provider API (i.e., without LangChain), you can use a custom streaming mode. This example demonstrates how to stream tokens with custom metadata and then filter them.

### Setting Up the Custom Stream

```python
from openai import AsyncOpenAI

# Initialize an asynchronous OpenAI client
openai_client = AsyncOpenAI()
model_name = "gpt-4o-mini"

# Function to stream tokens from the LLM provider directly.
async def stream_tokens(model_name: str, messages: list[dict]):
    # Send a request to the API with streaming enabled.
    response = await openai_client.chat.completions.create(
        messages=messages, model=model_name, stream=True
    )

    role = None
    # Iterate asynchronously over each chunk of the streamed response.
    async for chunk in response:
        delta = chunk.choices[0].delta

        # Capture the role (e.g., 'assistant') when it's first provided.
        if delta.role is not None:
            role = delta.role

        # Yield the token if it has content.
        if delta.content:
            yield {"role": role, "content": delta.content}
```

### Line-by-Line Explanation

- **Import & Initialization:**  
  ```python
  from openai import AsyncOpenAI
  openai_client = AsyncOpenAI()
  model_name = "gpt-4o-mini"
  ```  
  - Import the asynchronous OpenAI client and initialize it.  
  - Set the model name to be used.

- **Defining `stream_tokens`:**  
  ```python
  async def stream_tokens(model_name: str, messages: list[dict]):
  ```  
  This function is designed to stream tokens from the LLM provider. It accepts the model name and a list of message dictionaries.

- **API Request with Streaming:**  
  ```python
      response = await openai_client.chat.completions.create(
          messages=messages, model=model_name, stream=True
      )
  ```  
  - Sends a request to the OpenAI API with streaming enabled (`stream=True`).

- **Iterating Over the Stream:**  
  ```python
      async for chunk in response:
  ```  
  - Loops over each chunk returned by the API asynchronously.

- **Extracting Token Data:**  
  ```python
      delta = chunk.choices[0].delta
      if delta.role is not None:
          role = delta.role
      if delta.content:
          yield {"role": role, "content": delta.content}
  ```  
  - Extracts the `delta` object that contains token updates.  
  - Saves the role when provided.  
  - Yields each token (as a dictionary) if there is content.

### Integrating the Custom Stream into the Graph

```python
async def call_model(state, config, writer):
    topic = state["topic"]
    joke = ""
    poem = ""

    print("Writing joke...")
    # Stream tokens for the joke generation.
    async for msg_chunk in stream_tokens(
        model_name, [{"role": "user", "content": f"Write a joke about {topic}"}]
    ):
        joke += msg_chunk["content"]
        # Create metadata including the 'joke' tag.
        metadata = {**config["metadata"], "tags": ["joke"]}
        chunk_to_stream = (msg_chunk, metadata)
        writer(chunk_to_stream)

    print("\n\nWriting poem...")
    # Stream tokens for the poem generation.
    async for msg_chunk in stream_tokens(
        model_name, [{"role": "user", "content": f"Write a short poem about {topic}"}]
    ):
        poem += msg_chunk["content"]
        # Create metadata including the 'poem' tag.
        metadata = {**config["metadata"], "tags": ["poem"]}
        chunk_to_stream = (msg_chunk, metadata)
        writer(chunk_to_stream)

    return {"joke": joke, "poem": poem}

# Build the graph with the custom streaming node.
graph = StateGraph(State)\
    .add_node(call_model)\
    .add_edge(START, "call_model")\
    .compile()

# Use custom streaming mode.
stream_mode = "custom"

# Filtering streamed tokens based on tags (example: only "poem" tokens).
async for msg, metadata in graph.astream(
    {"topic": "cats"},
    stream_mode="custom",
):
    if "poem" in metadata.get("tags", []):
        print(msg["content"], end="|", flush=True)
```

### Detailed Explanation

- **`call_model` Function:**  
  - This async function handles both the joke and poem generation.
  - For each type of content, it calls `stream_tokens` to obtain the token stream.
  - It appends tokens to build the final joke and poem strings.
  - It also constructs metadata (with tags) and uses a `writer` callback to stream the token chunks.

- **Graph Construction:**  
  ```python
  graph = StateGraph(State).add_node(call_model).add_edge(START, "call_model").compile()
  ```  
  This sets up the processing graph with the custom `call_model` node.

- **Streaming with Custom Mode:**  
  ```python
  async for msg, metadata in graph.astream(
      {"topic": "cats"},
      stream_mode="custom",
  ):
  ```  
  We stream the tokens using `stream_mode="custom"`, which outputs our manually crafted token chunks and metadata.

- **Filtering the Output:**  
  ```python
  if "poem" in metadata.get("tags", []):
      print(msg["content"], end="|", flush=True)
  ```  
  - This line filters the tokens: it only prints the token if its metadata contains the `"poem"` tag.

---

## üí° Summary

- **Filtering LLM Invocations:**  
  You can filter token streams by checking metadata (such as tags) attached to each token.  
- **Real-World Benefit:**  
  This allows you to display or process only the parts of a multi-output system that you care about‚Äîfor example, showing just the summary in a complex chat response.
- **Implementation Options:**  
  - **With LangChain:** Use `stream_mode="messages"` and filter using metadata.
  - **Without LangChain:** Use `stream_mode="custom"` to have full control over the streamed data and metadata.

By understanding and applying these filtering techniques, you can build more focused and user-friendly LLM applications. Happy coding! üöÄ

---


# üöÄ Filter to Specific LLM Invocation

When streaming LLM tokens, you might receive tokens from **multiple LLM invocations**. For example, if your graph makes both a "joke" and a "poem" request, you may want to **filter the output** so that you only see tokens for one of these tasks. This is done by **tagging** each LLM invocation and then filtering the streamed tokens based on those tags.

Imagine you're developing a chatbot that not only tells jokes but also composes poems. If you want to display just the joke tokens while both are streaming, you'll need to filter out the tokens coming from the poem model. This ensures that the user sees only the desired content.

---

## üìå How It Works

### 1. **Filtering with LangChain**

You can use the metadata associated with each token stream to filter the output. Each token comes with metadata that includes tags. In our example, we add tags like `"joke"` or `"poem"` to differentiate the responses.

### Example Code with LangChain

```python
# Stream tokens from the graph and filter only the ones tagged as "joke"
async for msg, metadata in graph.astream(
    {"topic": "cats"},            # Initial state with the topic "cats"
    stream_mode="messages",       # Stream tokens as they are generated
):
    # Check if the token contains content and if its metadata tags include "joke"
    if msg.content and "joke" in metadata.get("tags", []):
        print(msg.content, end="|", flush=True)
```

---

### üîç Explanation: Line by Line

1. **Setting up the stream:**
   ```python
   async for msg, metadata in graph.astream(
       {"topic": "cats"},
       stream_mode="messages",
   ):
   ```
   - **`graph.astream(...)`:** Initiates an asynchronous stream from the graph.
   - **`{"topic": "cats"}`:** The initial state where the topic is set to "cats".
   - **`stream_mode="messages"`:** Specifies that the output should be streamed token-by-token, along with metadata.
   - **`async for msg, metadata in ...`**: Loops over each streamed token and its associated metadata.

2. **Filtering based on metadata:**
   ```python
   if msg.content and "joke" in metadata.get("tags", []):
   ```
   - **`msg.content`:** Checks if the current token chunk has content.
   - **`metadata.get("tags", [])`:** Retrieves the list of tags from metadata; if none are present, it defaults to an empty list.
   - **`"joke" in ...`:** Ensures that only tokens tagged as `"joke"` are processed.

3. **Outputting the filtered token:**
   ```python
   print(msg.content, end="|", flush=True)
   ```
   - **`print(..., end="|", flush=True)`:** Prints the token content followed by a separator (`|`) without adding a newline, ensuring a smooth streaming display.

---

## üåç Real-World Example

**Scenario:**  
Imagine you‚Äôre building an interactive entertainment app where a user can ask for multiple types of content. For instance, they might request a joke and a poem about "cats". While the backend processes both requests simultaneously, you want to display the joke immediately on the screen without waiting for the poem. By filtering with metadata tags, you can ensure that only the joke tokens are displayed in that particular section of your app.

---

## üîÑ Example Without LangChain

If you are not using LangChain, you can still stream tokens directly from the LLM provider API using custom streaming. Here‚Äôs how you can do it using the `openai` library's asynchronous client:

### Example Code Without LangChain

```python
from openai import AsyncOpenAI

# Initialize the async OpenAI client
openai_client = AsyncOpenAI()
model_name = "gpt-4o-mini"

# Function to stream tokens directly from the LLM provider
async def stream_tokens(model_name: str, messages: list[dict]):
    response = await openai_client.chat.completions.create(
        messages=messages, model=model_name, stream=True
    )
    role = None
    # Loop through each streamed chunk from the response
    async for chunk in response:
        delta = chunk.choices[0].delta
        if delta.role is not None:
            role = delta.role
        if delta.content:
            # Yield a dictionary with the role and the token content
            yield {"role": role, "content": delta.content}

# Node function to call the model and stream tokens for joke and poem
async def call_model(state, config, writer):
    topic = state["topic"]
    joke = ""
    poem = ""
    
    print("Writing joke...")
    # Stream tokens for the joke
    async for msg_chunk in stream_tokens(
        model_name, [{"role": "user", "content": f"Write a joke about {topic}"}]
    ):
        joke += msg_chunk["content"]
        # Build metadata with a "joke" tag
        metadata = {**config["metadata"], "tags": ["joke"]}
        chunk_to_stream = (msg_chunk, metadata)
        writer(chunk_to_stream)

    print("\n\nWriting poem...")
    # Stream tokens for the poem
    async for msg_chunk in stream_tokens(
        model_name, [{"role": "user", "content": f"Write a short poem about {topic}"}]
    ):
        poem += msg_chunk["content"]
        # Build metadata with a "poem" tag
        metadata = {**config["metadata"], "tags": ["poem"]}
        chunk_to_stream = (msg_chunk, metadata)
        writer(chunk_to_stream)

    return {"joke": joke, "poem": poem}

# Build the graph with our node function and compile it
graph = StateGraph(State).add_node(call_model).add_edge(START, "call_model").compile()
```

---

### üîç Explanation: Line by Line

1. **Initializing the OpenAI Client:**
   ```python
   from openai import AsyncOpenAI
   openai_client = AsyncOpenAI()
   model_name = "gpt-4o-mini"
   ```
   - **`AsyncOpenAI()`:** Creates an instance to make asynchronous API calls.
   - **`model_name`:** Sets the model you want to use.

2. **Streaming Tokens Function:**
   ```python
   async def stream_tokens(model_name: str, messages: list[dict]):
       response = await openai_client.chat.completions.create(
           messages=messages, model=model_name, stream=True
       )
       role = None
       async for chunk in response:
           delta = chunk.choices[0].delta
           if delta.role is not None:
               role = delta.role
           if delta.content:
               yield {"role": role, "content": delta.content}
   ```
   - **`stream_tokens`:** An async generator that streams tokens from the API.
   - **`await openai_client.chat.completions.create(...)`:** Calls the API with `stream=True` to get a streaming response.
   - **Loop over each chunk:** Iterates over the streamed response.
   - **`delta`:** Contains the changes in the message (either role or content).
   - **`yield`:** Outputs a dictionary with the token's role and content.

3. **Node Function `call_model`:**
   ```python
   async def call_model(state, config, writer):
       topic = state["topic"]
       joke = ""
       poem = ""
   ```
   - **Extracting `topic`:** Retrieves the topic from the state.
   - **Initialize `joke` and `poem`:** Prepare empty strings to accumulate tokens.

4. **Streaming and Tagging Tokens:**
   ```python
   async for msg_chunk in stream_tokens(
       model_name, [{"role": "user", "content": f"Write a joke about {topic}"}]
   ):
       joke += msg_chunk["content"]
       metadata = {**config["metadata"], "tags": ["joke"]}
       chunk_to_stream = (msg_chunk, metadata)
       writer(chunk_to_stream)
   ```
   - **Stream tokens for the joke:** Calls `stream_tokens` with the appropriate prompt.
   - **Accumulate tokens:** Adds each token to the `joke` string.
   - **Build metadata:** Merges existing metadata with a new `"tags": ["joke"]`.
   - **`writer(chunk_to_stream)`:** Streams the token and metadata to the output.

5. **Repeating for the Poem:**
   ```python
   async for msg_chunk in stream_tokens(
       model_name, [{"role": "user", "content": f"Write a short poem about {topic}"}]
   ):
       poem += msg_chunk["content"]
       metadata = {**config["metadata"], "tags": ["poem"]}
       chunk_to_stream = (msg_chunk, metadata)
       writer(chunk_to_stream)
   ```
   - The same process is applied for the poem, tagging tokens with `"poem"`.

6. **Graph Construction:**
   ```python
   graph = StateGraph(State).add_node(call_model).add_edge(START, "call_model").compile()
   ```
   - **`StateGraph(State)`:** Initializes the graph with the defined state.
   - **`.add_node(call_model)`:** Adds our custom node function.
   - **`.add_edge(START, "call_model")`:** Connects the start to our node.
   - **`.compile()`:** Finalizes the graph.

---

## üîÑ Filtering in Custom Mode

To filter tokens when using custom mode (without LangChain), you can apply the same metadata filtering technique:

```python
async for msg, metadata in graph.astream(
    {"topic": "cats"},
    stream_mode="custom",
):
    if "poem" in metadata.get("tags", []):
        print(msg["content"], end="|", flush=True)
```

- **`stream_mode="custom"`:** Specifies that the stream is coming from your custom implementation.
- **Filter condition:** Checks if the metadata contains the tag `"poem"` and prints only those tokens.

---

## üåç Real-World Example

**Scenario:**  
Imagine a multi-feature writing assistant where a user can request both a joke and a poem. You might want to display the joke in one section of the screen while the poem appears in another. Filtering tokens by tag allows you to direct the correct content to the right part of your user interface, ensuring a seamless and organized experience.

---

By using metadata to filter the streamed tokens, you gain precise control over what content is displayed. This method is particularly useful in applications that require **simultaneous processing of multiple tasks** while still providing a **clean and targeted user interface**.

Happy coding! üöÄ