# 5. How to review tool calls

# Human-in-the-Loop (HIL) and Review Tool Calls: A Beginner's Guide ü§ñ

In modern systems, **Human-in-the-loop (HIL)** interactions are essential for systems that involve decision-making. This concept is particularly crucial in **agentic systems**, where an AI or automated system might require human intervention at specific points to make better decisions or approve actions.

## What are Tool Calls? üõ†Ô∏è

Tool calls are actions that trigger external tools or functions. These can involve executing database queries, generating summaries, or other automated tasks that require human oversight. Here are a few examples of tool calls:

- **Executing SQL queries**: A system might call a database tool to fetch or update data.
- **Generating summaries**: After a process or function executes, the system might call a summarizing tool to create a concise summary of the result.

## Why Are Human-in-the-Loop Interactions Important? üí¨

Sometimes, an AI or automation might make decisions that aren't entirely accurate or may require human judgment to continue. This is where HIL interactions come into play. The system pauses execution and waits for human input to either:

1. **Approve the tool call and continue**: If the result looks correct, you can approve it.
2. **Modify the tool call manually and then continue**: You can tweak the parameters before continuing.
3. **Provide feedback**: You can give natural language feedback to guide the system on the next steps.

### Real-World Example üåç

Imagine an AI system that helps doctors diagnose medical conditions. The system might analyze medical data and then call a tool to execute certain tests. Before continuing with a treatment recommendation, the system might pause for the doctor to review the results, approve the action, or suggest modifications based on their expertise.

## Using Breakpoints in LangGraph üõë

In **LangGraph**, you can implement HIL using **breakpoints**. Breakpoints interrupt the execution of the graph at specific points, giving you the opportunity to review the state and take action based on human feedback.

### Example: Implementing a Breakpoint üìù

Let‚Äôs say you‚Äôre building a tool that generates reports based on user input. You can set up breakpoints to review tool calls before they are executed.

### Setting Up the Client üì±

Before we can interact with the graph, we first need to set up the client. This allows us to communicate with the hosted graph system.

Here‚Äôs how you do it in Python:

```python
from langgraph_sdk import get_client

# Create a client to communicate with the hosted graph
client = get_client(url=<DEPLOYMENT_URL>)

# Set the assistant ID and start a new thread for communication
assistant_id = "agent"
thread = await client.threads.create()
```

### Code Explanation:

- **`get_client(url=<DEPLOYMENT_URL>)`**: Initializes the client to communicate with the graph hosted at the specified URL.
- **`assistant_id = "agent"`**: Defines the assistant that will handle the interaction (in this case, an AI agent).
- **`thread = await client.threads.create()`**: Creates a new conversation thread to keep track of this particular interaction.

### Example with No Review üîÑ

If no tool calls are required (i.e., everything is happening automatically), the process will proceed without requiring any human intervention.

Here‚Äôs how it would look in Python:

```python
input = { 'messages':[{ "role":"user", "content":"hi!" }] }

async for chunk in client.runs.stream(
    thread["thread_id"],
    assistant_id,
    input=input,
    stream_mode="updates",
    interrupt_before=["action"],
):
    if chunk.data and chunk.event != "metadata": 
        print(chunk.data)
```

### Code Explanation:

- **`input = { 'messages':[{ "role":"user", "content":"hi!" }] }`**: This is the input from the user (e.g., a greeting).
- **`client.runs.stream()`**: This function streams the results from the graph, processing the conversation as it occurs.
- **`interrupt_before=["action"]`**: Ensures that the process can be paused before taking any action (e.g., calling an external tool).
- **`if chunk.data and chunk.event != "metadata"`**: Filters out unnecessary metadata and prints the relevant output.

### Output:

The system will output something like this:

```python
{'messages': [{'content': 'hi!', 'type': 'human'}]}
{'messages': [{'content': 'hi!', 'type': 'human'}, {'content': "Hello! How can I assist you today?", 'type': 'ai'}]}
```

Here, we see a simple conversation where the user says "hi!" and the AI responds with a greeting.

### Checking the State üßë‚Äçüíª

You can check the state of the conversation using this command:

```python
state = await client.threads.get_state(thread["thread_id"])

print(state['next'])
```

### Code Explanation:

- **`client.threads.get_state(thread["thread_id"])`**: Retrieves the current state of the conversation thread.
- **`print(state['next'])`**: Displays the next step in the process, which will be empty if the conversation has finished.

### Output:

```python
[]
```

The system finishes the interaction, and no further actions are required.

---

## Conclusion: Why Use Human-in-the-Loop? üí°

Human-in-the-loop interactions are important because they provide oversight and control over automated processes. In scenarios where tools or AI systems make decisions, allowing a human to step in and modify or approve actions ensures accuracy and reliability.

This concept can be applied to:

- **Healthcare**: Doctors reviewing AI diagnosis results.
- **Finance**: Financial advisors reviewing algorithmic trading decisions.
- **Customer Support**: Agents reviewing automated responses before sending them.

By using **LangGraph** and setting breakpoints, you can easily incorporate HIL into your applications, allowing for flexible, human-guided automation.


# **Approving and Editing Tool Calls**

In this example, we're discussing a process that involves managing tool calls within a system. It primarily focuses on how to approve or edit a tool call during a conversation, and how to stream data in an asynchronous way. This concept is commonly used in workflows involving AI assistants or automation platforms that interact with external tools and APIs. We will go step by step, explaining each part of the code with real-world examples.

---

### **1. What is a Tool Call? üõ†Ô∏è**

A **tool call** refers to the action of calling an external function or tool from within a system. In this context, we're using an AI assistant, which calls a tool to gather information (e.g., checking the weather) and then returns the response.

#### **Real-World Example**
Imagine you're using a virtual assistant like Siri or Google Assistant. When you ask for the weather in San Francisco, the assistant makes a tool call to a weather API to fetch the data and give you a response.

---

### **2. Approving a Tool Call ‚úÖ**

Approving a tool call means allowing the assistant to continue running or interacting with the tool, once it has been reviewed (like a human reviewing the system‚Äôs decision). 

The approval is done by **continuing the thread without any edits** (i.e., approving the process). This helps the assistant proceed with the next steps in the workflow.

#### **Code Example:**

```python
input = {"messages": [{"role": "user", "content": "what's the weather in sf?"}]}

async for chunk in client.runs.stream(
    thread["thread_id"],
    assistant_id,
    input=input,
):
    if chunk.data and chunk.event != "metadata":
        print(chunk.data)
```

#### **Explanation of Code:**

- **input**: This is the message we send to the system. It's a request asking, "what's the weather in SF?"
- **client.runs.stream()**: This function starts streaming the conversation between the assistant and the user, continuously sending chunks of data. 
- **if chunk.data and chunk.event != "metadata"**: We ensure that we only print the useful data, not the metadata (like system information).
- **print(chunk.data)**: We print the response from the system, which could be the weather or any data returned from the tool.

#### **Output:**

```json
{'messages': [{'content': "what's the weather in sf?", ...}]}
```

In the output, we can see the conversation flow. Initially, the system is reviewing the user's request.

---

### **3. Checking the State of the Thread üßµ**

Before we approve the tool call, we need to check if the system is waiting for human approval. This is done by inspecting the thread's state.

#### **Code Example:**

```python
state = await client.threads.get_state(thread["thread_id"])
print(state['next'])
```

#### **Explanation of Code:**

- **client.threads.get_state()**: This function retrieves the current state of the thread, which tells us if the process is waiting for human review.
- **state['next']**: This gives the next step in the process, indicating if it's waiting for human review (`human_review_node`).

#### **Output:**

```json
['human_review_node']
```

This confirms that the system is waiting for human approval before proceeding.

---

### **4. Approving the Tool Call (No Edits) üîÑ**

To approve the tool call, we send a new run with no input data, meaning we just want to continue from the current state without changing anything.

#### **Code Example:**

```python
async for chunk in client.runs.stream(
    thread["thread_id"],
    assistant_id,
    input=None,
    stream_mode="values",
):
    if chunk.data and chunk.event != "metadata":
        print(chunk.data)
```

#### **Explanation of Code:**

- **input=None**: We're not sending any new input data, just letting the assistant continue.
- **stream_mode="values"**: This ensures we only get the actual response data, not metadata.
- **print(chunk.data)**: We print the response after the tool call is approved.

#### **Output:**

```json
{'messages': [{'content': "what's the weather in sf?", ...}]}
```

The system now moves forward and completes the tool call, which in this case is fetching the weather information.

---

### **5. Editing the Tool Call üîß**

Sometimes, we might need to edit the tool call. For example, we may want to change parameters like the city in a weather search. This is useful when we need to make updates to the request before executing the tool.

#### **Code Example:**

```python
state = await client.threads.get_state(thread['thread_id'])
current_content = state['values']['messages'][-1]['content']
current_id = state['values']['messages'][-1]['id']
tool_call_id = state['values']['messages'][-1]['tool_calls'][0]['id']

# Construct new message with updated tool call
new_message = {
    "role": "assistant", 
    "content": current_content,
    "tool_calls": [
        {
            "id": tool_call_id,
            "name": "weather_search",
            "args": {"city": "San Francisco, USA"}
        }
    ],
    "id": current_id
}

await client.threads.update_state(
    thread['thread_id'], 
    {"messages": [new_message]}, 
    as_node="human_review_node"
)
```

#### **Explanation of Code:**

- **state['values']**: Fetches the current values in the thread, including the last message and tool call.
- **current_content and current_id**: Extract the content and ID of the last message to retain the context.
- **new_message**: This creates a new message with the updated tool call, changing the city from "San Francisco" to "San Francisco, USA".
- **client.threads.update_state()**: This updates the state of the thread, replacing the old message with the new one.

#### **Output:**

```json
{
  'messages': [
    {
      'content': "what's the weather in sf?", 
      'tool_calls': [
        {'name': 'weather_search', 'args': {'city': 'San Francisco, USA'}}
      ]
    }
  ]
}
```

Now the tool call is updated, and the assistant can use this new input to make a fresh API call.

---

### **Real-World Example of Editing Tool Calls in Action üåç**

In an **e-commerce platform** where users inquire about product availability or delivery status, you might need to edit or approve tool calls in response to certain conditions. For example, if a user asks, "What is the status of my order?", the system may initially fetch the order status from a tool. If the status needs human verification (e.g., due to an issue with inventory), the system waits for approval to continue.

Once approved, the system might update the tool call to reflect the latest order details and provide the response to the user.

---

### **Conclusion üåü**

In this process:
- **Approving a tool call** ensures that the system can continue executing its workflow.
- **Editing a tool call** allows us to make updates to the request before executing it, which is useful for dynamic scenarios.
  
This process is vital for building robust AI assistant workflows that interact with external services and tools. The ability to approve or modify tool calls provides flexibility and control in how data is fetched and processed.


### üéØ **Feedback to a Tool Call** 

When working with automated tools, there may be situations where you don‚Äôt want to execute a tool call directly but would still like to provide feedback to improve the tool's results. This can be helpful when you don't want users to manually modify the tool call or when you need to adjust or tweak its behavior in some way.

---

### üß† **What Does Feedback to a Tool Call Mean?**

Feedback to a tool call means providing input that helps guide or adjust the tool‚Äôs behavior without executing it directly. You can simulate tool output (like an error or result) by adding messages to the system state, instead of calling the tool outright.

This can be done in a couple of ways:
1. **Single Message Feedback:** You add a single message representing feedback, indicating a change needed.
2. **Dual Message Feedback:** You add two messages: one representing an error (if the tool failed or needs correction), and one representing human feedback (what the user wants changed).

Let's break down the two approaches with examples and practical applications.

---

### ‚öôÔ∏è **Implementation Example:**

Imagine you‚Äôre building a system where users can ask about the weather in different cities. However, instead of just letting the system pull weather data from an API, you want to incorporate feedback from users and adjust the query dynamically based on their preferences.

In this case, the system receives a tool call to get the weather for a city (e.g., San Francisco), but the user later requests changes to refine the query. Here's how this might be done:

#### **Code Example in Python:**

```python
input = {"messages": [{"role": "user", "content": "what's the weather in sf?"}]}

# Stream the response from the client
async for chunk in client.runs.stream(
    thread["thread_id"],
    assistant_id,
    input=input,
):
    if chunk.data and chunk.event != "metadata": 
        print(chunk.data)
```

- **What‚Äôs happening here?**  
   This code sends an initial query about the weather in San Francisco to the client. The `input` message contains the user's request.
   
- **The `async for chunk in client.runs.stream()` loop:**  
   This part streams data (tool responses, for example) from the client. The `chunk` is where the results are processed, and it checks if the data is valid before printing.

#### **Output for Initial Request:**
```
{'messages': [{'content': "what's the weather in sf?", 'type': 'human'}]}
```
- **Explanation:** The system processes the question, but it doesn‚Äôt yet call the tool (such as an API request). It simply logs the user‚Äôs query.

---

### üîÑ **Handling Feedback by Updating the State:**

To respond to the feedback (i.e., changes requested by the user like including the country in the query), you need to update the system‚Äôs state, including new messages or corrected inputs.

#### **Updating the State with Feedback:**

```python
# Fetch the current state from the client
state = await client.threads.get_state(thread['thread_id'])
print("Current State:")
print(state['values'])
print("\nCurrent Tool Call ID:")

# Get the tool call ID from the state to replace
tool_call_id = state['values']['messages'][-1]['tool_calls'][0]['id']
print(tool_call_id)

# Construct a new feedback message
new_message = {
    "role": "tool", 
    "content": "User requested changes: pass in the country as well",
    "name": "weather_search",
    "tool_call_id": tool_call_id
}

# Update the state with the new feedback
await client.threads.update_state(
    thread['thread_id'], 
    {"messages": [new_message]}, 
    as_node="human_review_node"
)
```

- **What‚Äôs happening here?**  
   The code fetches the current state of the thread, which holds the last messages and tool calls. 
   
   It then creates a new message (`new_message`) that represents the feedback (e.g., user asking to include the country in the weather search).
   
   Finally, it updates the state using the `update_state` method and includes the feedback as a ‚Äúhuman review‚Äù node.

---

### üß© **Practical Example in Real-World Scenarios:**

#### **Scenario: Weather Application**

- **Tool Call:** A user asks for the weather in San Francisco.
- **Feedback/Modification:** The user provides feedback, asking the system to include the country ("USA") for more accurate results.

#### **Why Use Feedback Instead of Re-running the Tool?**

- **User-Friendly:** Instead of prompting the user to edit a query manually, the system adjusts dynamically based on feedback.
- **Efficiency:** It saves time by using feedback without repeating the entire process from scratch.
- **Flexibility:** The system can handle multiple feedback types, allowing for personalized responses (e.g., adding more detailed information, changing a location, etc.).

---

### üìà **Real World Applications of Feedback to Tool Calls:**

1. **E-commerce Systems:**  
   If a user asks for product recommendations, and the system suggests generic options, they may request more specific filters. The feedback will adjust the query dynamically to show items with the right specifications, such as color, size, or brand.

2. **AI Assistants:**  
   A virtual assistant might make a suggestion, but based on user feedback (e.g., "Not interested in red shirts"), it can refine the search results or recommendations.

---

### üßë‚Äçüíª **Next Step: Continue Execution**

After the feedback is applied, you can continue executing the system, adjusting the query to match the new requirements (e.g., adding the country "USA" to the weather search).

```python
# Continuing the execution after feedback
async for chunk in client.runs.stream(
    thread["thread_id"],
    assistant_id,
    input=None,
):
    if chunk.data and chunk.event != "metadata": 
        print(chunk.data)
```

---

### ‚ú® **Conclusion**

Providing feedback to a tool call lets you refine and guide the system‚Äôs behavior dynamically without requiring users to intervene directly. This can be applied in various real-world applications where systems adapt based on user input, such as e-commerce, AI assistants, or even weather services.

This approach ensures that tools can be fine-tuned to meet the specific needs of users, enhancing the overall user experience and system flexibility.


In the context of the example provided, the feedback to a tool happens when we want to respond to a tool call with changes or feedback before proceeding. This is useful when we don't want to execute the tool immediately or if we want to update the tool's call arguments or handle errors before continuing.

To demonstrate how feedback is provided to a tool, let's break down the process in code.

### High-Level Overview:

1. **Initial Tool Call**: A tool call is made, and the system asks for the result (e.g., asking for the weather of San Francisco).
2. **Adding Feedback**: Instead of executing the tool call or if there's an issue, you can add feedback. This feedback is a message in the system, which acts as a mock result for the tool call.
3. **Updating the State**: The feedback (natural language) is inserted into the system as a new message, which could be a modification of the previous tool call.

### Code Example:

Below is a Python code example to simulate how feedback is given to a tool and how you can update the tool's call to make the necessary changes.

```python
import asyncio

# Assuming you are using a client that manages interactions with the system
async def give_feedback_to_tool(client, thread, assistant_id):
    # Step 1: Simulate an initial tool call for getting the weather in San Francisco
    input_message = {"messages": [{"role": "user", "content": "what's the weather in sf?"}]}

    async for chunk in client.runs.stream(thread["thread_id"], assistant_id, input=input_message):
        if chunk.data and chunk.event != "metadata":
            print("Tool Call Initiated:", chunk.data)

    # Step 2: Fetch the current state (this is where the tool call info is stored)
    state = await client.threads.get_state(thread['thread_id'])
    print("Current State:", state['values'])
    
    # Get the last tool call's ID
    tool_call_id = state['values']['messages'][-1]['tool_calls'][0]['id']
    print("Current Tool Call ID:", tool_call_id)

    # Step 3: Construct feedback to provide natural language feedback to the tool
    feedback_message = {
        "role": "tool",  # It's coming from a tool
        "content": "User requested changes: pass in the country as well",  # Feedback to the tool
        "name": "weather_search",  # Name of the tool being called
        "tool_call_id": tool_call_id  # The ID of the previous tool call that is being updated
    }

    # Step 4: Update the state with the feedback message
    await client.threads.update_state(
        thread['thread_id'],  # The thread ID we are working with
        {"messages": [feedback_message]},  # The new message containing the feedback
        as_node="human_review_node"  # Adding this feedback as if it is from a human reviewer
    )

    print("Feedback provided. Continuing execution...")

    # Step 5: Resume execution with the updated feedback
    async for chunk in client.runs.stream(thread["thread_id"], assistant_id, input=None):
        if chunk.data and chunk.event != "metadata":
            print("Execution resumed with feedback:", chunk.data)

# Example usage
# Assuming `client`, `thread`, and `assistant_id` are predefined in your system
await give_feedback_to_tool(client, thread, assistant_id)
```

### Explanation:

1. **Initial Tool Call (`input_message`)**:
   - We first simulate the user asking a question (`what's the weather in sf?`) by sending this message to the system. This initiates the tool call to fetch the weather.

2. **Fetching Current State**:
   - `await client.threads.get_state(thread['thread_id'])`: This fetches the current state, which contains information about all the messages, tool calls, etc. We then extract the tool call ID that was generated by the initial query.

3. **Constructing the Feedback Message**:
   - The feedback message is a new message that informs the tool about the changes that need to be made, like including the country (e.g., "San Francisco, USA"). This is sent as part of the `tool` role to signal it's coming from a tool, and it includes the ID of the tool call we want to update.

4. **Updating the State with Feedback**:
   - `await client.threads.update_state(...)`: Here, we update the thread's state by pushing the feedback message. We specify that the feedback is coming from a "human review node", and it will act as a mock response to the tool.

5. **Resuming Execution**:
   - After providing the feedback, execution continues, and the system will proceed with the updated tool call, which includes the modified information (e.g., "San Francisco, USA").

### Real-World Example:

- **Customer Support Tools**: Imagine you're building an AI-driven customer support tool. When a user requests something like the weather, the system initially calls a weather API. However, the user might provide additional information or context later, like asking for a specific location (city + country). Instead of directly modifying the original call, you can add feedback to update the request before making the API call.

- **Automated Data Processing**: In data pipelines, if a data source is missing some required parameters (like a missing region in a request), the tool can provide feedback (like "add country") before continuing to process the request.

This concept is crucial when interacting with tools or services where dynamic adjustments or corrections are needed based on user feedback or additional context.