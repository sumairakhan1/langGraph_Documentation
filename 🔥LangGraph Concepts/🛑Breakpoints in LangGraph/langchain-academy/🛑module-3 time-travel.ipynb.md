# ğŸ›‘mod-3 /time-travel.ipynb

# â³ Time Travel in LangGraph: Debugging and Human-in-the-Loop Interaction  

Time travel in LangGraph is a powerful debugging and control feature that allows users to **view, replay, and even fork from past states** of a process. This is particularly useful in scenarios where we need **human approval, debugging, and state editing**.  

---  

## ğŸ† **Why Do We Need Time Travel?**  

### âœ… **1. Approval**  
- Allows human users to **review and approve actions** before they are executed.  
- **Example:** A finance application where transactions require human approval before processing.  

### ğŸ” **2. Debugging**  
- Enables developers to **rewind the execution graph** to reproduce or avoid issues.  
- **Example:** In an AI chatbot system, we might want to go back and check why a certain incorrect response was generated.  

### âœï¸ **3. Editing**  
- Users can **modify the state** of an execution process when needed.  
- **Example:** A workflow automation system where a user can update an approval status before proceeding to the next step.  

---

## ğŸš€ **Setting Up LangGraph for Time Travel**  

We start by installing and configuring **LangGraph**, **LangChain**, and required libraries.  

### ğŸ“Œ **Install Dependencies**  
```python
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_openai langgraph_sdk
```
ğŸ”¹ `pip install` ensures all necessary libraries are installed.  

### ğŸ”‘ **Setting Up API Keys**  
We use OpenAI's API for processing. The `_set_env` function securely stores our API key.  

```python
import os, getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```
ğŸ”¹ **`getpass.getpass()`** prompts the user to enter their API key securely.  

---

## ğŸ¤– **Building an AI-Powered Arithmetic Agent**  

### âœ¨ **Defining Arithmetic Functions**  
We define simple arithmetic operations (addition, multiplication, and division) as tools that the AI agent can use.  

```python
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

def divide(a: int, b: int) -> float:
    """Divide two numbers."""
    return a / b
```
ğŸ”¹ Each function **performs an arithmetic operation** and returns the result.  
ğŸ”¹ These functions will later be bound to the AI model as tools.  

---

## ğŸ› ï¸ **Integrating the AI Model with Tools**  

We integrate our AI model (`gpt-4o`) and bind our arithmetic tools to it.  

```python
from langchain_openai import ChatOpenAI

tools = [add, multiply, divide]
llm = ChatOpenAI(model="gpt-4o")
llm_with_tools = llm.bind_tools(tools)
```
ğŸ”¹ **`ChatOpenAI(model="gpt-4o")`** loads the OpenAI GPT-4o model.  
ğŸ”¹ **`bind_tools(tools)`** allows the AI to use our arithmetic functions dynamically.  

---

## ğŸŒ **Building the Execution Graph**  

We create a **message processing system** that dynamically routes messages between AI and tools.  

### ğŸ”¹ **Defining Messages and System Instructions**  
```python
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

sys_msg = SystemMessage(content="You are a helpful assistant tasked with performing arithmetic on a set of inputs.")
```
ğŸ”¹ The **system message** provides instructions to the AI about its role.  

---

### ğŸ“ **Defining Graph Nodes (Processing Units)**  
A **node** is where computation happens. We define two main nodes:  
1. **`assistant` Node** â€“ The AI processes messages.  
2. **`tools` Node** â€“ The AI invokes the appropriate arithmetic function.  

```python
from langgraph.graph import MessagesState

def assistant(state: MessagesState):
   return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}
```
ğŸ”¹ The `assistant` function **invokes the AI model with system and user messages**.  
ğŸ”¹ If the AI needs to perform a calculation, it will call one of our arithmetic tools.  

---

### ğŸ”„ **Defining the Execution Flow**  
We define a **state graph** that controls how messages flow between nodes.  

```python
from langgraph.graph import START, END, StateGraph
from langgraph.prebuilt import tools_condition, ToolNode

builder = StateGraph(MessagesState)

# Define Nodes
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define Edges (Flow Logic)
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    tools_condition,  # Decides whether to go to tools or end execution
)
builder.add_edge("tools", "assistant")
```
ğŸ”¹ **Edges define the flow of execution:**  
  - The process **starts at `assistant`**.  
  - If AI **detects a tool call**, it **routes the request to the `tools` node**.  
  - The **result returns to `assistant`** for further processing.  
  - The process **ends if no tool call is required**.  

---

## â³ **Enabling Time Travel for Debugging**  

To enable **time travel (replaying past states, debugging, and modifications)**, we use **MemorySaver** for state checkpoints.  

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
graph = builder.compile(checkpointer=MemorySaver())
```
ğŸ”¹ **`MemorySaver()`** allows us to save, rewind, and fork execution states.  

---

## ğŸ–¼ï¸ **Visualizing the Graph**  

We generate a **graph visualization** of our AI-powered workflow.  

```python
from IPython.display import Image, display

display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
```
ğŸ”¹ **The graph visualization helps in debugging** by showing how messages flow between nodes.  

---

## ğŸ¯ **Real-World Applications of Time Travel in AI Systems**  

1ï¸âƒ£ **ğŸ›’ E-Commerce Chatbots**  
   - AI-powered assistants can **recall past conversations** to refine responses.  
   - Time travel helps debug why a chatbot **recommended the wrong product**.  

2ï¸âƒ£ **ğŸ“‘ Workflow Automation**  
   - Businesses can **rewind and modify** workflow execution if an error is found.  
   - Example: An automated approval system that **lets a manager modify a decision** before finalizing.  

3ï¸âƒ£ **ğŸ§  AI Debugging and Fine-Tuning**  
   - Time travel allows developers to **track model decisions** and improve accuracy.  
   - Example: A **medical AI assistant** that suggests treatments based on patient history.  

---

## âœ… **Key Takeaways**  

âœ”ï¸ **LangGraph enables time travel for debugging, editing, and approval workflows.**  
âœ”ï¸ **AI models can be combined with custom tools (like arithmetic functions) to perform dynamic tasks.**  
âœ”ï¸ **Graph-based workflows help manage complex AI interactions and enable control flow customization.**  
âœ”ï¸ **Memory checkpoints allow us to rewind and modify past states for better debugging.**  

---

## ğŸ”¥ **Whatâ€™s Next?**  
- Implementing **user interfaces** for interactive debugging.  
- Enhancing **graph logic** to support more complex workflows.  
- Experimenting with **advanced AI models and multi-step workflows**.  

---

This guide provides **a beginner-friendly explanation with real-world use cases and code breakdowns**. Would you like me to modify or expand any section? ğŸš€

---

# ğŸ½ï¸ AI Agent & Chatbot for Restaurants  

Creating an AI-powered agent and chatbot for a restaurant can enhance customer experience, streamline operations, and improve efficiency. Below is a detailed breakdown of the key **features and functionalities** you should work on, along with real-world use cases and code examples.  

---

## ğŸ† **Key Features & Functionalities**  

### 1ï¸âƒ£ **Reservation Management**  
âœ… **Allow customers to book tables via chatbot**  
âœ… **Check available slots**  
âœ… **Send confirmation messages**  
âœ… **Modify or cancel reservations**  

ğŸ’¡ *Real-world Use Case:* A restaurant chatbot lets users book a table by checking real-time availability and confirming the reservation via SMS or email.  

ğŸ“Œ **Example Code (Using Python & Flask)**  

```python
from flask import Flask, request, jsonify
from datetime import datetime

app = Flask(__name__)

reservations = {}  # Store reservations

@app.route('/reserve', methods=['POST'])
def reserve_table():
    data = request.json
    name = data.get("name")
    time = data.get("time")
    
    if time in reservations:
        return jsonify({"message": "Time slot already booked!"}), 400
    
    reservations[time] = name
    return jsonify({"message": f"Table reserved for {name} at {time}!"})

if __name__ == '__main__':
    app.run(debug=True)
```

ğŸ› ï¸ **Explanation:**  
- Creates a **Flask API** to handle reservations.  
- Checks if the requested time slot is available.  
- Stores reservation details in a dictionary.  

---

### 2ï¸âƒ£ **Menu Browsing & Ordering**  
âœ… **Allow customers to browse the menu**  
âœ… **Provide descriptions & images of dishes**  
âœ… **Place orders via chatbot**  

ğŸ’¡ *Real-world Use Case:* A customer can type â€œShow me vegetarian dishes,â€ and the chatbot will list relevant menu items.  

ğŸ“Œ **Example Code (Using Python & JSON for Menu Data)**  

```python
menu = {
    "items": [
        {"name": "Margherita Pizza", "category": "Vegetarian", "price": 10},
        {"name": "Grilled Chicken", "category": "Non-Vegetarian", "price": 15},
    ]
}

def get_menu(category=None):
    if category:
        return [item for item in menu["items"] if item["category"] == category]
    return menu["items"]

print(get_menu("Vegetarian"))
```

ğŸ› ï¸ **Explanation:**  
- Stores the **menu in JSON format**.  
- Filters menu items **based on user preference**.  

---

### 3ï¸âƒ£ **Order Tracking**  
âœ… **Provide real-time order status**  
âœ… **Send notifications for updates**  

ğŸ’¡ *Real-world Use Case:* A chatbot can update customers with â€œYour order is being preparedâ€ or â€œYour food is out for delivery.â€  

ğŸ“Œ **Example Code (Using WebSockets for Real-Time Updates)**  

```python
import time

order_status = ["Received", "Being Prepared", "Out for Delivery", "Delivered"]

def track_order(order_id):
    for status in order_status:
        print(f"Order {order_id}: {status}")
        time.sleep(2)

track_order(101)
```

ğŸ› ï¸ **Explanation:**  
- Simulates **order status updates** in real time.  
- Uses a **loop to update the order status** at intervals.  

---

### 4ï¸âƒ£ **Customer Support & FAQs**  
âœ… **Answer common customer questions (e.g., timings, special offers, ingredients)**  
âœ… **Provide live support or redirect to a human agent**  

ğŸ’¡ *Real-world Use Case:* The chatbot can answer, â€œWhat are your restaurant hours?â€ or â€œDo you offer gluten-free options?â€  

ğŸ“Œ **Example Code (Using OpenAI's GPT for Responses)**  

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")

question = "What are your opening hours?"
response = llm.invoke(question)

print(response)
```

ğŸ› ï¸ **Explanation:**  
- Uses **GPT-powered responses** to answer customer queries.  
- Can be expanded to include **predefined FAQs**.  

---

### 5ï¸âƒ£ **Personalized Recommendations**  
âœ… **Suggest dishes based on user preferences**  
âœ… **Analyze past orders to recommend new items**  

ğŸ’¡ *Real-world Use Case:* If a user frequently orders spicy food, the chatbot can suggest similar dishes.  

ğŸ“Œ **Example Code (Using Machine Learning for Recommendations)**  

```python
from sklearn.neighbors import NearestNeighbors
import numpy as np

order_data = np.array([
    [1, 0, 0],  # User likes spicy food
    [0, 1, 1],  # User prefers vegetarian dishes
])

model = NearestNeighbors(n_neighbors=1)
model.fit(order_data)

new_user = np.array([[1, 0, 0]])  # A new user likes spicy food
recommendation = model.kneighbors(new_user, return_distance=False)

print(f"Recommended dish based on past orders: {recommendation}")
```

ğŸ› ï¸ **Explanation:**  
- Uses **ML to suggest dishes** based on past preferences.  

---

### 6ï¸âƒ£ **Loyalty Program & Discounts**  
âœ… **Reward returning customers**  
âœ… **Offer discount coupons**  

ğŸ’¡ *Real-world Use Case:* After 5 orders, the chatbot can say, â€œYouâ€™ve earned a 10% discount on your next order!â€  

ğŸ“Œ **Example Code (Using Python for Reward Tracking)**  

```python
users = {"Alice": 4}

def check_rewards(name):
    if users.get(name, 0) >= 5:
        return "Youâ€™ve earned a 10% discount!"
    return "Keep ordering to earn rewards!"

print(check_rewards("Alice"))
```

ğŸ› ï¸ **Explanation:**  
- Tracks **customer order count**.  
- Provides **discounts for repeat customers**.  

---

### 7ï¸âƒ£ **Multilingual Support**  
âœ… **Support multiple languages (English, Spanish, Urdu, etc.)**  
âœ… **Auto-detect user language**  

ğŸ’¡ *Real-world Use Case:* If a user types in **Spanish**, the chatbot automatically switches to Spanish responses.  

ğŸ“Œ **Example Code (Using Google Translate API)**  

```python
from googletrans import Translator

translator = Translator()
message = "Hello, how can I help you?"

translated_message = translator.translate(message, dest="es")
print(translated_message.text)
```

ğŸ› ï¸ **Explanation:**  
- Uses **Google Translate API** for multilingual responses.  

---

## ğŸ¯ **Tech Stack for AI Restaurant Chatbot**  
ğŸ”¹ **Backend:** Python (Flask, FastAPI)  
ğŸ”¹ **Frontend:** React.js, Next.js  
ğŸ”¹ **AI Models:** OpenAI GPT-4, Langchain  
ğŸ”¹ **Database:** PostgreSQL, MongoDB  
ğŸ”¹ **Messaging:** WhatsApp API, Twilio, Telegram Bot API  
ğŸ”¹ **Payment Integration:** Stripe, Razorpay  

---

## ğŸš€ **Final Thoughts**  

By implementing an AI chatbot for restaurants, you can:  
âœ”ï¸ **Reduce waiting times** for customer inquiries.  
âœ”ï¸ **Improve operational efficiency** with automated orders.  
âœ”ï¸ **Increase revenue** through personalized recommendations.  
âœ”ï¸ **Enhance customer satisfaction** with real-time order tracking.  

Would you like help with a **specific feature** or a **detailed roadmap** for development? ğŸ˜Š


---

# ğŸª AI Agent & Chatbot for Restaurants: Features, Functionality & Implementation  

If you want to help restaurants by building an AI agent and chatbot, you need to focus on key features that enhance customer experience, streamline operations, and improve efficiency. Below is a detailed guide with explanations, real-world use cases, and code examples to help you build a restaurant chatbot.  

---

## ğŸ“Œ **1. Features of an AI Restaurant Chatbot**  

### ğŸ›ï¸ 1.1. Reservation & Table Booking  
âœ… Customers can book tables in advance through the chatbot.  
âœ… The chatbot checks availability and confirms reservations.  
âœ… Integrates with Google Calendar or restaurant booking systems.  

ğŸ”¹ **Example Use Case**: A customer wants to reserve a table for 2 at 7 PM. The chatbot checks availability and confirms the booking.  

---

### ğŸ½ï¸ 1.2. Menu Display & Recommendations  
âœ… Displays the restaurant menu with images, prices, and descriptions.  
âœ… Recommends dishes based on user preferences (e.g., vegetarian, spicy, bestsellers).  
âœ… Uses AI to analyze past orders and suggest personalized meals.  

ğŸ”¹ **Example Use Case**: A user types, "I want a spicy dish." The chatbot suggests "Spicy Chicken Biryani" or "Hot Wings."  

---

### ğŸ“¦ 1.3. Order Placement & Food Delivery  
âœ… Customers can place food orders directly via the chatbot.  
âœ… Integrates with delivery services (Uber Eats, FoodPanda, etc.).  
âœ… Provides real-time order tracking.  

ğŸ”¹ **Example Use Case**: A customer orders a pizza through the chatbot, which automatically sends the order to the kitchen and provides an estimated delivery time.  

---

### ğŸ’¬ 1.4. Customer Support & FAQs  
âœ… Answers common questions about restaurant hours, location, policies, and more.  
âœ… Handles refund and complaint requests automatically.  
âœ… Escalates complex issues to human staff if needed.  

ğŸ”¹ **Example Use Case**: A user asks, "What are your opening hours?" The chatbot instantly replies with, "We are open from 10 AM to 11 PM every day."  

---

### ğŸ’° 1.5. Payment Processing  
âœ… Supports multiple payment methods (credit cards, digital wallets, cash on delivery).  
âœ… Secure payment gateway integration (Stripe, PayPal, Razorpay).  
âœ… Generates digital receipts for customers.  

ğŸ”¹ **Example Use Case**: After a customer orders food, the chatbot asks, "Would you like to pay online or at delivery?" and processes the transaction accordingly.  

---

### ğŸ·ï¸ 1.6. Offers & Discounts  
âœ… Sends promotional offers, discounts, and coupons to customers.  
âœ… Provides loyalty rewards for frequent customers.  

ğŸ”¹ **Example Use Case**: "Get 20% off on your first order!" The chatbot automatically applies the discount when a user places an order.  

---

### ğŸ› ï¸ 1.7. Multilingual Support  
âœ… Supports multiple languages (English, Urdu, Arabic, etc.).  
âœ… Uses AI to detect the user's language preference.  

ğŸ”¹ **Example Use Case**: A user types in Urdu, "Ù…Ø¬Ú¾Û’ Ø¨Ø±ÛŒØ§Ù†ÛŒ Ú†Ø§ÛÛŒÛ’" (I want Biryani), and the chatbot responds in Urdu.  

---

### ğŸ“Š 1.8. Analytics & Insights  
âœ… Tracks customer preferences, peak ordering times, and best-selling dishes.  
âœ… Helps restaurant owners make data-driven decisions.  

ğŸ”¹ **Example Use Case**: The chatbot analyzes order history and suggests adding "Chocolate Lava Cake" to the menu because many customers searched for desserts.  

---

## ğŸ› ï¸ **2. Technologies & Tools to Use**  

| Feature               | Technology Stack           |
|-----------------------|---------------------------|
| Chatbot Development  | **Dialogflow, OpenAI GPT-4, Rasa** |
| Backend              | **Node.js, Python (Flask/Django), Firebase** |
| Database             | **MongoDB, PostgreSQL, MySQL** |
| Payments            | **Stripe, PayPal, Razorpay** |
| Frontend (if needed) | **React, Vue.js, HTML/CSS** |
| Hosting             | **AWS, Vercel, Firebase Hosting** |

---

## ğŸ’» **3. Code Example: Basic Chatbot with Python (Flask & OpenAI GPT-4)**  

### ğŸ”¹ **Step 1: Install Dependencies**  
```bash
pip install flask openai
```

---

### ğŸ”¹ **Step 2: Create `app.py` (Flask Backend for Chatbot)**
```python
from flask import Flask, request, jsonify
import openai

app = Flask(__name__)

# Configure OpenAI API Key
openai.api_key = "your_openai_api_key"

@app.route("/chat", methods=["POST"])
def chat():
    user_input = request.json.get("message")

    # Call OpenAI API for response
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": user_input}]
    )

    chatbot_response = response["choices"][0]["message"]["content"]
    
    return jsonify({"response": chatbot_response})

if __name__ == "__main__":
    app.run(debug=True)
```

---

### ğŸ”¹ **Step 3: Create a Simple Frontend with HTML & JavaScript**
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <title>Restaurant Chatbot</title>
</head>
<body>
    <h2>Chat with our AI Restaurant Assistant</h2>
    <input type="text" id="userInput" placeholder="Ask something..." />
    <button onclick="sendMessage()">Send</button>
    <div id="chat"></div>

    <script>
        async function sendMessage() {
            let userMessage = document.getElementById("userInput").value;
            let response = await fetch("/chat", {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify({ message: userMessage })
            });

            let data = await response.json();
            document.getElementById("chat").innerHTML += "<p><b>You:</b> " + userMessage + "</p>";
            document.getElementById("chat").innerHTML += "<p><b>Bot:</b> " + data.response + "</p>";
        }
    </script>
</body>
</html>
```

---

## ğŸ“Œ **4. Explanation of Code**  
### ğŸ”¹ **Backend (Flask)**
1. `Flask` is used to create a simple web server.  
2. The chatbot takes user input and sends it to OpenAIâ€™s GPT-4 API.  

---

# ğŸ”€ **Understanding Forking in Graph-Based Systems**

## ğŸ“Œ **Introduction**
Forking is a powerful concept in graph-based workflows that allows you to take a previous state, modify it, and create a new execution path. This is especially useful when you want to explore alternative scenarios, rerun a process with different inputs, or debug an AI agent.

In this guide, weâ€™ll cover:
- **What is forking?**
- **How does forking work in a computational graph?**
- **Real-world use cases of forking**
- **Step-by-step explanation of forking with code examples**
- **Alternative approaches to achieve similar results**

---

## ğŸ”„ **What is Forking?**
Forking means taking a previous checkpoint (a saved state of a process) and modifying it to create a new branch of execution. The original state remains unchanged, while the new state follows a different path.

Think of it like:
- **Version control (Git branches):** You start from a previous commit, modify the code, and test a new feature without affecting the main branch.
- **Video games (Save states):** If you reach a difficult level, you can save the game, try different strategies, and reload if needed.

---

## ğŸ¯ **Real-World Use Cases of Forking**
1. **AI Chatbot Debugging:** If an AI assistant gives an unexpected response, you can fork the conversation at a specific point, modify the user input, and test different replies.
2. **Financial Simulations:** If a stock prediction model gives a certain forecast, analysts can modify input parameters and test alternative economic scenarios.
3. **Scientific Experiments:** Researchers can tweak variables in simulation software and observe how changes affect results.
4. **Automated Testing:** Software engineers can fork test cases with different parameters and check if an application behaves correctly in all scenarios.

---

## ğŸ—ï¸ **How Forking Works in a Graph-Based System**
1. **Retrieve a previous state** from the system.
2. **Modify the state** by changing the input values or parameters.
3. **Create a new checkpoint** that maintains history but follows a different path.
4. **Execute the new state** and analyze results.

---

## ğŸ“ **Step-by-Step Code Explanation**
Letâ€™s go through the forking process using Python-like pseudo-code.

### **1ï¸âƒ£ Retrieve a Previous State**
We want to get a specific past state from the graph.

```python
to_fork = all_states[-2]  # Get the second-last state
print(to_fork.values["messages"])
```

ğŸ” **Explanation:**
- `all_states` stores all previous states.
- `[-2]` selects the second-last state.
- `.values["messages"]` retrieves the messages at this state.

### **2ï¸âƒ£ View Configuration of the State**
Before modifying, let's check its configuration.

```python
print(to_fork.config)
```

ğŸ” **Explanation:**
- The `config` contains metadata like thread ID and checkpoint ID.
- This helps ensure we are modifying the correct execution step.

### **3ï¸âƒ£ Modify the State (Create Fork)**
We update the message in the state to test a new scenario.

```python
fork_config = graph.update_state(
    to_fork.config,
    {"messages": [HumanMessage(content='Multiply 5 and 3', 
                               id=to_fork.values["messages"][0].id)]},
)
```

ğŸ” **Explanation:**
- `graph.update_state(...)` modifies the state at the checkpoint.
- `to_fork.config` tells the graph where the state belongs.
- `"messages": [HumanMessage(...)]` replaces the existing message.
- `id=to_fork.values["messages"][0].id` ensures we overwrite instead of appending.

### **4ï¸âƒ£ View the Updated Forked State**
Check if the new state has been properly created.

```python
print(fork_config)
```

ğŸ” **Explanation:**
- `fork_config` now contains the new checkpoint ID.
- This allows the system to recognize that a fork has been made.

### **5ï¸âƒ£ Execute the Forked State**
Now that we have modified the state, let's execute it.

```python
for event in graph.stream(None, fork_config, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

ğŸ” **Explanation:**
- `graph.stream(...)` starts processing from the new state.
- `"values"` mode ensures we get the output messages.
- `event['messages'][-1].pretty_print()` displays the result.

### **ğŸ“Œ Expected Output**
```
================================ Human Message =================================
Multiply 5 and 3
================================== Ai Message ==================================
Tool Calls:
  multiply (call_KP2CVNMMUKMJAQuFmamHB21r)
 Call ID: call_KP2CVNMMUKMJAQuFmamHB21r
  Args:
    a: 5
    b: 3
================================= Tool Message =================================
Name: multiply
15
================================== Ai Message ==================================
The result of multiplying 5 and 3 is 15.
```

---

## ğŸ­ **Alternative Approaches**
### ğŸ”¹ **1. Cloning Instead of Forking**
Instead of modifying an existing state, you can create an independent copy.

```python
new_config = graph.clone_state(to_fork.config)
graph.update_state(new_config, {"messages": [HumanMessage(content='Multiply 4 and 6')]})
```

ğŸ“Œ **Use Case:** If you want a fresh start without modifying history.

---

### ğŸ”¹ **2. Using Parameterized Execution**
Instead of modifying states manually, pass different parameters dynamically.

```python
def execute_with_different_inputs(a, b):
    return graph.execute({"messages": [HumanMessage(content=f"Multiply {a} and {b}")]})

execute_with_different_inputs(7, 2)
```

ğŸ“Œ **Use Case:** If you want to test multiple scenarios without storing states.

---

### ğŸ”¹ **3. Using Version Control for States**
Instead of forking within a graph, maintain checkpoints using a database.

```python
db.save_checkpoint(state_id="step_3", data=current_state)
db.load_checkpoint("step_3")
```

ğŸ“Œ **Use Case:** If your system needs to persist states for long-term storage.

---

## ğŸš€ **Conclusion**
Forking is a **powerful technique** for:
âœ”ï¸ Testing **alternative scenarios**  
âœ”ï¸ Debugging **AI conversations**  
âœ”ï¸ Simulating **"what-if" situations**  
âœ”ï¸ Enhancing **automated testing**  

### **ğŸ’¡ Key Takeaways**
âœ… **Forking helps you modify past states and test new scenarios.**  
âœ… **It's commonly used in AI, finance, and research applications.**  
âœ… **You can achieve similar results using cloning, parameterized execution, or version control.**  

ğŸ”¹ **Next Steps:** Try implementing forking in your own projects to better understand its potential!

Would you like a hands-on coding exercise to practice forking? ğŸ˜Š

---

It looks like you want a **detailed beginner-friendly explanation** of **Time Travel with LangGraph API**, including **real-world applications, code explanations line by line, and alternative examples** to help you understand it thoroughly.

I'll break this down into the following sections:

- **ğŸ” What is LangGraph API?**
- **â³ What is Time Travel in LangGraph?**
- **ğŸŒ Real-World Use Cases**
- **ğŸ“ Code Breakdown (Explained Line by Line)**
- **âš¡ Alternative Examples**
- **ğŸ’¡ Summary & Key Takeaways**

Let me put together a well-structured and visually appealing explanation for you. Stay tuned! ğŸš€

It looks like you want a **detailed, beginner-friendly explanation** of how **time travel with the LangGraph API** works, including **real-world applications, code breakdowns, and alternative examples**. I'll explain everything step by step.  

---

# ğŸš€ **Time Travel with LangGraph API â€“ A Beginner's Guide**  

## ğŸ§ **What is LangGraph API?**  
LangGraph is an API that allows you to create **graph-based workflows** for AI-powered applications. One of its key features is **"Time Travel,"** which lets you **replay, modify, and debug past states** of your workflow.  

### ğŸ† **Real-World Example of Time Travel in AI**  
Let's say you are building an **AI-powered chatbot for customer service**. A user asks a question, and the chatbot responds. But what if you want to:  

âœ”ï¸ **Replay the chatbot's decision-making process?**  
âœ”ï¸ **Modify the chatbot's past responses?**  
âœ”ï¸ **Test different scenarios without starting from scratch?**  

LangGraphâ€™s **Time Travel API** allows you to **go back to a previous state, edit it, and replay** from that pointâ€”just like rewinding a movie! ğŸ¥  

---

## ğŸ“Œ **Key Features of Time Travel in LangGraph API**  
1. **Replaying Past States:** Re-run a previous step in the conversation.  
2. **Streaming Updates:** Get real-time updates on changes in the conversation flow.  
3. **Forking a State:** Modify past messages and continue from a modified point.  

---

# ğŸ› ï¸ **Setting Up LangGraph API**
Before we start coding, let's check the system requirements.  

```python
import platform

if 'google.colab' in str(get_ipython()) or platform.system() != 'Darwin':
    raise Exception("Unfortunately, LangGraph Studio is currently not supported on Google Colab or requires a Mac.")
```

### ğŸ” **What This Code Does**  
âœ”ï¸ **Checks if the code is running on Google Colab** or **a non-Mac system.**  
âœ”ï¸ If yes, it raises an **exception** because LangGraph Studio requires macOS.  

---

# ğŸ“¡ **Connecting to LangGraph API**
Once we confirm that the system is compatible, we connect to the LangGraph API.  

```python
from langgraph_sdk import get_client

client = get_client(url="http://localhost:62780")
```

### ğŸ” **What This Code Does**  
âœ”ï¸ Imports `get_client` from LangGraph SDK.  
âœ”ï¸ Creates a `client` object that connects to **LangGraph Studio running locally** on port `62780`.  

---

# âª **Replaying a Past State**
Now, let's **send a request to our AI assistant** and **track the process.**  

### ğŸ”µ **Step 1: Create an AI Conversation Thread**  
```python
initial_input = {"messages": HumanMessage(content="Multiply 2 and 3")}
thread = await client.threads.create()
```

### ğŸ” **What This Code Does**  
âœ”ï¸ **Creates a new thread** (conversation).  
âœ”ï¸ Sends a **message** asking the AI assistant to **multiply 2 and 3**.  

---

### ğŸ”µ **Step 2: Stream the Conversation Updates**  
```python
async for chunk in client.runs.stream(
    thread["thread_id"],
    assistant_id="agent",
    input=initial_input,
    stream_mode="updates",
):
    if chunk.data:
        assistant_node = chunk.data.get('assistant', {}).get('messages', [])
        tool_node = chunk.data.get('tools', {}).get('messages', [])
        if assistant_node:
            print("-" * 20 + " Assistant Node " + "-" * 20)
            print(assistant_node[-1])
        elif tool_node:
            print("-" * 20 + " Tools Node " + "-" * 20)
            print(tool_node[-1])
```

### ğŸ” **What This Code Does**  
âœ”ï¸ **Streams real-time updates** of the assistantâ€™s actions.  
âœ”ï¸ **Displays AIâ€™s response** (Assistant Node).  
âœ”ï¸ **Shows tool execution results** (Tools Node).  

---

# ğŸ”„ **Replaying from a Checkpoint**
Sometimes, we may need to **replay from a specific point** in the conversation.  

### ğŸ”µ **Step 1: Retrieve Conversation History**  
```python
states = await client.threads.get_history(thread['thread_id'])
to_replay = states[-2]  # Get the second-last state
```

### ğŸ” **What This Code Does**  
âœ”ï¸ Retrieves the **entire chat history**.  
âœ”ï¸ Stores the **second-last state** for replay.  

---

### ğŸ”µ **Step 2: Replay from the Checkpoint**
```python
async for chunk in client.runs.stream(
    thread["thread_id"],
    assistant_id="agent",
    input=None,
    stream_mode="values",
    checkpoint_id=to_replay['checkpoint_id']
):      
    print(f"Receiving new event of type: {chunk.event}...")
    print(chunk.data)
```

### ğŸ” **What This Code Does**  
âœ”ï¸ **Replays past conversation state** without re-executing everything.  
âœ”ï¸ **Prints updates** of the conversation as it replays.  

---

# âœ‚ï¸ **Forking and Editing a Past State**
What if we want to **modify a past message** and continue the conversation from there?  

### ğŸ”µ **Step 1: Retrieve a Checkpoint**
```python
states = await client.threads.get_history(thread['thread_id'])
to_fork = states[-2]
to_fork['values']
```

### ğŸ” **What This Code Does**  
âœ”ï¸ Retrieves conversation history.  
âœ”ï¸ Stores the second-last checkpoint for modification.  

---

### ğŸ”µ **Step 2: Modify a Message**
```python
forked_input = {"messages": HumanMessage(content="Multiply 3 and 3",
                                         id=to_fork['values']['messages'][0]['id'])}

forked_config = await client.threads.update_state(
    thread["thread_id"],
    forked_input,
    checkpoint_id=to_fork['checkpoint_id']
)
```

### ğŸ” **What This Code Does**  
âœ”ï¸ **Edits the original message** from `"Multiply 2 and 3"` to `"Multiply 3 and 3"`.  
âœ”ï¸ **Uses the same message ID** to overwrite the past message.  
âœ”ï¸ **Updates the conversation state** with the new input.  

---

# ğŸ† **Alternative Examples**
### ğŸ”¹ **Example 1: Debugging a Customer Support Bot**  
A chatbot incorrectly understands a user's request. Instead of restarting the entire conversation, we **replay and modify** the incorrect response.  

### ğŸ”¹ **Example 2: AI-Powered Code Review**  
An AI-based code review tool flags an error. Instead of running the entire analysis again, we **replay from the error detection point** and make changes.  

### ğŸ”¹ **Example 3: Gaming AI Decision Trees**  
An AI-powered **game NPC (Non-Playable Character)** makes decisions using LangGraph. We use **time travel to replay AI actions** and tweak its behavior without restarting the game.  

---

# ğŸ **Conclusion**
âœ”ï¸ **LangGraph's Time Travel API** lets you replay, modify, and debug AI conversations efficiently.  
âœ”ï¸ Useful for **chatbots, AI assistants, and debugging workflows.**  
âœ”ï¸ Allows **real-time updates, forking, and replaying past states.**  

Would you like a **hands-on project** with this API? Let me know! ğŸš€