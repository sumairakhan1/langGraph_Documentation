# ğŸš€ **Understanding Streaming in LangGraph (Beginner-Friendly Guide)**
---
## ğŸ§© **Introduction**
In this module, we explore **streaming** in LangGraph, a key concept that enables real-time interaction with graphs and chat models. By the end of this guide, you will:
- Understand **what streaming is** and how it works in LangGraph.
- Learn how to **implement streaming in a chatbot** using `LangChain` and `LangGraph`.
- Explore **real-world applications** of streaming.
- Get **step-by-step code explanations** for better understanding.

---
## ğŸŒ **Real-World Use Case of Streaming**
Streaming is commonly used in:
1. **Chatbots** (like ChatGPT or customer support bots) for **real-time responses**.
2. **Financial Dashboards** that update **stock prices live**.
3. **Live Sports Scores** displaying **real-time game updates**.
4. **AI-Powered Coding Assistants** (like GitHub Copilot) showing **real-time code suggestions**.

---
## ğŸ”¥ **What is Streaming in LangGraph?**
Streaming allows us to **visualize graph output in real-time**. In LangGraph, this means:
- We can **see tokens as they are generated** (instead of waiting for the whole response).
- The chatbot can **remember previous conversations** and **summarize them when needed**.
- Users can interact with the chatbot **without waiting for the full response**.

---
## ğŸš€ **Setting Up the Environment**
Before we start coding, we need to install the required libraries.

```python
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_openai langgraph_sdk
```
ğŸ”¹ **Explanation**:
- `pip install -U langgraph langchain_openai langgraph_sdk` â†’ Updates the necessary Python packages.
- `%%capture --no-stderr` â†’ Hides unnecessary output.

---
## ğŸ”‘ **Setting Up API Keys for OpenAI**
Since we are using OpenAIâ€™s GPT-4 model, we need an **API Key**.

```python
import os, getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```
ğŸ”¹ **Explanation**:
- `os.environ.get(var)` â†’ Checks if the API key is already set.
- `getpass.getpass(f"{var}: ")` â†’ Securely prompts the user for an API key if it's missing.

---
## ğŸ›  **Setting Up LangChain and LangGraph**
Now, we define the necessary **import statements** and **graph components**.

```python
from IPython.display import Image, display
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph import MessagesState
```
ğŸ”¹ **Explanation**:
- `ChatOpenAI` â†’ Creates a chatbot using OpenAI's GPT-4.
- `MessagesState` â†’ Manages the chatbotâ€™s memory.
- `StateGraph` â†’ Creates a **graph-based workflow**.
- `START, END` â†’ Define where our chatbot workflow starts and stops.

---
## ğŸ§  **Defining the Chatbotâ€™s Memory State**
To keep track of **conversation history**, we define a **state class**.

```python
# LLM
model = ChatOpenAI(model="gpt-4o", temperature=0) 

# State
class State(MessagesState):
    summary: str
```
ğŸ”¹ **Explanation**:
- `temperature=0` â†’ Ensures **consistent** responses from GPT-4.
- `summary: str` â†’ Stores a **summary of the conversation**.

---
## ğŸ”„ **Calling the Chat Model**
We now define the function to interact with the **GPT-4 chatbot**.

```python
def call_model(state: State, config: RunnableConfig):
    
    # Get summary if it exists
    summary = state.get("summary", "")

    # If there is summary, then we add it
    if summary:
        system_message = f"Summary of conversation earlier: {summary}"
        messages = [SystemMessage(content=system_message)] + state["messages"]
    else:
        messages = state["messages"]

    response = model.invoke(messages, config)
    return {"messages": response}
```
ğŸ”¹ **Explanation**:
- `state.get("summary", "")` â†’ Fetches the **previous summary** (if any).
- If a summary exists, we **add it to the messages** so the chatbot remembers past interactions.
- `model.invoke(messages, config)` â†’ Calls **GPT-4** to generate a response.

---
## âœ **Summarizing Conversations for Memory**
To **summarize long conversations**, we use a helper function.

```python
def summarize_conversation(state: State):
    
    # First, we get any existing summary
    summary = state.get("summary", "")

    # Create our summarization prompt 
    if summary:
        summary_message = (
            f"This is summary of the conversation to date: {summary}\n\n"
            "Extend the summary by taking into account the new messages above:"
        )
    else:
        summary_message = "Create a summary of the conversation above:"

    # Add prompt to our history
    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = model.invoke(messages)
    
    # Delete all but the 2 most recent messages
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
    return {"summary": response.content, "messages": delete_messages}
```
ğŸ”¹ **Explanation**:
- If a **summary exists**, we extend it with the latest messages.
- Otherwise, we ask GPT-4 to **create a new summary**.
- `RemoveMessage(id=m.id) for m in state["messages"][:-2]` â†’ Keeps only **the last two messages** for efficiency.

---
## â“ **When Should the Chatbot Summarize?**
We define a function to decide **when to summarize the conversation**.

```python
def should_continue(state: State):
    
    """Return the next node to execute."""
    
    messages = state["messages"]
    
    # If there are more than six messages, then we summarize the conversation
    if len(messages) > 6:
        return "summarize_conversation"
    
    # Otherwise we can just end
    return END
```
ğŸ”¹ **Explanation**:
- If **more than 6 messages** exist, we **trigger a summary**.
- Otherwise, we **end the conversation**.

---
## ğŸ”— **Building the Chatbot Graph**
Now, we create a **workflow graph** to connect all the components.

```python
# Define a new graph
workflow = StateGraph(State)
workflow.add_node("conversation", call_model)
workflow.add_node(summarize_conversation)

# Set the entrypoint as conversation
workflow.add_edge(START, "conversation")
workflow.add_conditional_edges("conversation", should_continue)
workflow.add_edge("summarize_conversation", END)

# Compile
memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)
display(Image(graph.get_graph().draw_mermaid_png()))
```
ğŸ”¹ **Explanation**:
- `StateGraph(State)` â†’ Creates a **graph-based chatbot workflow**.
- `workflow.add_node("conversation", call_model)` â†’ Calls GPT-4 when a user sends a message.
- `workflow.add_conditional_edges("conversation", should_continue)` â†’ Decides **whether to continue or summarize**.
- `workflow.compile(checkpointer=memory)` â†’ Saves **conversation history**.
- `display(Image(graph.get_graph().draw_mermaid_png()))` â†’ **Visualizes the chatbot graph**.

---
## ğŸ¯ **Key Takeaways**
âœ… **Streaming** allows real-time **interaction with graphs**.  
âœ… **Memory** enables chatbots to **remember previous conversations**.  
âœ… **Summarization** helps maintain **long-running conversations efficiently**.  
âœ… **Graph-based workflows** make chatbots **modular and scalable**.  

---
## ğŸ“ **Next Steps**
Now that you understand **streaming in LangGraph**, you can:
1. Try **modifying the chatbot** to support **different memory strategies**.
2. Implement **live streaming visualization** for **real-time token updates**.
3. Explore **integrating other AI models** like **Claude or Gemini**.

Would you like a follow-up guide on **visualizing token-wise streaming**? ğŸš€

---
# ğŸŒ **Streaming Full State in LangGraph**  

## ğŸ“Œ **Introduction to Streaming in LangGraph**  
LangGraph provides a mechanism to stream back results while executing a conversation graph. It offers both **synchronous (`.stream`)** and **asynchronous (`.astream`)** methods to process and return data efficiently.  

This capability is useful in real-world applications like:  
âœ… **Chatbots** that update responses dynamically  
âœ… **Real-time dashboards** that display updates as they happen  
âœ… **Monitoring systems** that track live state changes  

---

## ğŸš€ **Streaming Modes in LangGraph**  

LangGraph supports two primary modes for streaming graph states:  

1ï¸âƒ£ **`values` Mode** â€“ Streams the **entire state** of the graph after each node execution.  
2ï¸âƒ£ **`updates` Mode** â€“ Streams **only the changes** (updates) to the graph state after each node runs.  

| Streaming Mode  | What it Streams? | When to Use? |
|----------------|------------------|--------------|
| `values`  | Full graph state  | When you need the entire context after each node execution |
| `updates` | Only state updates | When you want incremental changes without redundant data |

### ğŸ“Œ **Visual Representation**
The difference between `values` and `updates` can be visualized as:  

ğŸ”¹ **Values mode:** Gives a complete snapshot after each step.  
ğŸ”¹ **Updates mode:** Only provides **what changed** at each step.  

---

## ğŸ“ **Example: Streaming with `updates` Mode**  

Hereâ€™s how we can stream **only state updates** in LangGraph:  

### **ğŸ”¹ Step 1: Create a Conversation Thread**
We start by defining a **configuration** for the conversation thread.

```python
# Define a configuration for the thread
config = {"configurable": {"thread_id": "1"}}
```
- The `config` dictionary contains a `thread_id` to **uniquely identify** our conversation.  

### **ğŸ”¹ Step 2: Start Streaming State Updates**
Now, we run a **conversation** and stream the updates as they occur.

```python
# Start a conversation and stream updates
for chunk in graph.stream(
    {"messages": [HumanMessage(content="hi! I'm Lance")]}, 
    config, 
    stream_mode="updates"
):
    print(chunk)
```
### **ğŸ§ Explanation of Code:**
1ï¸âƒ£ `graph.stream(...)` â†’ Starts streaming the graph execution.  
2ï¸âƒ£ `{"messages": [HumanMessage(content="hi! I'm Lance")]}` â†’ Represents user input.  
3ï¸âƒ£ `stream_mode="updates"` â†’ Ensures only **state updates** are streamed.  
4ï¸âƒ£ `print(chunk)` â†’ Displays only **what changed** in the state.  

### **ğŸ“Œ Sample Output**  
```json
{
  "conversation": {
    "messages": AIMessage(content='Hi Lance! How can I assist you today?', 
    response_metadata={'total_tokens': 21})
  }
}
```
ğŸ’¡ **Notice:** We only receive the **update** (AI's response), not the entire conversation state.

---

## ğŸ“ **Example: Streaming with `values` Mode**  

Now, letâ€™s modify our example to stream **the full state** after each node execution.  

### **ğŸ”¹ Step 1: Define a New Conversation Thread**
```python
# Define a new thread for this example
config = {"configurable": {"thread_id": "2"}}
```
ğŸ”¹ We create a **new thread (`thread_id: "2"`)** for the next conversation session.

### **ğŸ”¹ Step 2: Start Streaming Full State (`values` Mode)**
```python
# Start a conversation and stream full state values
input_message = HumanMessage(content="hi! I'm Lance")

for event in graph.stream(
    {"messages": [input_message]}, 
    config, 
    stream_mode="values"
):
    for m in event['messages']:
        m.pretty_print()
    print("---" * 25)
```

### **ğŸ§ Explanation of Code:**
1ï¸âƒ£ `stream_mode="values"` â†’ Streams the **entire state** after each node execution.  
2ï¸âƒ£ `for m in event['messages']:` â†’ Loops through all messages in the state.  
3ï¸âƒ£ `m.pretty_print()` â†’ Displays messages in a **formatted way**.  
4ï¸âƒ£ `print("---" * 25)` â†’ Separates output for better readability.  

### **ğŸ“Œ Sample Output**  
```plaintext
================================ Human Message =================================
hi! I'm Lance
---------------------------------------------------------------------------
================================ Ai Message ==================================
Hi Lance! How can I assist you today?
---------------------------------------------------------------------------
```
ğŸ’¡ **Notice:** Here, we receive the **full conversation state**, including both the user input and AI response.

---

## ğŸ¯ **Key Differences Between `values` and `updates` Modes**  

| Feature         | `values` Mode ğŸ·ï¸ | `updates` Mode ğŸ”„ |
|---------------|----------------|----------------|
| Data Streamed | Entire graph state | Only state changes |
| Data Size | Larger | Smaller |
| Use Case | When full history is needed | When tracking only changes |

---

## ğŸ“Œ **Real-World Use Cases of Streaming Graph State**  

âœ… **Live Chatbots:** AI assistants that dynamically respond to users while maintaining conversation history.  
âœ… **Real-Time Monitoring:** Systems that continuously process updates (e.g., stock market, IoT devices).  
âœ… **Customer Support AI:** Automated systems that assist customers based on conversation state.  
âœ… **Workflow Execution:** Streaming updates of long-running processes (e.g., order processing).  

---

## ğŸ† **Conclusion: When to Use `values` vs `updates`?**  
- Use **`values` mode** if you need **the full state after each step**.  
- Use **`updates` mode** if you want **just the changes** to minimize data transfer.  

ğŸš€ **Mastering streaming state updates will help you build efficient, scalable, and real-time AI applications!**

---

# ğŸš€ **When to Use `values` vs. `updates` in LangGraph Streaming?**  

LangGraph provides two modes for streaming graph state:  
1ï¸âƒ£ **`values` Mode** â€“ Streams the full state after each step.  
2ï¸âƒ£ **`updates` Mode** â€“ Streams only the **changes** after each step.  

Understanding **when to use** each mode is **important** for optimizing performance and making the right design choice in your application. Letâ€™s go deep into **where to use and where not to use each mode** with **real-world examples** and **scenarios**.

---

## ğŸ”¹ **Scenario 1: Building a Customer Support Chatbot**
ğŸ’¡ **Situation:**  
You are developing an **AI-powered chatbot** for a customer service website.  

âœ… **Use `values` Mode (Full State Streaming)**  
- **Why?** You need to **keep track of the full conversation** history after every message.  
- **Example:** A chatbot assisting customers should remember past messages to give better responses.  

ğŸ”¹ **Example Code:**
```python
config = {"configurable": {"thread_id": "support_1"}}

for event in graph.stream(
    {"messages": [HumanMessage(content="I need help with my order")]},
    config,
    stream_mode="values"
):
    for m in event['messages']:
        m.pretty_print()
    print("---" * 25)
```
**ğŸ‘€ Output:**  
```
================================ Human Message ================================
I need help with my order
---------------------------------------------------------------------------
================================ AI Message ==================================
Sure! Can you provide your order number?
---------------------------------------------------------------------------
```
ğŸš€ **Why `values` mode?**  
- It keeps the full conversation history, which is crucial for **context-aware** chatbots.  

âŒ **Do NOT use `updates` Mode**  
- Since `updates` only provides the latest message, it **loses the full conversation history**.

---

## ğŸ”¹ **Scenario 2: Real-Time Stock Market Updates**
ğŸ’¡ **Situation:**  
You are building a **stock market tracking system** that continuously updates **only the latest stock prices**.  

âœ… **Use `updates` Mode (Incremental Updates)**  
- **Why?** You donâ€™t need the full history of stock pricesâ€”just the **latest changes**.  
- **Example:** A stock ticker that displays only **new price changes** every second.  

ğŸ”¹ **Example Code:**
```python
config = {"configurable": {"thread_id": "stock_market"}}

for chunk in graph.stream(
    {"stock_data": ["AAPL", "GOOGL", "TSLA"]}, 
    config, 
    stream_mode="updates"
):
    print(chunk)
```
**ğŸ‘€ Output:**  
```json
{"AAPL": "Price updated: $175.32"}
{"TSLA": "Price updated: $665.50"}
{"GOOGL": "Price updated: $2800.40"}
```
ğŸš€ **Why `updates` mode?**  
- It **only** sends **the latest stock price** instead of the entire price history.  
- It is **faster and more efficient** because it doesnâ€™t send unnecessary data.  

âŒ **Do NOT use `values` Mode**  
- **Unnecessary data transfer** â†’ You donâ€™t need the full stock price history every second.  

---

## ğŸ”¹ **Scenario 3: Live Transcription System (Captions for Videos)**
ğŸ’¡ **Situation:**  
You are creating a **live subtitle system** that transcribes speech in **real time** for videos or meetings.  

âœ… **Use `updates` Mode (Incremental Updates)**  
- **Why?** You only want **new words** to appear on the screen instead of retransmitting the entire conversation.  
- **Example:** A **YouTube Live transcription system** that updates captions word by word.  

ğŸ”¹ **Example Code:**
```python
config = {"configurable": {"thread_id": "live_transcription"}}

for chunk in graph.stream(
    {"audio": "Live speech input"}, 
    config, 
    stream_mode="updates"
):
    print(chunk)
```
**ğŸ‘€ Output:**  
```json
{"transcription": "Hello, everyone!"}
{"transcription": "Welcome to our event."}
```
ğŸš€ **Why `updates` mode?**  
- **Efficient:** It only streams **new words**, preventing redundant data.  
- **Fast:** Makes live captions **appear instantly** instead of waiting for the entire sentence.  

âŒ **Do NOT use `values` Mode**  
- It would **retransmit** the full conversation every time, making captions **slow and inefficient**.

---

## ğŸ”¹ **Scenario 4: Workflow Execution Monitoring**
ğŸ’¡ **Situation:**  
You are building a **workflow automation system** that executes multiple tasks sequentially.  

âœ… **Use `values` Mode (Full State Streaming)**  
- **Why?** You need to **track all completed steps** at each stage.  
- **Example:** A **bank loan approval system** that goes through multiple checks before approval.  

ğŸ”¹ **Example Code:**
```python
config = {"configurable": {"thread_id": "loan_process"}}

for event in graph.stream(
    {"loan_application": "12345"}, 
    config, 
    stream_mode="values"
):
    for m in event['workflow_steps']:
        print(m)
```
**ğŸ‘€ Output:**  
```
Loan Application Received âœ…
Credit Check Completed âœ…
Loan Approved âœ…
```
ğŸš€ **Why `values` mode?**  
- It keeps track of **all completed workflow steps**.  
- Users can see the **full history** of what has happened so far.  

âŒ **Do NOT use `updates` Mode**  
- It would only show the **latest step**, making it hard to track the full process.

---

## ğŸ”¥ **Comparison Table: When to Use `values` vs. `updates` Mode**  

| Use Case | âœ… Use `values` Mode (Full State) | âœ… Use `updates` Mode (Only Changes) |
|----------|--------------------------------|--------------------------------|
| ğŸ¤– Chatbot (with history) | âœ… | âŒ |
| ğŸ“ˆ Stock Market Updates | âŒ | âœ… |
| ğŸ¤ Live Transcriptions | âŒ | âœ… |
| ğŸ¦ Loan Approval Workflow | âœ… | âŒ |
| ğŸš€ Real-Time Sensor Monitoring | âŒ | âœ… |
| ğŸ“Š Data Dashboard (Historical Data) | âœ… | âŒ |

---

## ğŸ¯ **Final Summary: Which Mode Should You Use?**
| **Scenario** | **Mode to Use** | **Why?** |
|-------------|---------------|---------|
| **Keeping full history** (e.g., chatbots, workflows) | `values` | Keeps track of **all past interactions** |
| **Live data updates** (e.g., stock prices, live captions) | `updates` | Sends **only the latest changes**, avoiding redundancy |
| **Event-driven systems** (e.g., IoT monitoring) | `updates` | Streams **real-time changes** without old data |
| **Tracking completed steps** (e.g., approval processes) | `values` | Stores **entire progress history** |

---

## ğŸ† **Conclusion: Making the Right Choice**
- Use **`values` mode** when **you need the full state** at each step.  
- Use **`updates` mode** when **you only need new changes** and want **faster, efficient updates**.  

ğŸ’¡ **By choosing the right mode, you can build faster and smarter real-time applications! ğŸš€**

---

# ğŸŸ¢ **Streaming Tokens in AI Chat Models**  

Streaming tokens is a technique used to receive and process AI-generated responses in real time instead of waiting for the entire response to be completed. This is particularly useful in chat applications, real-time AI interactions, and dynamic content generation.

---

## ğŸ”¥ **Why Use Streaming Tokens?**
1. **Real-time User Experience:** Enhances responsiveness by showing words as they are generated.
2. **Efficient Processing:** Allows early processing of tokens without waiting for the full response.
3. **Better User Engagement:** Makes conversations feel more natural and interactive.

---

# ğŸ“Œ **Understanding `astream_events` Method**
The `.astream_events()` method is used to stream events as they occur inside nodes in an AI model. Each event contains useful information:

| ğŸ”¹ Key         | ğŸ”¹ Description |
|---------------|--------------|
| `event`      | The type of event being emitted (e.g., `on_chat_model_stream`) |
| `name`       | The name of the event |
| `data`       | The actual response content from the AI model |
| `metadata`   | Extra information, including `langgraph_node` (node emitting the event) |

---

## ğŸ— **Example: Streaming AI Chat Responses**
### ğŸ›  **Step-by-Step Code Breakdown**
```python
config = {"configurable": {"thread_id": "3"}}
input_message = HumanMessage(content="Tell me about the 49ers NFL team")

async for event in graph.astream_events({"messages": [input_message]}, config, version="v2"):
    print(f"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}")
```

### âœ… **Explanation of Code**
1. **Define Configuration:**
   ```python
   config = {"configurable": {"thread_id": "3"}}
   ```
   - This sets up the conversation thread with an ID (`thread_id: 3`).

2. **Create Input Message:**
   ```python
   input_message = HumanMessage(content="Tell me about the 49ers NFL team")
   ```
   - A message is created that asks the AI about the 49ers football team.

3. **Start Streaming Events:**
   ```python
   async for event in graph.astream_events({"messages": [input_message]}, config, version="v2"):
   ```
   - Calls `.astream_events()` to stream events from the AI model.

4. **Extract & Print Event Details:**
   ```python
   print(f"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}")
   ```
   - Displays which node emitted the event, its type, and name.

---

## ğŸ”¥ **Extracting Chat Model Tokens**
We can filter the events to extract only chat-related tokens.

### âœ¨ **Modified Code Example**
```python
node_to_stream = 'conversation'
config = {"configurable": {"thread_id": "4"}}
input_message = HumanMessage(content="Tell me about the 49ers NFL team")

async for event in graph.astream_events({"messages": [input_message]}, config, version="v2"):
    if event["event"] == "on_chat_model_stream" and event['metadata'].get('langgraph_node','') == node_to_stream:
        print(event["data"])
```

### âœ… **Explanation of Code**
1. **Filter Specific Node:**
   ```python
   node_to_stream = 'conversation'
   ```
   - We specify a node that we want to stream from.

2. **Stream Only Chat Model Tokens:**
   ```python
   if event["event"] == "on_chat_model_stream" and event['metadata'].get('langgraph_node','') == node_to_stream:
   ```
   - Checks if the event is `on_chat_model_stream` and belongs to the `conversation` node.

3. **Print Token Data:**
   ```python
   print(event["data"])
   ```
   - Displays AI response tokens in chunks.

---

## ğŸ† **Real-World Use Cases of Streaming Tokens**
| ğŸ“Œ Use Case | ğŸ¯ How Streaming Helps |
|------------|----------------------|
| **Chatbots** | Provides a smooth real-time chat experience |
| **Live Transcription** | Streams words as they are recognized by speech-to-text models |
| **Code Assistants** | AI-powered code suggestions appear instantly |
| **AI-Powered Search** | Streams search results as they are generated |

---

## ğŸ”„ **Alternative Approaches**
Here are three alternative ways to implement streaming.

### ğŸ”¹ **1. Using OpenAI's Streaming API**
If using OpenAI's `openai.ChatCompletion.create` with `stream=True`:
```python
import openai

response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Tell me about the 49ers NFL team"}],
    stream=True
)

for chunk in response:
    if chunk["choices"][0]["delta"]:
        print(chunk["choices"][0]["delta"]["content"], end="")
```
âœ… **Key Difference:** Uses OpenAIâ€™s built-in streaming for direct API interaction.

---

### ğŸ”¹ **2. Using WebSockets for Streaming**
If implementing a chatbot with WebSockets:
```python
import asyncio
import websockets

async def chat():
    async with websockets.connect("wss://your-ai-server.com/chat") as ws:
        await ws.send("Tell me about the 49ers NFL team")
        while True:
            response = await ws.recv()
            print(response, end=" ")

asyncio.run(chat())
```
âœ… **Key Difference:** Uses WebSockets for continuous streaming from an AI server.

---

### ğŸ”¹ **3. Using FastAPI for Streaming in Web Apps**
If you want to integrate token streaming in a FastAPI server:
```python
from fastapi import FastAPI
from starlette.responses import StreamingResponse
import asyncio

app = FastAPI()

async def generate_stream():
    tokens = ["The", " San", " Francisco", " 49ers", " are", " a", " professional", " football", " team."]
    for token in tokens:
        yield token
        await asyncio.sleep(0.2)  # Simulate delay

@app.get("/stream")
async def stream():
    return StreamingResponse(generate_stream(), media_type="text/plain")
```
âœ… **Key Difference:** Streams responses via an API endpoint.

---

## ğŸš€ **Final Thoughts**
- **Use `.astream_events()`** when working with AI models that provide real-time token generation.
- **Filter events** to extract only relevant responses.
- **Consider alternative approaches** like OpenAIâ€™s API, WebSockets, or FastAPI for different scenarios.

Would you like a hands-on project where we implement streaming in a chatbot? ğŸš€

---

# ğŸš€ **Streaming with LangGraph API**  

Streaming allows us to receive data in real time as it becomes available, rather than waiting for a full response. In the **LangGraph API**, we can use **streaming** to retrieve values, messages, and metadata efficiently.

---

## ğŸ”¥ **What is Streaming in LangGraph?**
Streaming in **LangGraph** allows us to continuously receive **tokens** or **data chunks** from a chat model or an agent. This is useful when working with **AI-generated content, real-time applications, and chatbot interactions**.

### ğŸ“Œ **Why Use Streaming?**
- âœ… **Faster Responses** â€“ Get data in real-time without waiting for full completion.
- âœ… **Better User Experience** â€“ Users see responses appearing gradually, like a real conversation.
- âœ… **Efficient Resource Utilization** â€“ Processes data as it arrives instead of waiting.

---

## ğŸ“ **Key Components in LangGraph Streaming**  
Each streaming event contains the following:

ğŸ”¹ **`event`** â†’ Type of event (e.g., `metadata`, `messages/partial`)  
ğŸ”¹ **`data`** â†’ Data associated with the event (e.g., message content)  

ğŸ”¹ **`metadata`** â†’ Information about the node emitting the event  

---

## ğŸ”· **Example 1: Basic Streaming with LangGraph**
### **ğŸ“Œ Setup LangGraph Client and Stream Data**
Below is a **Python** example demonstrating **LangGraph Streaming**:

### **ğŸ”¹ Step 1: Import Required Libraries**
```python
import platform
from langgraph_sdk import get_client
from langchain_core.messages import HumanMessage
import asyncio
```
- ğŸ“Œ `platform`: Checks the operating system (Mac, Windows, etc.).
- ğŸ“Œ `get_client`: Gets the LangGraph API client.
- ğŸ“Œ `HumanMessage`: Represents a human-generated message.
- ğŸ“Œ `asyncio`: Used for asynchronous streaming.

---

### **ğŸ”¹ Step 2: Check for Compatible OS**
```python
if 'google.colab' in str(get_ipython()) or platform.system() != 'Darwin':
    raise Exception("Unfortunately, LangGraph Studio is currently not supported on Google Colab or requires a Mac")
```
- âœ… Ensures compatibility (LangGraph Studio works only on **Mac**).
- âœ… Raises an error if **Google Colab** or **non-Mac OS** is detected.

---

### **ğŸ”¹ Step 3: Connect to LangGraph API**
```python
# Replace with your LangGraph API URL
URL = "http://localhost:56091"
client = get_client(url=URL)
```
- âœ… Connects to **LangGraph API** using a **local server URL**.
- âœ… `client` is used to interact with the LangGraph API.

---

### **ğŸ”¹ Step 4: Create a New Thread**
```python
thread = await client.threads.create()
```
- âœ… Creates a new **thread** for managing conversations.

---

### **ğŸ”¹ Step 5: Stream Messages in Real-Time**
```python
# Define a user message
input_message = HumanMessage(content="Multiply 2 and 3")

# Stream messages in real-time
async for event in client.runs.stream(
    thread["thread_id"], 
    assistant_id="agent", 
    input={"messages": [input_message]}, 
    stream_mode="values"
):
    print(event)
```
- âœ… Sends a **message** (`Multiply 2 and 3`) to the assistant.
- âœ… **Streams** the response **in real-time**.
- âœ… Each `event` represents a **part** of the streamed response.

---

### **ğŸ”· Output (Example)**
```
StreamPart(event='metadata', data={'run_id': '1ef6a3d0-41eb-66f4-a311-8ebdfa1b281f'})
StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'type': 'human'}]})
```
- âœ… The first response is **metadata** (Run ID).
- âœ… The second response contains **messages**.

---

## ğŸ”· **Example 2: Extracting Messages from Streaming**
Instead of printing raw data, we can extract **only the messages**:

```python
from langchain_core.messages import convert_to_messages

thread = await client.threads.create()
input_message = HumanMessage(content="Multiply 2 and 3")

async for event in client.runs.stream(
    thread["thread_id"], 
    assistant_id="agent", 
    input={"messages": [input_message]}, 
    stream_mode="values"
):
    messages = event.data.get('messages', None)
    if messages:
        print(convert_to_messages(messages)[-1])  # Display last message
    print('=' * 25)
```

### **ğŸ”· Output**
```
=========================
content='Multiply 2 and 3'
=========================
content='' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}}]
=========================
content='6'
=========================
```
- âœ… First message: **User input ("Multiply 2 and 3")**.
- âœ… Second message: **Function call to "multiply"**.
- âœ… Third message: **Final result ("6")**.

---

## ğŸŒ **Real-World Use Cases of Streaming in LangGraph**
ğŸ“Œ **Chatbots & Virtual Assistants** â€“ Generate AI responses in real-time.  
ğŸ“Œ **Stock Market Feeds** â€“ Stream live **stock price updates**.  
ğŸ“Œ **Sports Updates** â€“ Get real-time **match scores & statistics**.  
ğŸ“Œ **Live Transcription** â€“ Convert **speech to text** in **real-time**.  

---

## ğŸ¯ **Alternative Examples for Strong Understanding**
### **ğŸ”¹ Alternative 1: Streaming for AI Assistant**
```python
input_message = HumanMessage(content="What is the capital of France?")
async for event in client.runs.stream(thread["thread_id"], assistant_id="agent", input={"messages": [input_message]}, stream_mode="messages"):
    print(event.event)
```
ğŸ”¹ **Output:**  
```
metadata
messages/complete
messages/metadata
messages/partial
```

---

### **ğŸ”¹ Alternative 2: Streaming Live Weather Updates**
```python
input_message = HumanMessage(content="What's the weather in New York?")
async for event in client.runs.stream(thread["thread_id"], assistant_id="weather_bot", input={"messages": [input_message]}, stream_mode="values"):
    print(event)
```
ğŸ”¹ **Output:**  
```
Current temperature: 72Â°F
Humidity: 60%
```

---

### **ğŸ”¹ Alternative 3: Streaming API Logs for Debugging**
```python
async for event in client.runs.stream(thread["thread_id"], assistant_id="debugger", input={"messages": []}, stream_mode="messages"):
    print(f"Event: {event.event} - Data: {event.data}")
```
ğŸ”¹ **Output:**  
```
Event: metadata - Data: {'run_id': 'abc123'}
Event: messages/partial - Data: {'message': 'Processing...'}
```

---

## ğŸ”¥ **Summary**
âœ… **Streaming** allows real-time data flow in **LangGraph**.  
âœ… **Events & Data** help extract useful information.  
âœ… **Practical Applications** include **chatbots, live updates, AI assistants, and debugging**.  
âœ… **Different Streaming Modes** (`values`, `messages`) control how we retrieve data.  

Would you like additional explanations or a custom example? ğŸ˜Š ğŸš€

---
# ğŸš€ **Streaming with LangGraph API â€“ A Beginner-Friendly Guide**  

The **LangGraph API** enables real-time streaming of responses and interactions between users and AI agents. This is especially useful in applications like **chatbots, real-time AI assistants, and live analytics dashboards**.  

Letâ€™s break down **streaming with LangGraph API**, explaining each part of the process in a simple, detailed, and beginner-friendly manner.  

---

## ğŸ“Œ **What is Streaming in LangGraph API?**  
Streaming is a technique where **data is continuously transmitted** instead of waiting for the entire response to be ready. This is commonly used in:  

âœ… **Live chat applications** (e.g., ChatGPT, customer support bots)  
âœ… **Real-time analytics** (e.g., financial trading platforms)  
âœ… **Voice assistants** (e.g., Alexa, Siri)  

LangGraph API allows **streaming AI responses in parts**, making interactions **faster and more efficient**.  

---

## ğŸ›  **Setting Up the LangGraph Client**  
Before we begin, let's set up our **LangGraph API client** to interact with the backend.

### **ğŸ”¹ Step 1: Import Required Libraries**
```python
import platform

# Check if the environment supports LangGraph Studio
if 'google.colab' in str(get_ipython()) or platform.system() != 'Darwin':
    raise Exception("Unfortunately, LangGraph Studio is currently not supported on Google Colab or requires a Mac")

# Import the LangGraph SDK
from langgraph_sdk import get_client

# Set up the API client with the server URL
URL = "http://localhost:56091"  # Replace with your actual LangGraph server URL
client = get_client(url=URL)
```
### ğŸ” **Explanation**:
- **`platform.system()`** â†’ Checks if the system is a Mac (`Darwin`), as **LangGraph Studio only supports Mac**.
- **`get_client(url=URL)`** â†’ Creates a connection to the LangGraph server.
- **`raise Exception()`** â†’ Prevents execution if running in an unsupported environment.

---

## ğŸ“ **Streaming Responses from the AI Assistant**  

We will **send a user query** to the AI assistant and stream its response in real-time.

### **ğŸ”¹ Step 2: Search for Available Assistants**
```python
# Fetch all available AI assistants hosted on the LangGraph server
assistants = await client.assistants.search()
```
### ğŸ” **Explanation**:
- **`client.assistants.search()`** â†’ Fetches all **available AI assistants** from the LangGraph API.
- Useful if you **host multiple AI agents** and want to select the right one.

---

### **ğŸ”¹ Step 3: Create a Thread for the Conversation**
```python
# Create a new chat thread
thread = await client.threads.create()
```
### ğŸ” **Explanation**:
- **A thread** represents a **chat session**.  
- Every message in LangGraph is part of a **thread**, allowing AI to **maintain context**.

---

### **ğŸ”¹ Step 4: Send a Message and Stream the Response**
```python
# Create a user message
input_message = HumanMessage(content="Multiply 2 and 3")

# Stream AI responses
async for event in client.runs.stream(
    thread["thread_id"],
    assistant_id="agent",
    input={"messages": [input_message]},
    stream_mode="messages",
):
    print(event)
```
### ğŸ” **Explanation**:
- **Creates a user message (`HumanMessage(content="Multiply 2 and 3")`)**  
- **Streams the assistantâ€™s response** using `client.runs.stream()`.  
- The response is received **in chunks** instead of waiting for the entire reply.  

---

## ğŸ“¡ **Understanding Streamed Data (Events & Messages)**  

LangGraph **streams data in parts**, which means different types of events will arrive.  

### ğŸ“Œ **Key Event Types**:
| Event Type         | Purpose |
|--------------------|---------|
| **metadata**       | Information about the request (e.g., Run ID) |
| **messages/partial** | AI-generated responses arriving in chunks (tokens) |
| **messages/complete** | Fully formed AI response |

---

## ğŸ“¢ **Handling and Formatting AI Responses**
AI messages sometimes include **tool calls** (e.g., calling a calculator function).  
Letâ€™s **format** these tool calls into a **readable** format.

### **ğŸ”¹ Step 5: Define a Helper Function to Format Tool Calls**
```python
def format_tool_calls(tool_calls):
    """
    Format a list of tool calls into a readable string.

    Args:
        tool_calls (list): A list of dictionaries, each representing a tool call.

    Returns:
        str: A formatted string of tool calls, or "No tool calls" if the list is empty.
    """

    if tool_calls:
        formatted_calls = []
        for call in tool_calls:
            formatted_calls.append(
                f"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}"
            )
        return "\n".join(formatted_calls)
    return "No tool calls"
```
### ğŸ” **Explanation**:
- **Takes a list of tool calls** and extracts useful details like `function name`, `arguments`, and `ID`.
- **Returns a formatted string** for easy reading.

---

### **ğŸ”¹ Step 6: Handle Streamed Events and Process Responses**
```python
async for event in client.runs.stream(
    thread["thread_id"],
    assistant_id="agent",
    input={"messages": [input_message]},
    stream_mode="messages",
):
    # Handle metadata events
    if event.event == "metadata":
        print(f"Metadata: Run ID - {event.data['run_id']}")
        print("-" * 50)

    # Handle partial message events
    elif event.event == "messages/partial":
        for data_item in event.data:
            # Process user messages
            if "role" in data_item and data_item["role"] == "user":
                print(f"Human: {data_item['content']}")
            else:
                # Extract relevant data from the event
                tool_calls = data_item.get("tool_calls", [])
                invalid_tool_calls = data_item.get("invalid_tool_calls", [])
                content = data_item.get("content", "")
                response_metadata = data_item.get("response_metadata", {})

                if content:
                    print(f"AI: {content}")

                if tool_calls:
                    print("Tool Calls:")
                    print(format_tool_calls(tool_calls))

                if invalid_tool_calls:
                    print("Invalid Tool Calls:")
                    print(format_tool_calls(invalid_tool_calls))

                if response_metadata:
                    finish_reason = response_metadata.get("finish_reason", "N/A")
                    print(f"Response Metadata: Finish Reason - {finish_reason}")
                    
        print("-" * 50)
```
### ğŸ” **Explanation**:
- **Checks the type of event** (`metadata`, `messages/partial`, etc.).
- **Prints human messages and AI responses** in real-time.
- **Formats tool calls** for easy readability.
- **Extracts metadata** (e.g., why AI stopped responding).

---

## ğŸŒ **Real-World Applications of LangGraph Streaming**
| Application | Use Case |
|------------|----------|
| **AI Chatbots** | Streaming responses in real-time (e.g., customer support) |
| **Live Transcription** | AI-powered transcription of spoken words (e.g., YouTube subtitles) |
| **Stock Market Analytics** | Real-time price analysis (e.g., financial trading bots) |
| **Voice Assistants** | Alexa/Siri-style real-time interactions |

---

## ğŸ† **Key Takeaways**
âœ… **LangGraph API supports real-time AI streaming.**  
âœ… **Different event types (metadata, messages/partial, etc.) provide structured data.**  
âœ… **Tool calls can be formatted for better understanding.**  
âœ… **Streaming is used in AI assistants, chatbots, finance, and more.**  

---

## ğŸ¯ **Alternative Approaches**
1ï¸âƒ£ **Use REST API instead of streaming** if real-time responses are not required.  
2ï¸âƒ£ **Cache AI responses** to reduce redundant processing in repeated queries.  
3ï¸âƒ£ **Use WebSockets** for bidirectional communication between AI and users.  

---

This guide covers **everything a beginner needs to understand LangGraph streaming!** ğŸš€  

Would you like a **real-world example** of integrating LangGraph into a chatbot? Let me know! ğŸ˜Š